{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPFmKWEDygR6"
   },
   "source": [
    "![Banner](https://raw.githubusercontent.com/crunchdao/quickstarters/refs/heads/master/competitions/broad-3/assets/banner.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8PyJPSkxoRo"
   },
   "source": [
    "## **Autoimmune Disease Machine Learning Crunch #3**\n",
    "\n",
    "## Quickstarter: Gene Ranking for colon tissue using logFC method\n",
    "\n",
    "This is an example submission and can be used as a starting point, but should not be seen as the only way to approach the problem. However, this approach can be improved in many ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovnPAKW52GYm"
   },
   "source": [
    "This notebook addresses the task of **ranking genes** to identify **pre-cancerous tissue regions** in the context of inflammatory bowel disease (IBD). The goal is to rank **18 615 genes** based on their ability to distinguish dysplastic (pre-cancerous) from non-cancerous mucosa regions in colon tissue samples.\n",
    "\n",
    "The solution follows a three-step process:\n",
    "\n",
    "* **Crunch 1: Predict the Expression of 460 Measured Genes**: This step uses a **ResNet50 embedding model and regression models**, trained on H&E pathology images, to predict the expression of 460 genes for each cell in the dataset.\n",
    "\n",
    "* **Crunch 2: Predict the Expression of 18157 Unmeasured Genes**: Leveraging the predictions from Crunch 1 and a **single-cell RNA sequencing (scRNA-Seq)** reference dataset, a **similarity-based method** is applied to infer the expression of 18157 unmeasured genes for each cell in the dataset.\n",
    "\n",
    "* **Crunch 3: Gene Ranking Using logFC Method**: In this step, we apply the **log fold change (logFC)** method to rank genes based on their ability to differentiate between dysplastic and non-dysplastic tissue regions. The logFC method computes the logarithmic fold change between the predicted expression values for the two regions (dysplastic and non-dysplastic). Genes are ranked based on the magnitude of their logFC values, with higher values indicating greater differentiation potential between the regions. For a detailed explanation of the logFC method, please refer to the [logFC explanation document](https://olvtools.com/en/rnaseq/help/fold-change).\n",
    "\n",
    "> For CrunchDAO Challenge submission, you will need to **pre-download** the gene ranking file (`ranking_gene.csv`) from your local system and submit it along with the notebook.\n",
    "\n",
    "Most of the code is already used in the [Quickstarter Crunch 1 Resnet50 + Ridge](https://github.com/crunchdao/competitions/blob/master/competitions/broad-1/quickstarters/resnet50-plus-ridge/resnet50-plus-ridge.ipynb) and Quickstarter Crunch 2 Cosine similarity. For more details on the methodology and implementation, refer to these notebooks.\n",
    "\n",
    "The added sections are:\n",
    "\n",
    "* **A Sandbox Example for the BROAD 3 Challenge**\n",
    "* **A function to apply gene ranking**\n",
    "* **Train: a function to apply Crunch 1, Crunch 2 and gene ranking from UC9_I-crunch3 sample**\n",
    "\n",
    "Note: This notebook is still designed to run seamlessly on a **CPU** environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zQLiTmlN8QY"
   },
   "source": [
    "## **SHORT EXAMPLE**\n",
    "\n",
    "Short example to help you understand both the road 3 crunch problem and the technique used to address it:\n",
    "\n",
    "![gene_ranking_short_example](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/competitions/broad-3/quickstarters/log-fc/images/gene_ranking_short_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWQHlrgiNekV"
   },
   "source": [
    "`Identifying gene markers of pre-cancerous tisue regions in IBD (Crunch 3)`\n",
    "\n",
    "![crunch_3_challenge](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/competitions/broad-3/quickstarters/log-fc/images/crunch_3_challenge.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "9UjSFajKnBnq"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKVOi6J1loBn"
   },
   "source": [
    "You need a **token** to load the dataset of the challenge.\n",
    "\n",
    "Get a new token: https://hub.crunchdao.com/competitions/broad-3/submit/via/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "p9pR4gsI9VE2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: crunch-cli in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (5.15.0)\n",
      "Requirement already satisfied: click in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from crunch-cli) (8.1.8)\n",
      "Requirement already satisfied: coloredlogs in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from crunch-cli) (15.0.1)\n",
      "Requirement already satisfied: dataclasses_json in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from crunch-cli) (0.6.7)\n",
      "Requirement already satisfied: importlib_metadata in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from crunch-cli) (8.5.0)\n",
      "Requirement already satisfied: inquirer in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from crunch-cli) (3.4.0)\n",
      "Requirement already satisfied: joblib in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from crunch-cli) (1.4.2)\n",
      "Requirement already satisfied: networkx in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from crunch-cli) (3.4.2)\n",
      "Requirement already satisfied: packaging>=24.2 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from crunch-cli) (24.2)\n",
      "Requirement already satisfied: pandas in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from crunch-cli) (2.2.3)\n",
      "Requirement already satisfied: psutil in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from crunch-cli) (6.1.1)\n",
      "Requirement already satisfied: pyarrow in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from crunch-cli) (18.1.0)\n",
      "Requirement already satisfied: pytest in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from crunch-cli) (8.3.4)\n",
      "Requirement already satisfied: python-dotenv in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from crunch-cli) (1.0.1)\n",
      "Requirement already satisfied: pyyaml in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from crunch-cli) (6.0.2)\n",
      "Requirement already satisfied: redbaron in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from crunch-cli) (0.9.2)\n",
      "Requirement already satisfied: requests in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from crunch-cli) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from crunch-cli) (1.0.0)\n",
      "Requirement already satisfied: requirements-parser in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from crunch-cli) (0.11.0)\n",
      "Requirement already satisfied: scikit-learn in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from crunch-cli) (1.6.0)\n",
      "Requirement already satisfied: scipy in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from crunch-cli) (1.15.0)\n",
      "Requirement already satisfied: tqdm in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from crunch-cli) (4.67.1)\n",
      "Requirement already satisfied: spatialdata-plot in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from crunch-cli) (0.2.8)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from coloredlogs->crunch-cli) (10.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from dataclasses_json->crunch-cli) (3.23.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from dataclasses_json->crunch-cli) (0.9.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from importlib_metadata->crunch-cli) (3.21.0)\n",
      "Requirement already satisfied: blessed>=1.19.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from inquirer->crunch-cli) (1.20.0)\n",
      "Requirement already satisfied: editor>=1.6.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from inquirer->crunch-cli) (1.6.6)\n",
      "Requirement already satisfied: readchar>=4.2.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from inquirer->crunch-cli) (4.2.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from pandas->crunch-cli) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from pandas->crunch-cli) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from pandas->crunch-cli) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from pandas->crunch-cli) (2024.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from pytest->crunch-cli) (1.2.2)\n",
      "Requirement already satisfied: iniconfig in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from pytest->crunch-cli) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from pytest->crunch-cli) (1.5.0)\n",
      "Requirement already satisfied: tomli>=1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from pytest->crunch-cli) (2.2.1)\n",
      "Requirement already satisfied: baron>=0.7 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from redbaron->crunch-cli) (0.10.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from requests->crunch-cli) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from requests->crunch-cli) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from requests->crunch-cli) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from requests->crunch-cli) (2024.12.14)\n",
      "Requirement already satisfied: types-setuptools>=69.1.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from requirements-parser->crunch-cli) (75.6.0.20241223)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scikit-learn->crunch-cli) (3.5.0)\n",
      "Requirement already satisfied: matplotlib in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata-plot->crunch-cli) (3.10.0)\n",
      "Requirement already satisfied: matplotlib-scalebar in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata-plot->crunch-cli) (0.8.1)\n",
      "Requirement already satisfied: scanpy in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata-plot->crunch-cli) (1.10.4)\n",
      "Requirement already satisfied: spatialdata>=0.2.6 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata-plot->crunch-cli) (0.2.6)\n",
      "Requirement already satisfied: rply in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from baron>=0.7->redbaron->crunch-cli) (0.7.8)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from blessed>=1.19.0->inquirer->crunch-cli) (0.2.13)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from blessed>=1.19.0->inquirer->crunch-cli) (1.17.0)\n",
      "Requirement already satisfied: runs in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from editor>=1.6.0->inquirer->crunch-cli) (1.2.2)\n",
      "Requirement already satisfied: xmod in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from editor>=1.6.0->inquirer->crunch-cli) (1.8.1)\n",
      "Requirement already satisfied: anndata>=0.9.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.11.1)\n",
      "Requirement already satisfied: dask-image in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2024.5.3)\n",
      "Requirement already satisfied: dask>=2024.4.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2024.12.1)\n",
      "Requirement already satisfied: fsspec<=2023.6 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2023.6.0)\n",
      "Requirement already satisfied: geopandas>=0.14 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.0.1)\n",
      "Requirement already satisfied: multiscale-spatial-image>=2.0.2 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2.0.2)\n",
      "Requirement already satisfied: numba in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.60.0)\n",
      "Requirement already satisfied: ome-zarr>=0.8.4 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.10.2)\n",
      "Requirement already satisfied: pooch in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.8.2)\n",
      "Requirement already satisfied: rich in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (13.9.4)\n",
      "Requirement already satisfied: scikit-image in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.25.0)\n",
      "Requirement already satisfied: setuptools in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (75.8.2)\n",
      "Requirement already satisfied: shapely>=2.0.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2.0.6)\n",
      "Requirement already satisfied: spatial-image>=1.1.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (4.12.2)\n",
      "Requirement already satisfied: xarray-schema in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.0.3)\n",
      "Requirement already satisfied: xarray-spatial>=0.3.5 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.4.0)\n",
      "Requirement already satisfied: xarray>=2024.10.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2024.11.0)\n",
      "Requirement already satisfied: zarr<3 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2.18.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses_json->crunch-cli) (1.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from matplotlib->spatialdata-plot->crunch-cli) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from matplotlib->spatialdata-plot->crunch-cli) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from matplotlib->spatialdata-plot->crunch-cli) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from matplotlib->spatialdata-plot->crunch-cli) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from matplotlib->spatialdata-plot->crunch-cli) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from matplotlib->spatialdata-plot->crunch-cli) (3.2.1)\n",
      "Requirement already satisfied: h5py>=3.6 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scanpy->spatialdata-plot->crunch-cli) (3.12.1)\n",
      "Requirement already satisfied: legacy-api-wrap>=1.4 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scanpy->spatialdata-plot->crunch-cli) (1.4.1)\n",
      "Requirement already satisfied: natsort in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scanpy->spatialdata-plot->crunch-cli) (8.4.0)\n",
      "Requirement already satisfied: patsy!=1.0.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scanpy->spatialdata-plot->crunch-cli) (1.0.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scanpy->spatialdata-plot->crunch-cli) (0.5.13)\n",
      "Requirement already satisfied: seaborn>=0.13 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scanpy->spatialdata-plot->crunch-cli) (0.13.2)\n",
      "Requirement already satisfied: session-info in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scanpy->spatialdata-plot->crunch-cli) (1.0.0)\n",
      "Requirement already satisfied: statsmodels>=0.13 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scanpy->spatialdata-plot->crunch-cli) (0.14.4)\n",
      "Requirement already satisfied: umap-learn!=0.5.0,>=0.5 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scanpy->spatialdata-plot->crunch-cli) (0.5.7)\n",
      "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from anndata>=0.9.1->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.10.0)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from dask>=2024.4.1->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (3.1.0)\n",
      "Requirement already satisfied: partd>=1.4.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from dask>=2024.4.1->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.4.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from dask>=2024.4.1->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.0.0)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from geopandas>=0.14->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.10.0)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from geopandas>=0.14->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (3.7.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from numba->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.43.0)\n",
      "Requirement already satisfied: distributed in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2024.12.1)\n",
      "Requirement already satisfied: aiohttp<4 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (3.11.11)\n",
      "Requirement already satisfied: xarray-dataclasses>=1.1.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatial-image>=1.1.0->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.9.1)\n",
      "Requirement already satisfied: datashader>=0.15.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from xarray-spatial>=0.3.5->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.16.3)\n",
      "Requirement already satisfied: asciitree in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from zarr<3->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.3.3)\n",
      "Requirement already satisfied: numcodecs>=0.10.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from zarr<3->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.13.1)\n",
      "Requirement already satisfied: fasteners in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from zarr<3->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.19)\n",
      "Requirement already satisfied: pims>=0.4.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from dask-image->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.7)\n",
      "Requirement already satisfied: tifffile>=2018.10.18 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from dask-image->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2024.12.12)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from pooch->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (4.3.6)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from rich->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from rich->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2.18.0)\n",
      "Requirement already satisfied: appdirs in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from rply->baron>=0.7->redbaron->crunch-cli) (1.4.4)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scikit-image->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2.36.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scikit-image->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.4)\n",
      "Requirement already satisfied: stdlib_list in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from session-info->scanpy->spatialdata-plot->crunch-cli) (0.11.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from aiohttp<4->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.18.3)\n",
      "Requirement already satisfied: dask-expr<1.2,>=1.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from dask[array,dataframe]>=2024.4.1->dask-image->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.1.21)\n",
      "Requirement already satisfied: colorcet in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from datashader>=0.15.0->xarray-spatial>=0.3.5->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (3.1.0)\n",
      "Requirement already satisfied: multipledispatch in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from datashader>=0.15.0->xarray-spatial>=0.3.5->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.0.0)\n",
      "Requirement already satisfied: param in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from datashader>=0.15.0->xarray-spatial>=0.3.5->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2.2.0)\n",
      "Requirement already satisfied: pyct in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from datashader>=0.15.0->xarray-spatial>=0.3.5->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.5.0)\n",
      "Requirement already satisfied: s3fs in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2023.6.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.1.2)\n",
      "Requirement already satisfied: locket in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from partd>=1.4.0->dask>=2024.4.1->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.0.0)\n",
      "Requirement already satisfied: slicerator>=0.9.8 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from pims>=0.4.1->dask-image->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.1.0)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from distributed->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (3.1.5)\n",
      "Requirement already satisfied: msgpack>=1.0.2 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from distributed->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.1.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from distributed->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from distributed->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (3.0.0)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from distributed->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (6.2)\n",
      "Requirement already satisfied: zict>=3.0.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from distributed->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from jinja2>=2.10.3->distributed->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (3.0.2)\n",
      "Requirement already satisfied: aiobotocore~=2.5.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (2.5.4)\n",
      "Requirement already satisfied: botocore<1.31.18,>=1.31.17 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from aiobotocore~=2.5.0->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.31.17)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from aiobotocore~=2.5.0->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.17.0)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from aiobotocore~=2.5.0->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (0.12.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.0->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr>=0.8.4->spatialdata>=0.2.6->spatialdata-plot->crunch-cli) (1.0.1)\n",
      "main.py: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/submissions/20496/main.py (59325 bytes)\n",
      "notebook.ipynb: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/submissions/20496/notebook.ipynb (360680 bytes)\n",
      "REPORT.md: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/submissions/20496/REPORT.md (4172 bytes)\n",
      "requirements.txt: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/submissions/20496/requirements.original.txt (262 bytes)\n",
      "data/UC9_I-crunch3-HE-label-stardist.tif: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/98/UC9_I-crunch3-HE-label-stardist.tif (23778030 bytes)\n",
      "data/Crunch3_gene_list.csv: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/98/Crunch3_gene_list.csv (122177 bytes)\n",
      "data/Crunch3_scRNAseq.h5ad.zip: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/98/Crunch3_scRNAseq.h5ad.zip (924652673 bytes)\n",
      "data/Crunch3_scRNAseq.h5ad.zip: uncompress into data/Crunch3_scRNAseq.h5ad.zip.x8q3is4w\n",
      "data/UC9_I.zarr.zip: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/98/UC9_I.zarr.zip (927207232 bytes)\n",
      "data/UC9_I.zarr.zip: uncompress into data/UC9_I.zarr.zip.plyaixyf               \n",
      "data/UC9_I-crunch3-HE-dysplasia-ROI.tif: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/98/UC9_I-crunch3-HE-dysplasia-ROI.tif (778109 bytes)\n",
      "data/UC9_I-crunch3-HE.tif.zip: download from https:crunchdao--competition--production.s3.eu-west-1.amazonaws.com/data-releases/98/UC9_I-crunch3-HE.tif.zip (1227303078 bytes)\n",
      "data/UC9_I-crunch3-HE.tif.zip: uncompress into data/UC9_I-crunch3-HE.tif.zip.fk2_4zb8\n",
      "\n",
      "---\n",
      "Success! Your environment has been correctly setup.\n",
      "Next recommended actions:\n",
      " - To get inside your workspace directory, run: cd broad-3-catskills\n",
      " - To see all of the available commands of the CrunchDAO CLI, run: crunch --help\n"
     ]
    }
   ],
   "source": [
    "!pip install crunch-cli --upgrade\n",
    "!crunch setup --size default broad-3 catskills --token paUxd4z3n2t0oZfeTTpBFTvrYktBrhcsBCLYsIAb8guwlcaOBAe9IGMFinFG5lF7\n",
    "!cd broad-3-catskills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4gfY7jC5f-n"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tAzquCA99X33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded inline runner with module: <module '__main__'>\n"
     ]
    }
   ],
   "source": [
    "import crunch\n",
    "crunch = crunch.load_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0b6AgVirxijj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: numpy in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: torch in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: scipy in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (1.15.0)\n",
      "Collecting openslide-python\n",
      "  Downloading openslide_python-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: pydantic in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (2.10.6)\n",
      "Requirement already satisfied: pytorch-lightning in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (2.5.0.post0)\n",
      "Requirement already satisfied: dask in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (2024.12.1)\n",
      "Requirement already satisfied: distributed in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (2024.12.1)\n",
      "Requirement already satisfied: matplotlib in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (0.13.2)\n",
      "Collecting scikit-learn==1.5.2\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: opencv-python in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: scanpy in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (1.10.4)\n",
      "Requirement already satisfied: spatialdata in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (0.2.6)\n",
      "Requirement already satisfied: zarr in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (2.18.3)\n",
      "Requirement already satisfied: ome-zarr in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (0.10.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scikit-learn==1.5.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scikit-learn==1.5.2) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: filelock in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: Pillow in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from openslide-python) (11.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from pydantic) (2.27.2)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from pytorch-lightning) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from pytorch-lightning) (6.0.2)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from pytorch-lightning) (1.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from pytorch-lightning) (24.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from pytorch-lightning) (0.12.0)\n",
      "Requirement already satisfied: click>=8.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from dask) (8.1.8)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from dask) (3.1.0)\n",
      "Requirement already satisfied: partd>=1.4.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from dask) (1.4.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from dask) (1.0.0)\n",
      "Requirement already satisfied: importlib_metadata>=4.13.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from dask) (8.5.0)\n",
      "Requirement already satisfied: locket>=1.0.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from distributed) (1.0.0)\n",
      "Requirement already satisfied: msgpack>=1.0.2 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from distributed) (1.1.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from distributed) (6.1.1)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from distributed) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from distributed) (3.0.0)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from distributed) (6.2)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from distributed) (1.26.20)\n",
      "Requirement already satisfied: zict>=3.0.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from distributed) (3.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: anndata>=0.8 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scanpy) (0.11.1)\n",
      "Requirement already satisfied: h5py>=3.6 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scanpy) (3.12.1)\n",
      "Requirement already satisfied: legacy-api-wrap>=1.4 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scanpy) (1.4.1)\n",
      "Requirement already satisfied: natsort in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scanpy) (8.4.0)\n",
      "Requirement already satisfied: numba>=0.56 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scanpy) (0.60.0)\n",
      "Requirement already satisfied: patsy!=1.0.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scanpy) (1.0.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scanpy) (0.5.13)\n",
      "Requirement already satisfied: session-info in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scanpy) (1.0.0)\n",
      "Requirement already satisfied: statsmodels>=0.13 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scanpy) (0.14.4)\n",
      "Requirement already satisfied: umap-learn!=0.5.0,>=0.5 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scanpy) (0.5.7)\n",
      "Requirement already satisfied: dask-image in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata) (2024.5.3)\n",
      "Requirement already satisfied: geopandas>=0.14 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata) (1.0.1)\n",
      "Requirement already satisfied: multiscale-spatial-image>=2.0.2 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata) (2.0.2)\n",
      "Requirement already satisfied: pooch in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata) (1.8.2)\n",
      "Requirement already satisfied: pyarrow in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata) (18.1.0)\n",
      "Requirement already satisfied: rich in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata) (13.9.4)\n",
      "Requirement already satisfied: scikit-image in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata) (0.25.0)\n",
      "Requirement already satisfied: setuptools in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata) (75.8.2)\n",
      "Requirement already satisfied: shapely>=2.0.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata) (2.0.6)\n",
      "Requirement already satisfied: spatial-image>=1.1.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata) (1.1.0)\n",
      "Requirement already satisfied: xarray-schema in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata) (0.0.3)\n",
      "Requirement already satisfied: xarray-spatial>=0.3.5 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata) (0.4.0)\n",
      "Requirement already satisfied: xarray>=2024.10.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatialdata) (2024.11.0)\n",
      "Requirement already satisfied: asciitree in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from zarr) (0.3.3)\n",
      "Requirement already satisfied: numcodecs>=0.10.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from zarr) (0.13.1)\n",
      "Requirement already satisfied: fasteners in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from zarr) (0.19)\n",
      "Requirement already satisfied: aiohttp<4 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from ome-zarr) (3.11.11)\n",
      "Requirement already satisfied: requests in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from ome-zarr) (2.32.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from aiohttp<4->ome-zarr) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from aiohttp<4->ome-zarr) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from aiohttp<4->ome-zarr) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from aiohttp<4->ome-zarr) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from aiohttp<4->ome-zarr) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from aiohttp<4->ome-zarr) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from aiohttp<4->ome-zarr) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from aiohttp<4->ome-zarr) (1.18.3)\n",
      "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from anndata>=0.8->scanpy) (1.10.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from anndata>=0.8->scanpy) (1.2.2)\n",
      "Requirement already satisfied: s3fs in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from fsspec[s3]!=2021.07.0,>=0.8->ome-zarr) (2023.6.0)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from geopandas>=0.14->spatialdata) (0.10.0)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from geopandas>=0.14->spatialdata) (3.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from importlib_metadata>=4.13.0->dask) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from numba>=0.56->scanpy) (0.43.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: xarray-dataclasses>=1.1.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from spatial-image>=1.1.0->spatialdata) (1.9.1)\n",
      "Requirement already satisfied: datashader>=0.15.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from xarray-spatial>=0.3.5->spatialdata) (0.16.3)\n",
      "Requirement already satisfied: pims>=0.4.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from dask-image->spatialdata) (0.7)\n",
      "Requirement already satisfied: tifffile>=2018.10.18 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from dask-image->spatialdata) (2024.12.12)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from pooch->spatialdata) (4.3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from requests->ome-zarr) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from requests->ome-zarr) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from requests->ome-zarr) (2024.12.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from rich->spatialdata) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from rich->spatialdata) (2.18.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scikit-image->spatialdata) (2.36.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from scikit-image->spatialdata) (0.4)\n",
      "Requirement already satisfied: stdlib_list in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from session-info->scanpy) (0.11.0)\n",
      "Requirement already satisfied: dask-expr<1.2,>=1.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from dask[array,dataframe]>=2024.4.1->dask-image->spatialdata) (1.1.21)\n",
      "Requirement already satisfied: colorcet in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from datashader>=0.15.0->xarray-spatial>=0.3.5->spatialdata) (3.1.0)\n",
      "Requirement already satisfied: multipledispatch in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from datashader>=0.15.0->xarray-spatial>=0.3.5->spatialdata) (1.0.0)\n",
      "Requirement already satisfied: param in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from datashader>=0.15.0->xarray-spatial>=0.3.5->spatialdata) (2.2.0)\n",
      "Requirement already satisfied: pyct in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from datashader>=0.15.0->xarray-spatial>=0.3.5->spatialdata) (0.5.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->spatialdata) (0.1.2)\n",
      "Requirement already satisfied: slicerator>=0.9.8 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from pims>=0.4.1->dask-image->spatialdata) (1.1.0)\n",
      "Requirement already satisfied: aiobotocore~=2.5.0 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr) (2.5.4)\n",
      "Requirement already satisfied: botocore<1.31.18,>=1.31.17 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from aiobotocore~=2.5.0->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr) (1.31.17)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from aiobotocore~=2.5.0->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr) (1.17.0)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from aiobotocore~=2.5.0->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr) (0.12.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/catskills/Desktop/broad/broad/lib/python3.10/site-packages (from botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.0->s3fs->fsspec[s3]!=2021.07.0,>=0.8->ome-zarr) (1.0.1)\n",
      "Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m01\u001b[0m\n",
      "Downloading openslide_python-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.whl (36 kB)\n",
      "Installing collected packages: openslide-python, scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.6.0\n",
      "    Uninstalling scikit-learn-1.6.0:\n",
      "      Successfully uninstalled scikit-learn-1.6.0\n",
      "Successfully installed openslide-python-1.4.1 scikit-learn-1.5.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas torch scipy openslide-python pydantic pytorch-lightning dask distributed matplotlib seaborn scikit-learn==1.5.2 opencv-python scanpy spatialdata zarr ome-zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqt_h1tULghx"
   },
   "source": [
    "## A Sandbox Example for the Broad 3 Challenge\n",
    "\n",
    "This short example will help you understand both the Broad 3 challenge problem and the technique used to address it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 648,
     "status": "ok",
     "timestamp": 1734620962657,
     "user": {
      "displayName": "alexis gassmann",
      "userId": "07787599245733678355"
     },
     "user_tz": -60
    },
    "id": "QCGccc5zkCpu",
    "outputId": "12be64c0-ee58-4cb4-e9b9-277c26277fda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Problem statement: Identify the most discriminative genes that distinguish dysplasia (pre-cancerous) from non-cancerous tissue regions using log Fold Change (logFC).**\n",
      "\n",
      "Gene expression data for non-cancerous cells (n_cells x 18615 genes), example representation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene A</th>\n",
       "      <th>Gene B</th>\n",
       "      <th>Gene C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cell no_cancer 1</th>\n",
       "      <td>2.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cell no_cancer 2</th>\n",
       "      <td>1.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cell no_cancer 3</th>\n",
       "      <td>2.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Gene A  Gene B  Gene C\n",
       "Cell no_cancer 1     2.1     3.0     0.5\n",
       "Cell no_cancer 2     1.8     2.7     0.4\n",
       "Cell no_cancer 3     2.2     3.2     0.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gene expression data for pre-cancerous cells (m_cells x 18615 genes), example representation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene A</th>\n",
       "      <th>Gene B</th>\n",
       "      <th>Gene C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cell cancer 1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cell cancer 2</th>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cell cancer 3</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Gene A  Gene B  Gene C\n",
       "Cell cancer 1     4.0     1.2     0.3\n",
       "Cell cancer 2     3.8     1.5     0.2\n",
       "Cell cancer 3     4.2     1.3     0.4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Compute log Fold Change (logFC):**\n",
      "\n",
      "Gene ranking based on absolute logFC:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene</th>\n",
       "      <th>Mean No Cancer</th>\n",
       "      <th>Mean Cancer</th>\n",
       "      <th>Variance No Cancer</th>\n",
       "      <th>Variance Cancer</th>\n",
       "      <th>Difference Mean</th>\n",
       "      <th>logFC</th>\n",
       "      <th>abs_logFC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gene B</td>\n",
       "      <td>2.97</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.63</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>1.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gene A</td>\n",
       "      <td>2.03</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gene C</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gene  Mean No Cancer  Mean Cancer  Variance No Cancer  Variance Cancer  \\\n",
       "1  Gene B            2.97         1.33                0.06             0.02   \n",
       "0  Gene A            2.03         4.00                0.04             0.04   \n",
       "2  Gene C            0.50         0.30                0.01             0.01   \n",
       "\n",
       "   Difference Mean  logFC  abs_logFC  \n",
       "1             1.63  -1.15       1.15  \n",
       "0             1.97   0.98       0.98  \n",
       "2             0.20  -0.74       0.74  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"**Problem statement: Identify the most discriminative genes that distinguish dysplasia (pre-cancerous) from non-cancerous tissue regions using log Fold Change (logFC).**\\n\")\n",
    "\n",
    "# Examples\n",
    "prediction_cell_ids_no_cancer = pd.DataFrame(\n",
    "    [[2.1, 3.0, 0.5], [1.8, 2.7, 0.4], [2.2, 3.2, 0.6]],\n",
    "    columns=[\"Gene A\", \"Gene B\", \"Gene C\"],\n",
    "    index=[\"Cell no_cancer 1\", \"Cell no_cancer 2\", \"Cell no_cancer 3\"]\n",
    ")\n",
    "\n",
    "prediction_cell_ids_cancer = pd.DataFrame(\n",
    "    [[4.0, 1.2, 0.3], [3.8, 1.5, 0.2], [4.2, 1.3, 0.4]],\n",
    "    columns=[\"Gene A\", \"Gene B\", \"Gene C\"],\n",
    "    index=[\"Cell cancer 1\", \"Cell cancer 2\", \"Cell cancer 3\"]\n",
    ")\n",
    "\n",
    "print(\"Gene expression data for non-cancerous cells (n_cells x 18615 genes), example representation:\")\n",
    "display(prediction_cell_ids_no_cancer)\n",
    "print(\"\\nGene expression data for pre-cancerous cells (m_cells x 18615 genes), example representation:\")\n",
    "display(prediction_cell_ids_cancer)\n",
    "\n",
    "print(\"\\n**Compute log Fold Change (logFC):**\\n\")\n",
    "\n",
    "# Means\n",
    "mean_no_cancer = prediction_cell_ids_no_cancer.mean(axis=0)\n",
    "mean_cancer = prediction_cell_ids_cancer.mean(axis=0)\n",
    "\n",
    "# Log Fold Change\n",
    "# From: https://olvtools.com/en/rnaseq/help/fold-change\n",
    "epsilon = 1e-6  # To avoid division by zero\n",
    "log_fc = np.log2((mean_cancer + epsilon) / (mean_no_cancer + epsilon))\n",
    "\n",
    "# Variance\n",
    "var_no_cancer = prediction_cell_ids_no_cancer.var(axis=0)\n",
    "var_cancer = prediction_cell_ids_cancer.var(axis=0)\n",
    "\n",
    "# Means difference\n",
    "dif_abs_mean = np.abs(mean_no_cancer - mean_cancer)\n",
    "\n",
    "gene_ranking_df = pd.DataFrame({\n",
    "    \"Gene\": prediction_cell_ids_no_cancer.columns,\n",
    "    \"Mean No Cancer\": mean_no_cancer.values,\n",
    "    \"Mean Cancer\": mean_cancer.values,\n",
    "    \"Variance No Cancer\": var_no_cancer.values,\n",
    "    \"Variance Cancer\": var_cancer.values,\n",
    "    \"Difference Mean\": dif_abs_mean.values,\n",
    "    \"logFC\": log_fc.values\n",
    "})\n",
    "\n",
    "# Sort by absolute logFC\n",
    "gene_ranking_df['abs_logFC'] = gene_ranking_df['logFC'].abs()\n",
    "gene_ranking_df = gene_ranking_df.sort_values(by='abs_logFC', ascending=False)\n",
    "\n",
    "print(\"Gene ranking based on absolute logFC:\")\n",
    "display(round(gene_ranking_df, 2))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(gene_ranking_df['Gene'], gene_ranking_df['logFC'], color='dodgerblue', alpha=0.7)\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.title(f\"Genes Ranked by logFC\", fontsize=14)\n",
    "plt.ylabel(\"logFC\", fontsize=12)\n",
    "plt.xlabel(\"Gene\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vd1p0CnExnj5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/catskills/Desktop/broad/broad/lib/python3.10/site-packages/dask/dataframe/__init__.py:31: FutureWarning: The legacy Dask DataFrame implementation is deprecated and will be removed in a future version. Set the configuration option `dataframe.query-planning` to `True` or None to enable the new Dask Dataframe implementation and silence this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "# Core Python Libraries\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import gc\n",
    "import joblib\n",
    "from types import SimpleNamespace\n",
    "from abc import abstractmethod\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import psutil\n",
    "\n",
    "# Core Data manipulation Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization Library\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Spatial Data Processing\n",
    "import spatialdata as sd  # Manage multi-modal spatial omics datasets\n",
    "import anndata as ad  # Manage annotated data matrices in memory and on disk\n",
    "import scanpy as sc  # For analyzing single-cell data, especially for dimensionality reduction and clustering.\n",
    "from skimage.measure import regionprops  # Get region properties of nucleus/cell image from masked nucleus image\n",
    "import h5py  # For handling HDF5 data files\n",
    "import skimage.io\n",
    "\n",
    "# Frameworks for ML and DL models\n",
    "import torch\n",
    "import timm  # timm: A library to load pretrained SOTA computer vision models (e.g. classification, feature extraction, ...)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"ignoring keyword argument 'read_only'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db_767G4nCso"
   },
   "source": [
    "## Utilities for Saving and Reading HDF5 Files\n",
    "\n",
    "This section contains functions for efficiently saving and loading data from HDF5 files, which is a common format for storing large datasets.\n",
    "\n",
    "* `save_hdf5` & `read_assets_from_h5`: Functions for saving and reading datasets and attributes to/from HDF5 files.\n",
    "* `Patcher` class: Extracts image patches from a larger image using given coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTzYKCIfENgW"
   },
   "source": [
    "![patcher](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/competitions/broad-1/quickstarters/resnet50-plus-ridge/images/patcher.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "n4N7eth0Sfss"
   },
   "outputs": [],
   "source": [
    "#####\n",
    "#  Utilities for saving and reading HDF5 files\n",
    "####\n",
    "\n",
    "def save_hdf5(output_fpath, asset_dict, attr_dict=None, mode='a', auto_chunk=True, chunk_size=None):\n",
    "    \"\"\"\n",
    "    Save data and attributes into an HDF5 file, or initialize a new file with the given data.\n",
    "\n",
    "    Parameters:\n",
    "        output_fpath (str): Path to save the HDF5 file.\n",
    "        asset_dict (dict): Dictionary containing keys and their corresponding data (e.g., numpy arrays) to save.\n",
    "        attr_dict (dict, optional): Dictionary of attributes for each key. Format: {key: {attr_key: attr_val, ...}}.\n",
    "        mode (str): File mode ('a' for append, 'w' for write, etc.).\n",
    "        auto_chunk (bool): Whether to enable automatic chunking for HDF5 datasets.\n",
    "        chunk_size (int, optional): If auto_chunk is False, specify the chunk size for the first dimension.\n",
    "\n",
    "    Returns:\n",
    "        str: Path of the saved HDF5 file.\n",
    "    \"\"\"\n",
    "\n",
    "    with h5py.File(output_fpath, mode) as f:\n",
    "        for key, val in asset_dict.items():\n",
    "            data_shape = val.shape\n",
    "            # Ensure data has at least 2 dimensions\n",
    "            if len(data_shape) == 1:\n",
    "                val = np.expand_dims(val, axis=1)\n",
    "                data_shape = val.shape\n",
    "\n",
    "            if key not in f:  # if key does not exist, create a new dataset\n",
    "                data_type = val.dtype\n",
    "\n",
    "                if data_type.kind == 'U':  # Handle Unicode strings\n",
    "                    chunks = (1, 1)\n",
    "                    max_shape = (None, 1)\n",
    "                    data_type = h5py.string_dtype(encoding='utf-8')\n",
    "                else:\n",
    "                    if data_type == np.object_:\n",
    "                        data_type = h5py.string_dtype(encoding='utf-8')\n",
    "                    # Determine chunking strategy\n",
    "                    if auto_chunk:\n",
    "                        chunks = True  # let h5py decide chunk size\n",
    "                    else:\n",
    "                        chunks = (chunk_size,) + data_shape[1:]\n",
    "                    maxshape = (None,) + data_shape[1:]  # Allow unlimited size for the first dimension\n",
    "\n",
    "                try:\n",
    "                    dset = f.create_dataset(key,\n",
    "                                            shape=data_shape,\n",
    "                                            chunks=chunks,\n",
    "                                            maxshape=maxshape,\n",
    "                                            dtype=data_type)\n",
    "                    # Save attributes for the dataset\n",
    "                    if attr_dict is not None:\n",
    "                        if key in attr_dict.keys():\n",
    "                            for attr_key, attr_val in attr_dict[key].items():\n",
    "                                dset.attrs[attr_key] = attr_val\n",
    "                    # Write the data to the dataset\n",
    "                    dset[:] = val\n",
    "                except:\n",
    "                    print(f\"Error encoding {key} of dtype {data_type} into hdf5\")\n",
    "\n",
    "            else:  # Append data to an existing dataset\n",
    "                dset = f[key]\n",
    "                dset.resize(len(dset) + data_shape[0], axis=0)\n",
    "                # assert dset.dtype == val.dtype\n",
    "                dset[-data_shape[0]:] = val\n",
    "\n",
    "    return output_fpath\n",
    "\n",
    "\n",
    "def read_assets_from_h5(h5_path, keys=None, skip_attrs=False, skip_assets=False):\n",
    "    \"\"\"\n",
    "    Read data and attributes from an HDF5 file.\n",
    "\n",
    "    Parameters:\n",
    "        h5_path (str): Path to the HDF5 file.\n",
    "        keys (list, optional): List of keys to read. Reads all keys if None.\n",
    "        skip_attrs (bool): If True, skip reading attributes.\n",
    "        skip_assets (bool): If True, skip reading data assets.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A dictionary of data assets and a dictionary of attributes.\n",
    "    \"\"\"\n",
    "\n",
    "    assets = {}\n",
    "    attrs = {}\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        if keys is None:\n",
    "            keys = list(f.keys())\n",
    "\n",
    "        for key in keys:\n",
    "            if not skip_assets:\n",
    "                assets[key] = f[key][:]\n",
    "            if not skip_attrs and f[key].attrs is not None:\n",
    "                attrs[key] = dict(f[key].attrs)\n",
    "\n",
    "    return assets, attrs\n",
    "\n",
    "\n",
    "class Patcher:\n",
    "\n",
    "    def __init__(self, image, coords, patch_size_target, name=None):\n",
    "        \"\"\"\n",
    "        Initializes the patcher object to extract patches (localized square sub-region of an image) from an image at specified coordinates.\n",
    "\n",
    "        :param image: Input image as a numpy array (H x W x 3), the input image from which patches will be extracted.\n",
    "        :param coords: List or array of cell coordinates (centroïd) [(x1, y1), (x2, y2), ...].\n",
    "        :param patch_size_target: Target size of patches.\n",
    "        :param name: Name of the whole slide image (optional).\n",
    "        \"\"\"\n",
    "\n",
    "        self.image = image\n",
    "        self.height, self.width = image.shape[:2]\n",
    "        self.coords = coords\n",
    "        self.patch_size_target = patch_size_target\n",
    "        self.name = name\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterates over coordinates, yielding image patches and their coordinates.\n",
    "        \"\"\"\n",
    "\n",
    "        for x, y in self.coords:\n",
    "            # Extract patch dimension centered at (x, y)\n",
    "            x_start = max(x - self.patch_size_target // 2, 0)\n",
    "            y_start = max(y - self.patch_size_target // 2, 0)\n",
    "            x_end = min(x_start + self.patch_size_target, self.width)\n",
    "            y_end = min(y_start + self.patch_size_target, self.height)\n",
    "\n",
    "            # Ensure the patch size matches the target size, padding with zeros if necessary\n",
    "            patch = np.zeros((self.patch_size_target, self.patch_size_target, 3), dtype=np.uint8)\n",
    "            patch[:y_end - y_start, :x_end - x_start, :] = self.image[y_start:y_end, x_start:x_end, :]\n",
    "\n",
    "            yield patch, x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of patches based on the number of coordinates.\n",
    "        This is used to determine how many iterations will be done when iterating over the object.\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.coords)\n",
    "\n",
    "    def save_visualization(self, path, vis_width=300, dpi=150):\n",
    "        \"\"\"\n",
    "        Save a visualization of patches overlayed on the tissue H&E image.\n",
    "        This function creates a plot where each patch's location is marked with a rectangle overlaid on the image.\n",
    "\n",
    "        :param path: File path where the visualization will be saved.\n",
    "        :param vis_width: Target width of the visualization in pixels.\n",
    "        :param dpi: Resolution of the saved visualization.\n",
    "        \"\"\"\n",
    "\n",
    "        # Generate the tissue visualization mask\n",
    "        mask_plot = self.image\n",
    "\n",
    "        # Calculate downscale factor for visualization\n",
    "        downscale_vis = vis_width / self.width\n",
    "\n",
    "        # Create a plot\n",
    "        _, ax = plt.subplots(figsize=(self.height / self.width * vis_width / dpi, vis_width / dpi))\n",
    "        ax.imshow(mask_plot)\n",
    "\n",
    "        # Add patches\n",
    "        patch_rectangles = []\n",
    "        for x, y in self.coords:\n",
    "            x_start, y_start = x - self.patch_size_target // 2, y - self.patch_size_target // 2\n",
    "            patch_rectangles.append(Rectangle((x_start, y_start), self.patch_size_target, self.patch_size_target))\n",
    "\n",
    "        # Add rectangles to the plot\n",
    "        ax.add_collection(PatchCollection(patch_rectangles, facecolor='none', edgecolor='black', linewidth=0.3))\n",
    "\n",
    "        ax.set_axis_off()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path, dpi=dpi, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    def view_coord_points(self, vis_width=300, dpi=150):\n",
    "        \"\"\"\n",
    "        Visualizes the coordinates as small points in 2D.\n",
    "        This function generates a scatter plot of the patch coordinates on the H&E image.\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate downscale factor for visualization\n",
    "        downscale_vis = vis_width / self.width\n",
    "\n",
    "        # Create a plot\n",
    "        _, ax = plt.subplots(figsize=(self.height / self.width * vis_width / dpi, vis_width / dpi))\n",
    "        plt.scatter(self.coords[:, 0], -self.coords[:, 1], s=0.2)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    def to_h5(self, path, extra_assets={}):\n",
    "        \"\"\"\n",
    "        Saves the extracted patches and their associated information to an HDF5 file.\n",
    "\n",
    "        Each patch is saved as a dataset along with its coordinates and any additional assets (extra_assets).\n",
    "        The HDF5 file is structured with a dataset for the image patch ('img') and coordinates ('coords').\n",
    "\n",
    "        :param path: File path where the HDF5 file will be saved.\n",
    "        :param extra_assets: Dictionary of additional assets to save (optional). Each value in extra_assets must have the same length as the patches.\n",
    "        \"\"\"\n",
    "\n",
    "        mode_HE = 'w'  # Start with write mode for the first patch\n",
    "        i = 0\n",
    "\n",
    "        # Check that the extra_assets match the number of patches\n",
    "        if extra_assets:\n",
    "            for _, value in extra_assets.items():\n",
    "                if len(value) != len(self):\n",
    "                    raise ValueError(\"Each value in extra_assets must have the same length as the patcher object.\")\n",
    "\n",
    "        # Ensure the file has the correct extension\n",
    "        if not (path.endswith('.h5') or path.endswith('.h5ad')):\n",
    "            path = path + '.h5'\n",
    "\n",
    "        # Loop through each patch and save it to the HDF5 file (loop through __iter__ function)\n",
    "        for tile, x, y in tqdm(self):\n",
    "            assert tile.shape == (self.patch_size_target, self.patch_size_target, 3)\n",
    "\n",
    "            # Prepare the data to be saved for this patch\n",
    "            asset_dict = {\n",
    "                'img': np.expand_dims(tile, axis=0),  # Shape (1, h, w, 3)\n",
    "                'coords': np.expand_dims([x, y], axis=0)  # Shape (1, 2)\n",
    "            }\n",
    "\n",
    "            # Add any extra assets to the asset dictionary\n",
    "            extra_asset_dict = {key: np.expand_dims([value[i]], axis=0) for key, value in extra_assets.items()}\n",
    "            asset_dict = {**asset_dict, **extra_asset_dict}\n",
    "\n",
    "            # Define the attributes for the image patch\n",
    "            attr_dict = {'img': {'patch_size_target': self.patch_size_target}}\n",
    "\n",
    "            if self.name is not None:\n",
    "                attr_dict['img']['name'] = self.name\n",
    "\n",
    "            # Save the patch data to the HDF5 file\n",
    "            save_hdf5(path, asset_dict, attr_dict, mode=mode_HE, auto_chunk=False, chunk_size=1)\n",
    "            mode_HE = 'a'  # Switch to append mode after the first patch\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oneR4KRvpMbS"
   },
   "source": [
    "## Preprocessing Spatial Transcriptomics Data\n",
    "\n",
    "This section contains functions for preprocessing spatial transcriptomics data, including extracting spatial coordinates, generating image patches and preparing datasets for training and testing.\n",
    "\n",
    "* **`extract_spatial_positions`**: Extracts spatial coordinates (centroids) of cells.\n",
    "* **`process_and_visualize_image`**: Extracts square image patches from H&E images and visualizes them.\n",
    "* **`preprocess_spatial_transcriptomics_data_train`**: Prepares training data by generating gene expression (Y) and image patch datasets (X).\n",
    "* **`preprocess_spatial_transcriptomics_data_test`**: Prepares test data by generating image patches (X) for selected cells.\n",
    "* **`create_cross_validation_splits`**: Creates leave-one-out cross-validation splits for model evaluation.\n",
    "\n",
    "![data_X_Y](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/competitions/broad-1/quickstarters/resnet50-plus-ridge/images/data_X_Y.png)\n",
    "\n",
    "`Leave-one-out cross-validation schema:`\n",
    "![cross_validation](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/competitions/broad-1/quickstarters/resnet50-plus-ridge/images/cross_validation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DRqmAKSqYtJS"
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# Preprocessing spatial transcriptomics data\n",
    "##########\n",
    "\n",
    "def extract_spatial_positions(sdata, cell_id_list):\n",
    "    \"\"\"\n",
    "    Extracts spatial positions (centroids) of regions from the nucleus image where cell IDs match the provided cell list.\n",
    "\n",
    "    Need to use 'HE_nuc_original' to extract spatial coordinate of cells\n",
    "    HE_nuc_original: The nucleus segmentation mask of H&E image, in H&E native coordinate system. The cell_id in this segmentation mask matches with the nuclei by gene matrix stored in anucleus.\n",
    "    HE_nuc_original is like a binary segmentation mask 0 - 1 but replace 1 with cell_ids.\n",
    "    You can directly find the location of a cell, with cell_id, through HE_nuc_original==cell_id\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    sdata: SpatialData\n",
    "        A spatial data object containing the nucleus segmentation mask ('HE_nuc_original').\n",
    "    cell_id_list: array-like\n",
    "        A list or array of cell IDs to filter the regions.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        A NumPy array of spatial coordinates (x_center, y_center) for matched regions.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Extracting spatial positions ...\")\n",
    "    # Get region properties from the nucleus image: for each cell_id get its location on HE image\n",
    "    if \"tif_HE_nuc\" in sdata:\n",
    "        regions = regionprops(sdata['tif_HE_nuc'])\n",
    "    else:\n",
    "        regions = regionprops(sdata['HE_nuc_original'][0, :, :].to_numpy())\n",
    "\n",
    "    dict_spatial_positions = {}\n",
    "    # Loop through each region and extract centroid if the cell ID matches\n",
    "    for props in tqdm(regions):\n",
    "        cell_id = props.label\n",
    "        centroid = props.centroid\n",
    "        # Extract only coordinates from the provided cell_id list\n",
    "        if cell_id in cell_id_list:\n",
    "            y_center, x_center = int(centroid[0]), int(centroid[1])\n",
    "            dict_spatial_positions[cell_id] = [x_center, y_center]\n",
    "\n",
    "    # To maintain cell IDs order\n",
    "    spatial_positions = []\n",
    "    for cell_id in cell_id_list:\n",
    "        try:\n",
    "            spatial_positions.append(dict_spatial_positions[cell_id])\n",
    "        except KeyError:\n",
    "            print(f\"Warning: Cell ID {cell_id} not found in the segmentation mask.\")\n",
    "            spatial_positions.append([1000, 1000])\n",
    "\n",
    "    return np.array(spatial_positions)\n",
    "\n",
    "\n",
    "def process_and_visualize_image(sdata, patch_save_dir, name_data, coords_center, target_patch_size, barcodes,\n",
    "                                show_extracted_images=False, vis_width=1000):\n",
    "    \"\"\"\n",
    "    Load and process the spatial image data, creates patches, saves them in an HDF5 file,\n",
    "    and visualizes the extracted images and spatial coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    sdata: SpatialData\n",
    "        A spatial data object containing the image to process ('HE_original') and associated metadata.\n",
    "    patch_save_dir: str\n",
    "        Directory where the resulting HDF5 file and visualizations will be saved.\n",
    "    name_data: str\n",
    "        Name used for saving the dataset.\n",
    "    coords_center: array-like\n",
    "        Coordinates of the regions to be patched (centroids of cell regions).\n",
    "    target_patch_size: int\n",
    "        Size of the patches to extract from the image.\n",
    "    barcodes: array-like\n",
    "        Barcodes associated with patches.\n",
    "    show_extracted_images: bool, optional (default=False)\n",
    "        If True, will show extracted images during the visualization phase.\n",
    "    vis_width: int, optional (default=1000)\n",
    "        Width of the visualization images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the image and transpose it to the correct format\n",
    "    print(\"Loading imgs ...\")\n",
    "    if 'tif_HE' in sdata:\n",
    "        intensity_image = sdata['tif_HE'].copy()\n",
    "    else:\n",
    "        intensity_image = np.transpose(sdata['HE_original'].to_numpy(), (1, 2, 0))\n",
    "\n",
    "    print(\"Patching: create image dataset (X) ...\")\n",
    "    # Path for the .h5 image dataset\n",
    "    h5_path = os.path.join(patch_save_dir, name_data + '.h5')\n",
    "\n",
    "    # Create the patcher object to extract patches (localized square sub-region of an image) from an image at specified coordinates.\n",
    "    patcher = Patcher(\n",
    "        image=intensity_image,\n",
    "        coords=coords_center,\n",
    "        patch_size_target=target_patch_size\n",
    "    )\n",
    "\n",
    "    # Build and Save patches to an HDF5 file\n",
    "    patcher.to_h5(h5_path, extra_assets={'barcode': barcodes})\n",
    "\n",
    "    # Visualization\n",
    "    print(\"Visualization\")\n",
    "    if show_extracted_images:\n",
    "        print(\"Extracted Images (high time and memory consumption...)\")\n",
    "        patcher.save_visualization(os.path.join(patch_save_dir, name_data + '_viz.png'), vis_width=vis_width)\n",
    "\n",
    "    print(\"Spatial coordinates\")\n",
    "    patcher.view_coord_points(vis_width=vis_width)\n",
    "\n",
    "    # Display some example images from the created dataset\n",
    "    print(\"Examples from the created .h5 dataset\")\n",
    "    assets, _ = read_assets_from_h5(h5_path)\n",
    "\n",
    "    n_images = 3\n",
    "    fig, axes = plt.subplots(1, n_images, figsize=(15, 5))\n",
    "    for i in range(n_images):\n",
    "        axes[i].imshow(assets[\"img\"][i])\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Delete variables that are no longer used\n",
    "    del intensity_image, patcher, assets\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def preprocess_spatial_transcriptomics_data_test(name_data, sdata, cell_id_list, dir_processed_dataset, target_patch_size=32, vis_width=1000, show_extracted_images=False):\n",
    "    \"\"\"\n",
    "    Test step: Preprocesses spatial transcriptomics data by performing the following steps for the selected ST data:\n",
    "    1. Extract spatial coordinates of the selected cells.\n",
    "    2. Generates and saves patches of images centered on spatial coordinates to HDF5 files (X) into directory 'patches'.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    name_data: str\n",
    "        Name used for saving the dataset.\n",
    "    sdata: SpatialData\n",
    "        A spatial data object containing the image to process ('HE_original') and associated metadata.\n",
    "    cell_id_list : array-like\n",
    "        A list or array of cell IDs to filter the regions.\n",
    "    dir_processed_dataset: str\n",
    "        Path to the directory where processed datasets and outputs will be saved.\n",
    "    target_patch_size: int, optional\n",
    "        Target size of image patches to extract.\n",
    "    vis_width: int, optional\n",
    "        Width of the visualization output for spatial and image patches.\n",
    "    show_extracted_images: bool\n",
    "    \"\"\"\n",
    "\n",
    "    # Creates directories for saving patches ('patches')\n",
    "    patch_save_dir = os.path.join(dir_processed_dataset, \"patches\")\n",
    "    os.makedirs(patch_save_dir, exist_ok=True)\n",
    "\n",
    "    print(\"\\n -- PREPROCESS SPATIAL TRANSCRIPTOMICS DATASET --------------------------------------------\\n\")\n",
    "\n",
    "    # Extract spatial positions for selected cells\n",
    "    new_spatial_coord = extract_spatial_positions(sdata, cell_id_list)\n",
    "\n",
    "    # Spatial coordinates and barcodes (cell IDs) for the patches\n",
    "    coords_center = new_spatial_coord\n",
    "    barcodes = np.array(['x' + str(i).zfill(6) for i in list(cell_id_list)])  # Trick to set all index to same length to avoid problems when saving to h5\n",
    "\n",
    "    # Generate and visualize image patches centered around spatial coordinates ({name_data}.h5 file in directory os.path.join(dir_processed_dataset, \"patches\"))\n",
    "    process_and_visualize_image(sdata, patch_save_dir, name_data, coords_center, target_patch_size, barcodes,\n",
    "                                show_extracted_images=False, vis_width=1000)\n",
    "\n",
    "    print(\"\\nPreprocess dataset DONE\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tBRiWNowz6b"
   },
   "source": [
    "## Encoder functions\n",
    "\n",
    "Provide the structure for building and utilizing pre-trained models specifically designed for extracting features from images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MBn6-RVJ4THe"
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# Encoder functions\n",
    "##########\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "class InferenceEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Abstract base class for building inference encoders.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    weights_path : str or None\n",
    "        Path to the model weights (optional).\n",
    "    model : torch.nn.Module\n",
    "        The model architecture.\n",
    "    eval_transforms : callable\n",
    "        Evaluation transformations applied to the input images.\n",
    "    precision : torch.dtype\n",
    "        The data type of the model's parameters and inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights_path=None, **build_kwargs):\n",
    "        super(InferenceEncoder, self).__init__()\n",
    "\n",
    "        self.weights_path = weights_path\n",
    "        self.model, self.eval_transforms, self.precision = self._build(weights_path, **build_kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.model(x)\n",
    "        return z\n",
    "\n",
    "    @abstractmethod\n",
    "    def _build(self, **build_kwargs):\n",
    "        pass\n",
    "\n",
    "\n",
    "def get_eval_transforms(mean, std):\n",
    "    \"\"\"\n",
    "    Creates the evaluation transformations for preprocessing images. This includes\n",
    "    converting the images to tensor format and normalizing them with given mean and std.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    mean : list\n",
    "        The mean values used for normalization.\n",
    "    std : list\n",
    "        The standard deviation values used for normalization.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    transforms.Compose\n",
    "        A composed transformation function that applies the transformations in sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    trsforms = []\n",
    "\n",
    "    # Convert image to tensor\n",
    "    trsforms.append(lambda img: TF.to_tensor(img))\n",
    "\n",
    "    if mean is not None and std is not None:\n",
    "        # Normalize the image\n",
    "        trsforms.append(lambda img: TF.normalize(img, mean, std))\n",
    "\n",
    "    return transforms.Compose(trsforms)\n",
    "\n",
    "\n",
    "class ResNet50InferenceEncoder(InferenceEncoder):\n",
    "    \"\"\"\n",
    "    A specific implementation of the InferenceEncoder class for ResNet50.\n",
    "    This encoder is used to extract features from images using a pretrained ResNet50 model.\n",
    "    \"\"\"\n",
    "\n",
    "    def _build(\n",
    "        self,\n",
    "        weights_root=\"resnet50.tv_in1k\",\n",
    "        timm_kwargs={\"features_only\": True, \"out_indices\": [3], \"num_classes\": 0},\n",
    "        pool=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Build the ResNet50 model and load its weights. It supports both pretrained models\n",
    "        from the internet and pretrained models from a given weights path (offline).\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        weights_root : str\n",
    "            Path to pretrained model weights. Defaults to \"resnet50.tv_in1k\" (if online).\n",
    "        timm_kwargs : dict\n",
    "            Additional arguments for creating the ResNet50 model via the timm library.\n",
    "        pool : bool\n",
    "            Whether to apply adaptive average pooling to the output of the model. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            A tuple containing the ResNet50 model, the evaluation transformations and the precision type.\n",
    "        \"\"\"\n",
    "\n",
    "        if weights_root == \"resnet50.tv_in1k\":\n",
    "            pretrained = True\n",
    "            print(\"Load pretrained Resnet50 from internet\")\n",
    "        else:\n",
    "            pretrained = False\n",
    "            print(f\"Load pretrained Resnet50 offline from weights path: {weights_root}\")\n",
    "\n",
    "        # Build the model using the timm library\n",
    "        model = timm.create_model(\"resnet50.tv_in1k\", pretrained=pretrained, **timm_kwargs)\n",
    "\n",
    "        # If not using a pretrained model, load weights from the specified path\n",
    "        if not pretrained and os.path.exists(weights_root):\n",
    "            # Load the weights\n",
    "            checkpoint = torch.load(weights_root, map_location='cpu', weights_only=True)  # or 'cuda' if using GPU\n",
    "\n",
    "            # Remove the classifier layers from the checkpoint\n",
    "            model_state_dict = model.state_dict()\n",
    "            checkpoint = {k: v for k, v in checkpoint.items() if k in model_state_dict}\n",
    "\n",
    "            # Load the weights into the model\n",
    "            model.load_state_dict(checkpoint, strict=False)\n",
    "        elif not pretrained:\n",
    "            # Issue a warning if the weights file is missing\n",
    "            print(f\"\\n!!! WARNING: The specified weights file '{weights_root}' does not exist. The model will be initialized with random weights.\\n\")\n",
    "\n",
    "        imagenet_mean, imagenet_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "        eval_transform = get_eval_transforms(imagenet_mean, imagenet_std)\n",
    "        precision = torch.float32\n",
    "        if pool:\n",
    "            self.pool = torch.nn.AdaptiveAvgPool2d(1)\n",
    "        else:\n",
    "            self.pool = None\n",
    "\n",
    "        return model, eval_transform, precision\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.forward_features(x)\n",
    "        if self.pool:\n",
    "            out = self.pool(out).squeeze(-1).squeeze(-1)\n",
    "        return out\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        out = self.model(x)\n",
    "        if isinstance(out, list):\n",
    "            assert len(out) == 1\n",
    "            out = out[0]\n",
    "        return out\n",
    "\n",
    "\n",
    "def inf_encoder_factory(enc_name):\n",
    "    \"\"\"\n",
    "    Factory function to instantiate an encoder based on the specified name.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    enc_name : str\n",
    "        The name of the encoder model to instantiate (e.g., 'resnet50').\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    class\n",
    "        The encoder class corresponding to the specified encoder name.\n",
    "    \"\"\"\n",
    "\n",
    "    if enc_name == 'resnet50':\n",
    "        return ResNet50InferenceEncoder\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown encoder name {enc_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_gewG-TqZL_"
   },
   "source": [
    "## Torch Dataset & Embeddings\n",
    "\n",
    "This section defines a custom torch dataset for handling spatial transcriptomics data and generating embeddings for image patches.\n",
    "\n",
    "* **`H5Dataset`**: A torch `Dataset` class for loading spatial transcriptomics data from an HDF5 file. It loads image patches, their associated barcodes/cells and coordinates in batches to efficiently handle large datasets.\n",
    "* **`generate_embeddings`**: A utility for generating embeddings from images  and saving them to an HDF5 file. It handles creating a `DataLoader`, running the model in evaluation mode and saving embeddings to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "g3pPQPc6EKEq"
   },
   "outputs": [],
   "source": [
    "#########\n",
    "# Torch Dataset & Embeddings\n",
    "#########\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class H5Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset to read ST + H&E from an HDF5 (.h5) file\n",
    "    The dataset loads images and their associated barcodes/cells and coordinates in chunks for efficient data handling.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    h5_path : str\n",
    "        Path to the HDF5 file containing the images, barcodes and coordinates.\n",
    "    img_transform : callable, optional\n",
    "        A transformation function to apply to the images. Defaults to None.\n",
    "    chunk_size : int, optional\n",
    "        Number of items to load per batch. Defaults to 1000.\n",
    "    n_chunks : int\n",
    "        The total number of chunks, calculated based on the size of the 'barcode' array.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h5_path, img_transform=None, chunk_size=1000):\n",
    "        self.h5_path = h5_path\n",
    "        self.img_transform = img_transform\n",
    "        self.chunk_size = chunk_size\n",
    "        with h5py.File(h5_path, 'r') as f:\n",
    "            self.n_chunks = int(np.ceil(len(f['barcode']) / chunk_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_chunks\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetches a batch of data (images, barcodes and coordinates) from the HDF5 file.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        idx : int\n",
    "            The index of the chunk to fetch.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            A dictionary containing the images, barcodes and coordinates for the specified chunk.\n",
    "        \"\"\"\n",
    "\n",
    "        start_idx = idx * self.chunk_size\n",
    "        end_idx = (idx + 1) * self.chunk_size\n",
    "        # Open the HDF5 file and load the specific chunk of data\n",
    "        with h5py.File(self.h5_path, 'r') as f:\n",
    "            imgs = f['img'][start_idx:end_idx]\n",
    "            barcodes = f['barcode'][start_idx:end_idx].flatten().tolist()\n",
    "            coords = f['coords'][start_idx:end_idx]\n",
    "\n",
    "        # Apply image transformations if any (e.g. to Torch and normalization)\n",
    "        if self.img_transform:\n",
    "            imgs = torch.stack([self.img_transform(img) for img in imgs])\n",
    "\n",
    "        return {'imgs': imgs, 'barcodes': barcodes, 'coords': coords}\n",
    "\n",
    "\n",
    "def post_collate_fn(batch):\n",
    "    \"\"\" Post collate function to clean up batch \"\"\"\n",
    "\n",
    "    if batch[\"imgs\"].dim() == 5:\n",
    "        assert batch[\"imgs\"].size(0) == 1\n",
    "        batch[\"imgs\"] = batch[\"imgs\"].squeeze(0)\n",
    "\n",
    "    if batch[\"coords\"].dim() == 3:\n",
    "        assert batch[\"coords\"].size(0) == 1\n",
    "        batch[\"coords\"] = batch[\"coords\"].squeeze(0)\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "def embed_tiles(dataloader, model: torch.nn.Module, embedding_save_path: str, device: str, precision):\n",
    "    \"\"\"\n",
    "    Extracts embeddings from image tiles using the specified model and saves them to an HDF5 file.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataloader : torch.utils.data.DataLoader\n",
    "        DataLoader providing the batches of image tiles.\n",
    "    model : torch.nn.Module\n",
    "        The model used to generate embeddings from the tiles.\n",
    "    embedding_save_path : str\n",
    "        Path where the generated embeddings will be saved.\n",
    "    device : str\n",
    "        The device to run the model on (e.g., 'cuda' or 'cpu').\n",
    "    precision : torch.dtype\n",
    "        The precision (data type) to use for inference (e.g., float16 for mixed precision).\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    # Iterate over the batches in the DataLoader\n",
    "    for batch_idx, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        batch = post_collate_fn(batch)\n",
    "        imgs = batch['imgs'].to(device).float()\n",
    "        # Apply model on images\n",
    "        with torch.inference_mode():\n",
    "            if torch.cuda.is_available():  # Use mixed precision only if CUDA is available\n",
    "                with torch.amp.autocast('cuda', dtype=precision):\n",
    "                    embeddings = model(imgs)\n",
    "            else:  # No mixed precision on CPU\n",
    "                embeddings = model(imgs)\n",
    "\n",
    "        # Set mode to 'w' for the first batch, 'a' for appending subsequent batches\n",
    "        mode = 'w' if batch_idx == 0 else 'a'\n",
    "\n",
    "        # Create a dictionary with embeddings and other relevant data to save\n",
    "        asset_dict = {'embeddings': embeddings.cpu().numpy()}\n",
    "        asset_dict.update({key: np.array(val) for key, val in batch.items() if key != 'imgs'})\n",
    "\n",
    "        # Save the embeddings to the HDF5 file\n",
    "        save_hdf5(embedding_save_path, asset_dict=asset_dict, mode=mode)\n",
    "\n",
    "    return embedding_save_path\n",
    "\n",
    "\n",
    "def generate_embeddings(embed_path, encoder, device, tile_h5_path, batch_size, num_workers, overwrite=False):\n",
    "    \"\"\"\n",
    "    Generate embeddings for images and save to a specified path.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    embed_path : str\n",
    "        Path to save the embeddings.\n",
    "    encoder : torch.nn.Module\n",
    "        The encoder model for generating embeddings.\n",
    "    device : torch.device\n",
    "        Device to use for computation (e.g., 'cuda' or 'cpu').\n",
    "    tile_h5_path : str\n",
    "        Path to the HDF5 file containing images.\n",
    "    batch_size : int\n",
    "        Batch size for the DataLoader.\n",
    "    num_workers : int\n",
    "        Number of worker threads for data loading.\n",
    "    overwrite : bool, optional\n",
    "        If True, overwrite existing embeddings. Default is False.\n",
    "    \"\"\"\n",
    "\n",
    "    # If the embeddings file doesn't exist or overwrite is True, proceed to generate embeddings\n",
    "    if not os.path.isfile(embed_path) or overwrite:\n",
    "        print(f\"Generating embeddings for {embed_path} ...\")\n",
    "\n",
    "        # Set encoder to evaluation mode and move it to the device\n",
    "        encoder.eval()\n",
    "        encoder.to(device)\n",
    "\n",
    "        # Create dataset and dataloader for tiles\n",
    "        tile_dataset = H5Dataset(tile_h5_path, chunk_size=batch_size, img_transform=encoder.eval_transforms)\n",
    "        tile_dataloader = torch.utils.data.DataLoader(\n",
    "            tile_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "\n",
    "        # Generate and save embeddings\n",
    "        embed_tiles(tile_dataloader, encoder, embed_path, device, encoder.precision)\n",
    "\n",
    "        del tile_dataset, tile_dataloader\n",
    "        gc.collect()\n",
    "    else:\n",
    "        print(f\"Skipping embedding {os.path.basename(embed_path)} as it already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hT_2qag7gXBO"
   },
   "source": [
    "## Utility functions used in Crunch 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0OSDlFhPEx3P"
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# functions for inference (crunch 1)\n",
    "##########\n",
    "\n",
    "def embedding_and_load_data(name_data, dir_processed_dataset_test, test_embed_dir, args, device):\n",
    "    \"\"\"\n",
    "    Embedding of the images using the specified encoder and load the resulting data.\n",
    "\n",
    "    Args:\n",
    "    - name_data (str): The name of the data to process.\n",
    "    - dir_processed_dataset_test (str): Directory where the processed test dataset is stored.\n",
    "    - test_embed_dir (str): Directory where the embeddings should be saved.\n",
    "    - args (namespace): Arguments object containing parameters like encoder, batch_size, etc.\n",
    "    - device (torch.device): The device (CPU or GPU) to perform computations on.\n",
    "\n",
    "    Returns:\n",
    "    - assets (dict): Dictionary containing the 'barcodes', 'coords' and 'embeddings' from the embedded data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Print the encoder being used\n",
    "    print(f\"Embedding images using {args.encoder} encoder\")\n",
    "\n",
    "    # Create encoder based on the specified model and load its weights\n",
    "    encoder = inf_encoder_factory(args.encoder)(args.weights_root)\n",
    "\n",
    "    # Define the path for the patches data (h5 file)\n",
    "    tile_h5_path = os.path.join(dir_processed_dataset_test, \"patches\", f'{name_data}.h5')\n",
    "\n",
    "    # Check if the file exists\n",
    "    assert os.path.isfile(tile_h5_path), f\"Patches h5 file not found at {tile_h5_path}\"\n",
    "\n",
    "    # Define the embedding output path\n",
    "    embed_path = os.path.join(test_embed_dir, f'{name_data}.h5')\n",
    "\n",
    "    # Generate the embeddings and save them to the defined path\n",
    "    generate_embeddings(embed_path, encoder, device, tile_h5_path, args.batch_size, args.num_workers, overwrite=args.overwrite)\n",
    "\n",
    "    # Load the embeddings and related assets\n",
    "    assets, _ = read_assets_from_h5(embed_path)\n",
    "\n",
    "    # Extract cell IDs and convert to a list of strings\n",
    "    # The cell IDs are not necessary because the images are kept in the same order as the gene expression data\n",
    "    cell_ids = assets['barcodes'].flatten().astype(str).tolist()\n",
    "\n",
    "    del encoder\n",
    "    gc.collect()\n",
    "\n",
    "    return assets\n",
    "\n",
    "\n",
    "def load_models_from_directories(base_path):  # Updated\n",
    "    \"\"\"\n",
    "    Load all trained regression models (one model for each cross-validation split)\n",
    "    Load 'model.pkl' from each directory within the base_path.\n",
    "\n",
    "    :param base_path: The parent directory containing split subdirectories.\n",
    "    :return: A dictionary where keys are directory names and values are the loaded models.\n",
    "    \"\"\"\n",
    "\n",
    "    models = {}\n",
    "    for name in os.listdir(base_path):\n",
    "        model_path = os.path.join(base_path, name)\n",
    "        if model_path.endswith(\".pkl\"):\n",
    "            models[name] = joblib.load(model_path)\n",
    "            print(f\"Loaded model from {model_path}\")\n",
    "    if len(models) == 0:\n",
    "        print(f\"NO .pkl MODEL FOUND IN {base_path}. A RANDOM PREDICTION WILL BE PERFORMED !\")\n",
    "    return models\n",
    "\n",
    "\n",
    "def predict_and_aggregate_models(X_test, results_dir, gene_460_names):\n",
    "    \"\"\"\n",
    "    Load models from the given directory, make predictions on the test set (X_test),\n",
    "    aggregate the predictions by averaging and set negative predictions to 0.\n",
    "\n",
    "    Args:\n",
    "    - X_test (np.array): The test data to make predictions on.\n",
    "    - results_dir (str): Directory containing the saved models.\n",
    "    - gene_460_names (list): List of gene names to predict\n",
    "\n",
    "    Returns:\n",
    "    - np.array: The aggregated predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load models from the specified directory\n",
    "    models = load_models_from_directories(results_dir)\n",
    "\n",
    "    if len(models) == 0:\n",
    "        # random predictions if no model found\n",
    "        return np.random.rand(len(X_test), len(gene_460_names))\n",
    "\n",
    "    # Initialize a list to store predictions\n",
    "    predictions = []\n",
    "\n",
    "    # Iterate through each model and make predictions\n",
    "    for split_name in models.keys():\n",
    "        preds = models[split_name].predict(X_test)\n",
    "        predictions.append(preds)\n",
    "\n",
    "    # Stack the predictions into a 2D array (models x samples)\n",
    "    predictions = np.stack(predictions)\n",
    "\n",
    "    # Aggregate predictions by calculating the mean across all models\n",
    "    average_predictions = np.mean(predictions, axis=0)\n",
    "\n",
    "    # Set any negative predictions to 0\n",
    "    average_predictions = np.where(average_predictions < 0, 0.0, average_predictions)\n",
    "\n",
    "    del models\n",
    "\n",
    "    return average_predictions\n",
    "\n",
    "\n",
    "def print_memory_usage(label):\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"({label}: Total={mem.total/1e9:.2f} GB, Available={mem.available/1e9:.2f} GB, Used={mem.used/1e9:.2f} GB, Percent={mem.percent}%)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6Vd4lABak7_D"
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# functions for inference (crunch 2)\n",
    "##########\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def find_matches_cos_similarity(spot_vectors, query_vectors, top_k=1):\n",
    "    \"\"\"\n",
    "    Find the top-k most similar scRNA-Seq cells for each Xenium cell using cosine similarity.\n",
    "\n",
    "    Args:\n",
    "        spot_vectors (array): 2D array of gene expression of scRNA-Seq cells.\n",
    "        query_vectors (array): 2D array of gene expression of Xenium cells.\n",
    "        top_k (int): Number of top matches to return.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Indices and similarity values of top-k matches.\n",
    "    \"\"\"\n",
    "\n",
    "    # Use PyTorch functionalities for efficiency and scalability\n",
    "    # Normalize the vectors\n",
    "    spot_vectors = F.normalize(torch.tensor(spot_vectors, dtype=torch.float32), p=2, dim=-1)\n",
    "    query_vectors = F.normalize(torch.tensor(query_vectors, dtype=torch.float32), p=2, dim=-1)\n",
    "\n",
    "    # Compute dot product (cosine similarity because vectors are first normalized to unit norm)\n",
    "    dot_similarity = query_vectors @ spot_vectors.T\n",
    "\n",
    "    # Find the top_k similar spots for each query\n",
    "    values, indices = torch.topk(dot_similarity, k=top_k, dim=-1)\n",
    "    return indices.cpu().numpy(), values.cpu().numpy()\n",
    "\n",
    "\n",
    "def log1p_normalization_scale_factor(arr, scale_factor=10000):\n",
    "    row_sums = np.sum(arr, axis=1, keepdims=True)\n",
    "    row_sums = np.where(row_sums == 0, 1, row_sums)  # Avoid division by zero\n",
    "    return np.log1p((arr / row_sums) * scale_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6R_RAbOtxHh"
   },
   "source": [
    "## Crunch 1 & 2 Inference function\n",
    "\n",
    "The `infer_crunch_1` and `infer_crunch_2` functions load locally trained models from your Crunch 1 & 2 works and now perform inference on new datasets.\n",
    "\n",
    "**Crunch 1:**\n",
    "1.   Prepare the necessary directories and load the configuration parameters from the previously trained model.\n",
    "2.   Provide a dataset as a dict containing at least an H&E image of tissue regions (`HE`) and a nuclear segmentation mask (`HE_nuc`).\n",
    "3.   Preprocess the data into image patches (X_test).\n",
    "4.   **Load locally `pytorch_model.bin`** in `./resources` to generate embeddings for the dataset\n",
    "5.   **Load locally the trained regression models** (`config.json` and `.pkl` files from Crunch 1 in `./resources`) to apply regression predictions on the 460 measured genes.\n",
    "\n",
    "`pytorch_model.bin`, `config.json` and baseline regression model from Crunch 1 quickstarter are available on Hugging Face: https://huggingface.co/Tarandros/Autoimmune_Disease_Machine_Learning_Crunch_1/tree/main (Use your own config file and model if you can)\n",
    "\n",
    "**Note: You can also manage this Crunch 3 challenge without Crunch 1 & 2: \"For those who did not participate in Crunch 1 and 2, you can design\n",
    "a gene panel from scratch using biological understanding or other approaches.\"**\n",
    "\n",
    "![train_architecture](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/competitions/broad-1/quickstarters/resnet50-plus-ridge/images/train_architecture.png)\n",
    "\n",
    "**Crunch 2:**\n",
    "\n",
    "5.   Apply cosine similarities between predictions and single-cell RNA sequencing from shared genes.\n",
    "6.   Compute weighted average of unmeasured genes from scRNA-Seq based on similarity scores.\n",
    "7.   Return the predictions of the 18615 genes (458 shared genes + 18157 unmeasured genes).\n",
    "\n",
    "![similarity_short_example](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/competitions/broad-2/quickstarters/cosine-similarity/images/similarity_short_example.png)\n",
    "\n",
    "**Note: You can also manage this Crunch 3 challenge without Crunch 1 & 2: \"For those who did not participate in Crunch 1 and 2, you can design\n",
    "a gene panel from scratch using biological understanding or other approaches.\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UXjf0yinpeAl"
   },
   "outputs": [],
   "source": [
    "def infer_crunch_1(\n",
    "    name_data: str,  # The name of the dataset being processed (only used for logging and directory naming)\n",
    "    data_file_path: str,\n",
    "    model_directory_path: str,  # Path to save the trained model and results\n",
    "    sdata: dict,  # a dataset as a dict containing at least an H&E image of tissue regions and a nuclear segmentation mask\n",
    "    cell_ids: list,  # List of cell identifiers to predict gene expression\n",
    "    gene_460_names: list,  # List of 460 gene names for which predictions will be made\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform Crunch 1 inference: Predict the expression of 460 genes in spatial transcriptomics data\n",
    "    using a pre-trained Resnet50 model and regression inference.\n",
    "    \"\"\"\n",
    "\n",
    "    ### Prepare Directories ###\n",
    "    print(f\"\\n-- {name_data} INFERENCE ---------------------------------------------------------------\\n\")\n",
    "\n",
    "    # Previous Directory to store models and results\n",
    "    dir_models_and_results = model_directory_path  # updated\n",
    "    # Load training configuration parameters\n",
    "    config_path = os.path.join(dir_models_and_results, \"config.json\")\n",
    "    with open(config_path, 'r') as f:\n",
    "        args_dict = json.load(f)\n",
    "    args = SimpleNamespace(**args_dict)\n",
    "\n",
    "    args.results_dir = dir_models_and_results  # updated\n",
    "\n",
    "    # Directory for processed test dataset (temporary storage)\n",
    "    dir_processed_dataset_test = os.path.join(\"/tmp\", f\"processed_dataset_test\")\n",
    "    os.makedirs(dir_processed_dataset_test, exist_ok=True)\n",
    "\n",
    "    # Directory to store the test data embeddings (temporary storage)\n",
    "    test_embed_dir = os.path.join(dir_processed_dataset_test, \"ST_data_emb\")\n",
    "    os.makedirs(test_embed_dir, exist_ok=True)\n",
    "\n",
    "    # Set device to GPU if available, else use CPU!!\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ##################################################################\n",
    "    ##### BROAD Crunch 1: predict the expression of 460 genes #####\n",
    "    print(f\"\\n** {name_data} Predict the expression of 460 genes (Crunch 1) ****************************************************************\\n\")\n",
    "\n",
    "    ### Preprocess and Embedding Data + Regression inference ###\n",
    "\n",
    "    # Preprocess the test data for embedding (patch extraction)\n",
    "    preprocess_spatial_transcriptomics_data_test(name_data, sdata, cell_ids, dir_processed_dataset_test,\n",
    "                                                 args.target_patch_size, args.vis_width, args.show_extracted_images)\n",
    "\n",
    "    print(f\"\\n--{name_data} EMBEDDING--\\n\")\n",
    "    # Generate and load the embeddings for the test data\n",
    "    assets = embedding_and_load_data(name_data, dir_processed_dataset_test, test_embed_dir, args, device)\n",
    "\n",
    "    # Extract embeddings features for prediction\n",
    "    X_test = assets[\"embeddings\"]\n",
    "    print(\"Embedding shape (X_test):\", X_test.shape)\n",
    "\n",
    "    print(f\"\\n--{name_data} REGRESSION PREDICTIONS--\\n\")\n",
    "    # Make predictions and aggregate results across cross-validation regression models\n",
    "    average_predictions = predict_and_aggregate_models(X_test, args.results_dir, gene_460_names)\n",
    "\n",
    "    ### Prepare and Return Predictions ###\n",
    "\n",
    "    # Convert the predictions to a DataFrame\n",
    "    prediction_460_genes = pd.DataFrame(np.round(average_predictions, 2), index=cell_ids, columns=gene_460_names)\n",
    "    print(\"\\n Predictions shape:\", prediction_460_genes.shape)\n",
    "\n",
    "    del average_predictions, X_test, assets\n",
    "    gc.collect()\n",
    "\n",
    "    return prediction_460_genes\n",
    "\n",
    "\n",
    "def infer_crunch_2(\n",
    "    prediction_460_genes,  # Predicted expression values for 460 genes (Crunch 1 output)\n",
    "    name_data: str,  # The name of the dataset being processed (only used for logging and directory naming)\n",
    "    data_file_path: str,\n",
    "    model_directory_path: str,  # Path to save the trained model and results\n",
    "    scRNAseq,  # Single-cell RNA sequencing data (AnnData)\n",
    "    filter_column: str = 'dysplasia',  # Column name used to filter scRNA-seq data\n",
    "    filter_value: str = 'y'  # Value within the filter column used for filtering\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform Crunch 2 inference: Predict the expression of 18,157 unmeasured genes in spatial\n",
    "    transcriptomics data using the expression of the 460 inferred genes and single-cell RNA sequencing data for similarity-based matching.\n",
    "    \"\"\"\n",
    "\n",
    "    ##############################################################################\n",
    "    ##### BROAD Crunch 2: predict the expression of 18157 unmeasured genes #####\n",
    "    print(f\"\\n** {name_data} Predict the expression of 18157 unmeasured genes (Crunch 2) ****************************************************************\\n\")\n",
    "\n",
    "    print(f\"--COSINE SIMILARITIES between {name_data} and single-cell RNA sequencing from shared genes--\\n\")\n",
    "\n",
    "    # Get the 18615 genes to rank\n",
    "    gene_18615_list = list(scRNAseq.var.index)\n",
    "\n",
    "    # Extract shared and unmeasured genes\n",
    "    gene_460 = list(prediction_460_genes.columns)\n",
    "    common_genes = [g for g in gene_460 if g in gene_18615_list]\n",
    "    print(\"Number of shared genes between scRNA-seq and xenium data:\", len(common_genes))\n",
    "    unmeasured_genes = [gene for gene in gene_18615_list if gene not in common_genes]\n",
    "    print(\"Number of unmeasured genes in Xenium data:\", len(unmeasured_genes), \"\\n\")\n",
    "\n",
    "    # scRNA-Seq data log1p-normalized with scale factor 10000 on 18615 genes\n",
    "    rna_data_norm_10000_unmeasured_genes = scRNAseq[:, unmeasured_genes].X.toarray()\n",
    "    # scRNA-Seq data log1p-normalized with scale factor 100 on 460 genes\n",
    "    rna_data_norm_100_common_genes = log1p_normalization_scale_factor(scRNAseq[:, common_genes].layers[\"counts\"].toarray(), scale_factor=100)\n",
    "    print(f\"scRNA-Seq data shape ({len(rna_data_norm_100_common_genes)} samples x {scRNAseq.X.shape[1]} shared genes + unmeasured genes)\")\n",
    "    print(f\"Xenium data shape ({len(prediction_460_genes)} samples x {len(common_genes)} shared genes)\")\n",
    "    del scRNAseq\n",
    "    gc.collect()\n",
    "\n",
    "    print_memory_usage(\"Before computing cosine similarity\")\n",
    "\n",
    "    # Similarity-Based Matching: Find the top_k most similar spots for each query\n",
    "    top_k = 30\n",
    "    print(f\"\\nCompute COSINE SIMILARITY: Find the top_k(={top_k}) similar scRNA-Seq cells for each Xenium cell...\\n\")\n",
    "    matches, similarities = find_matches_cos_similarity(rna_data_norm_100_common_genes, prediction_460_genes[common_genes].values, top_k=top_k)\n",
    "    del rna_data_norm_100_common_genes\n",
    "    gc.collect()\n",
    "\n",
    "    # Weighted Averaging of scRNA-Seq data log1p-normalized with scale factor 10000\n",
    "    print(\"Compute WEIGHTED AVERAGE of unmeasured genes from scRNA-Seq based on similarity scores...\")\n",
    "    weighted_avg_df_10000 = pd.DataFrame([\n",
    "        {\n",
    "            **dict(zip(unmeasured_genes, np.average(rna_data_norm_10000_unmeasured_genes[indices, :], axis=0, weights=similarity).round(2)))  # updated\n",
    "        }\n",
    "        for i, (indices, similarity) in enumerate(zip(matches, similarities))\n",
    "    ])\n",
    "    weighted_avg_df_10000.index = prediction_460_genes.index\n",
    "\n",
    "    # Free memory by deleting large variables and performing garbage collection\n",
    "    del rna_data_norm_10000_unmeasured_genes, matches, similarities\n",
    "    gc.collect()\n",
    "\n",
    "    prediction_18615_genes = pd.concat([prediction_460_genes, weighted_avg_df_10000], axis=1)[gene_18615_list]\n",
    "\n",
    "    print(f\"\\n-- {name_data} PREDICTION DONE --\\n\")\n",
    "\n",
    "    # Return the final prediction DataFrame\n",
    "    return prediction_18615_genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yjbNmd4rtAf"
   },
   "source": [
    "## Crunch 3 Function\n",
    "\n",
    "The `gene_ranking` function identifies the top discriminating genes that differentiate dysplastic (pre-cancerous) from non-cancerous mucosa regions.\n",
    "\n",
    "**Steps:**\n",
    "1. **Input Predictions**: Take gene expression data for non-cancerous and dysplastic cells (shapes: n_cells x 18615genes v.s. m_cells x 18615genes).\n",
    "2. **Compute Metrics**: Calculate mean, variance and absolute log fold change (logFC) for all 18 615 protein-coding genes.\n",
    "3. **Rank Genes**: Rank genes by their ability to distinguish dysplastic from non-cancerous regions using absolute log fold change.\n",
    "4. **Output**: Return a DataFrame of gene names ranked from best to worst discriminator.\n",
    "\n",
    "**Note: The method’s effectiveness heavily relies on the model/method performances in Crunch #1 & Crunch #2 !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "uTKRN30cK5C3"
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# Gene ranking function (crunch 3)\n",
    "##########\n",
    "\n",
    "def gene_ranking(prediction_cell_ids_no_cancer, prediction_cell_ids_cancer,\n",
    "                 column_for_ranking=\"abs_logFC\", ascending=False):\n",
    "    \"\"\"\n",
    "    Rank all 18,615 protein-coding genes based on their ability to distinguish dysplasia\n",
    "    from non-cancerous mucosa regions. Each gene is assigned a rank from 1\n",
    "    (best discriminator) to 18,615 (worst), comparing expression levels between\n",
    "    non-cancerous and dysplastic tissue cells.\n",
    "\n",
    "    Parameters:\n",
    "    prediction_cell_ids_no_cancer: np.ndarray\n",
    "        Predicted gene expression for cells from non-cancerous regions.\n",
    "    prediction_cell_ids_cancer: np.ndarray\n",
    "        Predicted gene expression for cells from dysplastic (pre-cancerous) regions.\n",
    "    column_for_ranking: str\n",
    "        Column name used to rank genes (default is \"abs_logFC\").\n",
    "    ascending: bool\n",
    "        Whether to sort in ascending order (default is False).\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing gene names ranked by the specified metric.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate mean and variance for each gene\n",
    "    mean_no_cancer = prediction_cell_ids_no_cancer.mean(axis=0)\n",
    "    mean_cancer = prediction_cell_ids_cancer.mean(axis=0)\n",
    "\n",
    "    var_no_cancer = prediction_cell_ids_no_cancer.var(axis=0)\n",
    "    var_cancer = prediction_cell_ids_cancer.var(axis=0)\n",
    "\n",
    "    ### Compute ranking metrics ###\n",
    "    # Compute the absolute difference in mean expression levels\n",
    "    dif_abs_mean = np.abs(mean_no_cancer - mean_cancer)\n",
    "\n",
    "    # Compute the log fold change (logFC) for each gene\n",
    "    epsilon = 1e-6  # Small value to avoid division by zero\n",
    "    log_fc = np.log2((mean_cancer + epsilon) / (mean_no_cancer + epsilon))\n",
    "\n",
    "    gene_ranking_df = pd.DataFrame({\n",
    "        'mean_no_cancer': mean_no_cancer,\n",
    "        'mean_cancer': mean_cancer,\n",
    "        'variance_no_cancer': var_no_cancer,\n",
    "        'variance_cancer': var_cancer,\n",
    "        'dif_abs_mean': dif_abs_mean,\n",
    "        'logFC': log_fc,\n",
    "        'abs_logFC': np.abs(log_fc)\n",
    "    })\n",
    "\n",
    "    # Sort by column_for_ranking\n",
    "    gene_ranking_df = gene_ranking_df.sort_values(by=column_for_ranking, ascending=ascending)\n",
    "\n",
    "    print(f\"Gene ranking by {column_for_ranking}:\")\n",
    "    display(gene_ranking_df)\n",
    "\n",
    "    # Create the final ranked DataFrame with gene names and their ranks\n",
    "    prediction = pd.DataFrame(\n",
    "        gene_ranking_df.index,\n",
    "        index=np.arange(1, len(gene_ranking_df) + 1),\n",
    "        columns=['Gene Name'],\n",
    "    )\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZR-DmDxot1uI"
   },
   "source": [
    "### Combined Crunches Training Function\n",
    "\n",
    "The `train` function integrates Crunch 1, Crunch 2 and Crunch 3 workflows to infer gene expression and identify gene markers distinguishing dysplastic (pre-cancerous) from non-cancerous tissue regions from UC9_I-crunch3 sample.\n",
    "\n",
    "**Key Steps:**\n",
    "1. **Initialize Directories and Parameters**:\n",
    "   - Set up paths and sampling sizes.\n",
    "   - Define ranking parameters for gene discrimination.\n",
    "\n",
    "2. **Load and Preprocess Data**:\n",
    "   - Read a spatial data and extract the 460 measured gene names.\n",
    "   - Load H&E images and segment cells into dysplastic or non-dysplastic groups using ROI masks from UC9_I-crunch3 sample.\n",
    "\n",
    "3. **Crunch 1 (Gene Expression Prediction)**:\n",
    "   - Infer expression of 460 measured genes for sampled dysplastic and non-dysplastic cells.\n",
    "\n",
    "4. **Crunch 2 (Extended Gene Prediction)**:\n",
    "   - Use scRNAseq data and inferred 460 genes to predict the expression of 18 157 additional genes.\n",
    "\n",
    "5. **Crunch 3 (Gene Ranking)**:\n",
    "   - Rank all 18 615 protein-coding genes based on their ability to distinguish dysplastic from non-cancerous tissue.\n",
    "\n",
    "6. **Save Results**:\n",
    "   - Save in the specified `model_directory_path` ranked genes to a CSV file for CrunchDAO submission\n",
    "\n",
    "**Note: During the CrunchDAO submission, there is no need to run the `train` function. Simply, use the `infer` function loading the locally saved `gene_ranking.csv` file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "66qZutltt1Av"
   },
   "outputs": [],
   "source": [
    "# In the training function, users build and train the model to create a gene ranking.\n",
    "# Your gene ranking must be stored in the `model_directory_path`.\n",
    "def train(\n",
    "    data_directory_path: str,  # Path to the input data directory\n",
    "    model_directory_path: str  # Path to save the trained model and results\n",
    "):\n",
    "    print_memory_usage(\"Start\")\n",
    "    os.makedirs(model_directory_path, exist_ok=True)\n",
    "\n",
    "    # Parameters\n",
    "    args_dict = {\n",
    "        \"size_subset_uc9_i\": 2000,  # Number of cells to sample for dysplasia analysis - cancer/no-cancer cells for UC9_I-crunch3\n",
    "\n",
    "        \"file_name_scRNAseq\": 'Crunch3_scRNAseq.h5ad',  # Filename for scRNAseq data\n",
    "        \"size_subset_scRNAseq\": 10000,  # Number of scRNAseq cells to sample\n",
    "        \"filter_column_scRNAseq\": \"dysplasia\",  # Column for filtering scRNAseq data\n",
    "        \"filter_value_no_cancer\": \"n\",  # Filtered column value indicating absence of cancer\n",
    "        \"filter_value_cancer\": \"y\",  # Filtered column value indicating the presence of cancer\n",
    "\n",
    "        # Parameters for ranking genes\n",
    "        \"column_for_ranking\": \"abs_logFC\",\n",
    "        \"ascending\": False\n",
    "    }\n",
    "    # Convert the dictionary to a simple namespace for easy access to attributes\n",
    "    args = SimpleNamespace(**args_dict)\n",
    "\n",
    "    print(\"Read xenium data and dysplasia files...\")\n",
    "    # Loading Spatial Data\n",
    "    # UC9_I.zarr contains H&E Image noncancerous mucosa (already provided in Crunch 1 and Crunch 2)\n",
    "    sdata = sd.read_zarr(os.path.join(data_directory_path, 'UC9_I.zarr'))\n",
    "    # Extract the 460 measured gene names from xenium data\n",
    "    gene_460_names = list(sdata[\"anucleus\"].var.index)\n",
    "    del sdata\n",
    "    gc.collect()\n",
    "\n",
    "    # Load dysplasia-related files\n",
    "    # These files include:\n",
    "    # - HE: An H&E image of tissue regions exhibiting dysplasia\n",
    "    # - HE_nuc: A nuclear segmentation mask\n",
    "    # - region: An ROI mask indicating dysplastic vs. non-dysplastic regions of the tissue\n",
    "    #\n",
    "    # Using these images, you can extract additional spatial features and labels that may\n",
    "    # be relevant for training or evaluating your model.\n",
    "    dysplasia_file = {\n",
    "        # H&E image of tissue with dysplasia\n",
    "        'tif_HE': os.path.join(data_directory_path, 'UC9_I-crunch3-HE.tif'),\n",
    "\n",
    "        # Nucleus segmentation of H&E image\n",
    "        'tif_HE_nuc': os.path.join(data_directory_path, 'UC9_I-crunch3-HE-label-stardist.tif'),\n",
    "\n",
    "        # Regions in H&E image highlighting dysplasia and non-dysplasia\n",
    "        'tif_region': os.path.join(data_directory_path, 'UC9_I-crunch3-HE-dysplasia-ROI.tif')\n",
    "    }\n",
    "    # Read the dysplasia-related images and store them in a dictionary\n",
    "    dysplasia_img_list = {}\n",
    "    for key in dysplasia_file:\n",
    "        dysplasia_img_list[key] = skimage.io.imread(dysplasia_file[key])\n",
    "\n",
    "    print_memory_usage(\"After Reading dysplasia files\")\n",
    "\n",
    "    ##############################################################################\n",
    "    # Extract cell IDs highlighting dysplasia and non-dysplasia from UC9_I-crunch3-HE\n",
    "    print(f\"\\n***********************************************************************************************************************************************************\")\n",
    "    print(f\"** Extract cell IDs highlighting dysplasia and non-dysplasia from UC9_I-crunch3-HE ************************************************************************\")\n",
    "    print(f\"***********************************************************************************************************************************************************\\n\")\n",
    "\n",
    "    # Get region properties from the nucleus segmentation image: for each cell_id get its location on H&E image\n",
    "    regions = regionprops(dysplasia_img_list['tif_HE_nuc'])\n",
    "\n",
    "    # Divide cell IDs between dysplasia and non-dysplasia status\n",
    "    cell_ids_no_cancer, cell_ids_cancer = [], []\n",
    "    # Loop through each region and extract centroid if the cell ID matches\n",
    "    for props in tqdm(regions):\n",
    "        cell_id = props.label\n",
    "        centroid = props.centroid\n",
    "        y_center, x_center = int(centroid[0]), int(centroid[1])\n",
    "        # Using UC9_I-crunch3-HE-dysplasia-ROI.tif, check if cell ID highlight dysplasia or non-dysplasia (or 0 indicating other tissue regions)\n",
    "        dysplasia = dysplasia_img_list['tif_region'][y_center, x_center]\n",
    "        if dysplasia == 1:\n",
    "            cell_ids_no_cancer.append(cell_id)\n",
    "        elif dysplasia == 2:\n",
    "            cell_ids_cancer.append(cell_id)\n",
    "\n",
    "    del dysplasia_img_list['tif_region'], regions\n",
    "    gc.collect()\n",
    "\n",
    "    ##############################################################################\n",
    "    ##### BROAD Crunch 1 & 2: predict the expression of 460 measured genes + 18155 unmeasured genes #####\n",
    "    print(f\"\\n***********************************************************************************************************************************************************\")\n",
    "    print(f\"** Predict the expression of 18615 genes for UC9_I-crunch3 cells (Crunch 1 & 2) ***********************************************************************************\")\n",
    "    print(f\"***********************************************************************************************************************************************************\\n\")\n",
    "\n",
    "    ### Crunch 1 ###\n",
    "\n",
    "    # No cancer status: Sample cells for prediction and infer 460 gene expression (Crunch 1)\n",
    "    subset_cell_ids_no_cancer = random.sample(cell_ids_no_cancer, args.size_subset_uc9_i)\n",
    "    prediction_cell_ids_no_cancer = infer_crunch_1(\n",
    "        name_data=\"UC9_I no cancer\",\n",
    "        data_file_path=data_directory_path,\n",
    "        model_directory_path=model_directory_path,\n",
    "        sdata=dysplasia_img_list,\n",
    "        cell_ids=subset_cell_ids_no_cancer,\n",
    "        gene_460_names=gene_460_names\n",
    "    )\n",
    "\n",
    "    # Cancer status: Sample cells for prediction and infer 460 gene expression (Crunch 1)\n",
    "    subset_cell_ids_cancer = random.sample(cell_ids_cancer, args.size_subset_uc9_i)\n",
    "    prediction_cell_ids_cancer = infer_crunch_1(\n",
    "        name_data=\"UC9_I cancer\",\n",
    "        data_file_path=data_directory_path,\n",
    "        model_directory_path=model_directory_path,\n",
    "        sdata=dysplasia_img_list,\n",
    "        cell_ids=subset_cell_ids_cancer,\n",
    "        gene_460_names=gene_460_names\n",
    "    )\n",
    "\n",
    "    del dysplasia_img_list\n",
    "    gc.collect()\n",
    "\n",
    "    ### Crunch 2 ###\n",
    "\n",
    "    print_memory_usage(\"Before reading scRNAseq file\")\n",
    "    # Load scRNAseq file\n",
    "    scRNAseq = sc.read_h5ad(os.path.join(data_directory_path, args.file_name_scRNAseq))\n",
    "\n",
    "    # For low-memory RAM system: sampling a subset of scRNAseq cells\n",
    "    if args.size_subset_scRNAseq < len(scRNAseq.obs):\n",
    "        rows_to_keep = list(scRNAseq.obs.sample(n=args.size_subset_scRNAseq, random_state=42).index)\n",
    "        scRNAseq = scRNAseq[rows_to_keep, :]\n",
    "\n",
    "    # Filter scRNAseq data by dysplasia status\n",
    "    scRNAseq_no_cancer = scRNAseq[scRNAseq.obs[args.filter_column_scRNAseq] == args.filter_value_no_cancer].copy()\n",
    "    scRNAseq_cancer = scRNAseq[scRNAseq.obs[args.filter_column_scRNAseq] == args.filter_value_cancer].copy()\n",
    "    del scRNAseq\n",
    "    gc.collect()\n",
    "    print_memory_usage(\"After reading scRNAseq file\")\n",
    "\n",
    "    # No cancer status: predict the expression of 18157 genes using the expression of the 460 inferred genes and scRNAseq data (Crunch 2)\n",
    "    prediction_cell_ids_no_cancer = infer_crunch_2(\n",
    "        prediction_460_genes=prediction_cell_ids_no_cancer,\n",
    "        name_data=\"UC9_I no cancer\",\n",
    "        data_file_path=data_directory_path,\n",
    "        model_directory_path=model_directory_path,\n",
    "        scRNAseq=scRNAseq_no_cancer,\n",
    "        filter_column=args.filter_column_scRNAseq,\n",
    "        filter_value=args.filter_value_no_cancer\n",
    "    )\n",
    "\n",
    "    # Cancer status: predict the expression of 18157 genes using the expression of the 460 inferred genes and scRNAseq data (Crunch 2)\n",
    "    prediction_cell_ids_cancer = infer_crunch_2(\n",
    "        prediction_460_genes=prediction_cell_ids_cancer,\n",
    "        name_data=\"UC9_I cancer\",\n",
    "        data_file_path=data_directory_path,\n",
    "        model_directory_path=model_directory_path,\n",
    "        scRNAseq=scRNAseq_cancer,\n",
    "        filter_column=args.filter_column_scRNAseq,\n",
    "        filter_value=args.filter_value_cancer\n",
    "    )\n",
    "\n",
    "    ##############################################################################\n",
    "    ##### BROAD Crunch 3: Gene Ranking:  Identifying gene markers of pre-cancerous tissue regions in IBD #####\n",
    "    print(f\"\\n***********************************************************************************************************************************************************\")\n",
    "    print(f\"** Gene Ranking:  Identifying gene markers of pre-cancerous tissue regions in IBD (Crunch 3) **************************************************************\")\n",
    "    print(f\"***********************************************************************************************************************************************************\\n\")\n",
    "\n",
    "    # Rank all 18615 protein-coding genes based on their ability to distinguish dysplasia from noncancerous mucosa regions, assigning them ranks\n",
    "    # from 1 (best discriminator) to 18,615 (worst)\n",
    "    # We compare prediction_cell_ids_no_cancer V.S. prediction_cell_ids_cancer\n",
    "    df_gene_ranking = gene_ranking(prediction_cell_ids_no_cancer, prediction_cell_ids_cancer,\n",
    "                                   column_for_ranking=args.column_for_ranking, ascending=args.ascending)\n",
    "\n",
    "    del dysplasia_file, prediction_cell_ids_no_cancer, prediction_cell_ids_cancer\n",
    "    gc.collect()\n",
    "\n",
    "    # Save the ranked genes to a CSV file -> to use for the inder function and crunchDAO crunch 3 submission\n",
    "    df_gene_ranking.to_csv(os.path.join(model_directory_path, \"gene_ranking.csv\"))\n",
    "\n",
    "    return df_gene_ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfORqE7sgRHD"
   },
   "source": [
    "Preprocessed datasets are saved in `/tmp/processed_dataset`(temporary storage - not needed for inference).\n",
    "\n",
    "**Note: You need to upload in `./resources` (model_directory_path) the pre-trained Resnet50 `pytorch_model.bin`, the config file `config.json` and your trained regression models from Crunch 1 (baseline quickstarter model: https://huggingface.co/Tarandros/Autoimmune_Disease_Machine_Learning_Crunch_1/tree/main (Use your own config file and model if you can)).**\n",
    "\n",
    "### Crunch 1 - Crunch 2 - Crunch 3 -> Gene Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 349785,
     "status": "ok",
     "timestamp": 1734621355858,
     "user": {
      "displayName": "alexis gassmann",
      "userId": "07787599245733678355"
     },
     "user_tz": -60
    },
    "id": "h6DMc8YL9kEN",
    "outputId": "8da4f980-2ce3-4a67-a4d3-de58b0cb5919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Start: Total=67.35 GB, Available=61.24 GB, Used=5.28 GB, Percent=9.1%)\n",
      "\n",
      "Read xenium data and dysplasia files...\n",
      "(After Reading dysplasia files: Total=67.35 GB, Available=55.89 GB, Used=10.64 GB, Percent=17.0%)\n",
      "\n",
      "\n",
      "***********************************************************************************************************************************************************\n",
      "** Extract cell IDs highlighting dysplasia and non-dysplasia from UC9_I-crunch3-HE ************************************************************************\n",
      "***********************************************************************************************************************************************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 460243/460243 [00:17<00:00, 26689.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***********************************************************************************************************************************************************\n",
      "** Predict the expression of 18615 genes for UC9_I-crunch3 cells (Crunch 1 & 2) ***********************************************************************************\n",
      "***********************************************************************************************************************************************************\n",
      "\n",
      "\n",
      "-- UC9_I no cancer INFERENCE ---------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './resources/config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_gene_ranking \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_directory_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_directory_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./resources\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 97\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data_directory_path, model_directory_path)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m### Crunch 1 ###\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# No cancer status: Sample cells for prediction and infer 460 gene expression (Crunch 1)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m subset_cell_ids_no_cancer \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(cell_ids_no_cancer, args\u001b[38;5;241m.\u001b[39msize_subset_uc9_i)\n\u001b[0;32m---> 97\u001b[0m prediction_cell_ids_no_cancer \u001b[38;5;241m=\u001b[39m \u001b[43minfer_crunch_1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUC9_I no cancer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_directory_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_directory_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_directory_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43msdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdysplasia_img_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcell_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset_cell_ids_no_cancer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgene_460_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgene_460_names\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Cancer status: Sample cells for prediction and infer 460 gene expression (Crunch 1)\u001b[39;00m\n\u001b[1;32m    107\u001b[0m subset_cell_ids_cancer \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(cell_ids_cancer, args\u001b[38;5;241m.\u001b[39msize_subset_uc9_i)\n",
      "Cell \u001b[0;32mIn[12], line 21\u001b[0m, in \u001b[0;36minfer_crunch_1\u001b[0;34m(name_data, data_file_path, model_directory_path, sdata, cell_ids, gene_460_names)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load training configuration parameters\u001b[39;00m\n\u001b[1;32m     20\u001b[0m config_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dir_models_and_results, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     22\u001b[0m     args_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     23\u001b[0m args \u001b[38;5;241m=\u001b[39m SimpleNamespace(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs_dict)\n",
      "File \u001b[0;32m~/Desktop/broad/broad/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './resources/config.json'"
     ]
    }
   ],
   "source": [
    "df_gene_ranking = train(\n",
    "    data_directory_path=\"./data\",\n",
    "    model_directory_path=\"./resources\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQFiibInp9gU"
   },
   "source": [
    "## Infer function\n",
    "\n",
    "The `infer` function loads the gene ranking CSV file and and returns it as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjyE1S3epuim"
   },
   "outputs": [],
   "source": [
    "def infer(\n",
    "    data_file_path: str,\n",
    "    data_directory_path: str,\n",
    "    model_directory_path: str\n",
    "):\n",
    "    # Load the list of genes to predict\n",
    "    # gene_list = pd.read_csv(os.path.join(data_directory_path, 'Crunch3_gene_list.csv'))\n",
    "    # gene_names = gene_list['gene_symbols']\n",
    "\n",
    "    # The intended goal is to rank all 18,615 protein-coding genes based on their ability\n",
    "    # to distinguish dysplasia from noncancerous mucosa regions, assigning them ranks\n",
    "    # from 1 (best discriminator) to 18,615 (worst).\n",
    "\n",
    "    # Currently, we generate a random permutation of gene names as a placeholder.\n",
    "    # Replace the logic below with actual model inference:\n",
    "    # 1. Load the trained model from the model directory.\n",
    "    # 2. Use the model to score and rank the genes accordingly.\n",
    "    # 3. Return the predicted ranking as a DataFrame.\n",
    "\n",
    "    # prediction = pd.DataFrame(\n",
    "    #     np.random.permutation(gene_names),\n",
    "    #     index=np.arange(1, len(gene_names) + 1),\n",
    "    #     columns=['Gene Name'],\n",
    "    # )\n",
    "\n",
    "    # Load the ranked genes\n",
    "    prediction = pd.read_csv(os.path.join(model_directory_path, \"gene_ranking.csv\"), index_col=0)\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzPravesGNpz"
   },
   "source": [
    "**Note: During the CrunchDAO submission, there is no need to run the `train` function. Simply, use the `infer` function loading the locally saved `gene_ranking.csv` file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1734621355859,
     "user": {
      "displayName": "alexis gassmann",
      "userId": "07787599245733678355"
     },
     "user_tz": -60
    },
    "id": "as-q66EiiPkw",
    "outputId": "9152312e-df03-4921-d2d4-0b2e6893a72a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"prediction\",\n  \"rows\": 18615,\n  \"fields\": [\n    {\n      \"column\": \"Gene Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 18615,\n        \"samples\": [\n          \"TSTD1\",\n          \"AP001458.2\",\n          \"ZBTB10\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "prediction"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-6e483408-c635-4c66-bebc-fbbfbee27e5f\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DEFA6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SERPINB5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MYBPC1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HRK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DEFA5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18611</th>\n",
       "      <td>GOLGA8H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18612</th>\n",
       "      <td>SLC12A5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18613</th>\n",
       "      <td>GOLGA7B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18614</th>\n",
       "      <td>GOLGA6L7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18615</th>\n",
       "      <td>NANOGP8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18615 rows × 1 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e483408-c635-4c66-bebc-fbbfbee27e5f')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-6e483408-c635-4c66-bebc-fbbfbee27e5f button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-6e483408-c635-4c66-bebc-fbbfbee27e5f');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-e8a28d51-82f0-4586-9905-6bd4cb81d5c0\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e8a28d51-82f0-4586-9905-6bd4cb81d5c0')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-e8a28d51-82f0-4586-9905-6bd4cb81d5c0 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "  <div id=\"id_439ec7c0-0675-4157-80b1-48573fe7e6f1\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('prediction')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_439ec7c0-0675-4157-80b1-48573fe7e6f1 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('prediction');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "      Gene Name\n",
       "1         DEFA6\n",
       "2      SERPINB5\n",
       "3        MYBPC1\n",
       "4           HRK\n",
       "5         DEFA5\n",
       "...         ...\n",
       "18611   GOLGA8H\n",
       "18612   SLC12A5\n",
       "18613   GOLGA7B\n",
       "18614  GOLGA6L7\n",
       "18615   NANOGP8\n",
       "\n",
       "[18615 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = infer(\n",
    "    data_file_path=\"./data\",\n",
    "    data_directory_path=\"./data\",\n",
    "    model_directory_path=\"./resources\"\n",
    ")\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5w-KSO52hDj"
   },
   "outputs": [],
   "source": [
    "# This command is running a local test with your submission\n",
    "# making sure that your submission can be accepted by the system\n",
    "# crunch.test(\n",
    "#     no_determinism_check=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZLIXE3mZdyc"
   },
   "source": [
    "The final step is to write your report as specified in the [# Justification Report](https://docs.crunchdao.com/competitions/competitions/broad-institute-autoimmune-disease/crunch-3#justification-report).\n",
    "\n",
    "You must:\n",
    "1. Explain how your method works. (5-10 sentences)\n",
    "2. Describe the reasoning behind your gene panel design. (5-10 sentences)\n",
    "3. Specify the datasets and any other resources utilized. (5-10 sentences)\n",
    "\n",
    "The limit is about one page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiYZw_v4ZfSE"
   },
   "source": [
    "---\n",
    "file: REPORT.md\n",
    "---\n",
    "\n",
    "# Method Description\n",
    "\n",
    "The scientific method begins with observation of a phenomenon, followed by formulation of a question about what was observed. \n",
    "\n",
    "Researchers then develop a hypothesis that provides a potential explanation for the observed phenomenon. The next step involves designing and conducting experiments to test the hypothesis under controlled conditions. Data collection must be systematic and thorough, with careful attention to variables that might influence results. \n",
    "\n",
    "Analysis of the collected data using appropriate statistical methods helps determine if the results support or refute the hypothesis. Based on this analysis, researchers may refine their hypothesis or develop a new one if necessary. \n",
    "\n",
    "The findings are then documented in detail, including methodology, results, and conclusions. \n",
    "Peer review follows, where other scientists critically evaluate the research for validity and reliability. \n",
    "\n",
    "Finally, replication of the study by other researchers helps confirm the findings and establish their credibility in the scientific community.\n",
    "\n",
    "# Rationale\n",
    "\n",
    "Scientific procedures require rigorous methodology to ensure validity and reliability of findings across different contexts and conditions. \n",
    "\n",
    "A well-designed scientific procedure minimizes bias and maximizes objectivity by controlling for confounding variables that might influence the results. \n",
    "\n",
    "Standardization of procedures allows for replication by other researchers, which is essential for verifying findings and building scientific consensus. \n",
    "\n",
    "The systematic approach of the scientific method helps researchers distinguish between correlation and causation, preventing premature conclusions based on limited evidence. Detailed documentation of methods and results creates transparency that allows others to evaluate the quality of the research and build upon previous work. \n",
    "\n",
    "Statistical analysis provides tools for determining whether observed effects are significant or merely due to chance. \n",
    "\n",
    "The peer review process serves as a quality control mechanism, identifying potential flaws or limitations before research is published. Ethical considerations must be integrated throughout the procedure to protect subjects and maintain integrity. \n",
    "\n",
    "Scientific procedures evolve over time as new technologies and methodologies emerge, allowing for increasingly sophisticated investigations. \n",
    "\n",
    "The ultimate goal of scientific procedures is to develop reliable knowledge that can be applied to solve problems and advance understanding in various fields.\n",
    "\n",
    "# Data and Resources Used\n",
    "\n",
    "Scientific procedures typically utilize both primary data collected directly by researchers and secondary data from existing databases or previous studies. \n",
    "\n",
    "Specialized equipment calibrated for precision and accuracy is essential for reliable measurements and observations across different scientific disciplines. \n",
    "\n",
    "Statistical software packages such as R, SPSS, or Python with scientific libraries enable complex data analysis and visualization of results. \n",
    "\n",
    "Literature databases including PubMed, Web of Science, and Google Scholar provide access to existing research for literature reviews and background information. \n",
    "\n",
    "Laboratory facilities with controlled environments allow for manipulation of variables and standardization of experimental conditions. \n",
    "\n",
    "Computing resources ranging from personal computers to high-performance computing clusters support data processing, simulation, and modeling of complex systems. \n",
    "\n",
    "Research funding from government agencies, private foundations, or industry partnerships provides necessary financial resources for equipment, materials, and personnel. \n",
    "\n",
    "Human resources including principal investigators, research assistants, statisticians, and technical specialists contribute diverse expertise to the research process. \n",
    "\n",
    "Ethical approval from institutional review boards ensures research involving human or animal subjects adheres to established standards for protection and humane treatment. \n",
    "\n",
    "Collaborative networks with other researchers and institutions facilitate sharing of resources, methodologies, and expertise across geographical and disciplinary boundaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47HkOHSSynG5"
   },
   "source": [
    "## **You can download this notebook and then submit it at https://hub.crunchdao.com/competitions/broad-3/submit/**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kppG1inFIopT"
   },
   "source": [
    "Steps to submit the notebook:\n",
    "\n",
    "* Download locally the ranked genes `gene_ranking.csv`\n",
    "* Create a directory named `resources`\n",
    "* Place the file `gene_ranking.csv` inside the newly created `resources` directory\n",
    "* Submit the `resources` directory in the \"Model Files\" field\n",
    "* Run notebook in the cloud with CPU environnment\n",
    "* Set Training to False\n",
    "\n",
    "**Note: During the CrunchDAO submission, there is no need to run the `train` function. Simply, use the `infer` function loading the locally saved `gene_ranking.csv` file.**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1mrCYp0MSIolnJBcTFuZoPBU39xNyiMBY",
     "timestamp": 1734438290923
    },
    {
     "file_id": "15O0Uur2rCLQjgNSXPce7TytD3Ci07nm0",
     "timestamp": 1732783611507
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

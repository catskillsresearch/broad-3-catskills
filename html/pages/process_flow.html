<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Process Flow - Spatial Transcriptomics</title>
    <link rel="stylesheet" href="../css/styles.css">
</head>
<body>
    <header>
        <h1>Process Flow for Dataset Creation and Model Training</h1>
    </header>
    <nav>
        <ul>
            <li><a href="../index.html">Home</a></li>
            <li><a href="getting_started.html">Getting Started</a></li>
            <li><a href="data_structure.html">Data Structure</a></li>
            <li><a href="synthetic_datasets.html">Synthetic Datasets</a></li>
            <li><a href="deepspot_architecture.html">DeepSpot Architecture</a></li>
            <li><a href="crunch_approaches.html">Crunch Approaches</a></li>
            <li><a href="unified_approach.html">Unified Approach</a></li>
            <li><a href="process_flow.html" class="active">Process Flow</a></li>
        </ul>
    </nav>
    <main>
        <section>
            <h2>Overview</h2>
            <p>
                This page provides detailed instructions for dataset creation, model training, and hyperparameter tuning
                for three different dataset types: small real dataset, synthetic dataset, and large real dataset.
                Each dataset type has specific requirements and considerations that are outlined in the following sections.
            </p>
        </section>

        <section id="small-real-dataset">
            <h2>1. Small Real Dataset</h2>
            <p>
                The small real dataset is useful for rapid development, testing, and debugging. It contains a subset of the full dataset
                while maintaining the essential characteristics needed for model development.
            </p>

            <h3>1.1 Dataset Creation</h3>
            <ol>
                <li>
                    <strong>Configure Small Dataset Parameters</strong>
                    <p>Edit <code>config/small_dataset_config.yaml</code> to set the parameters for the small dataset:</p>
                    <pre><code>
# Small dataset configuration
output_dir: output
data_path: data/full_dataset
small_data_path: data/small_dataset
n_samples: 500  # Number of samples to include
random_seed: 42  # Seed for reproducibility
stratify_by: region_type  # Stratification variable
val_test_size: 0.3  # Proportion for validation and test sets
                    </code></pre>
                </li>
                <li>
                    <strong>Run Small Dataset Creation Script</strong>
                    <p>Execute the following command to create the small dataset:</p>
                    <pre><code>python scripts/create_small_dataset.py --config config/small_dataset_config.yaml</code></pre>
                    <p>This will create a stratified subset of the full dataset in the specified output directory.</p>
                </li>
                <li>
                    <strong>Verify Dataset Creation</strong>
                    <p>Check that the small dataset has been created correctly:</p>
                    <pre><code>ls -la data/small_dataset</code></pre>
                    <p>You should see the following files:</p>
                    <ul>
                        <li><code>cell_coordinates.npy</code> - Cell spatial coordinates</li>
                        <li><code>gene_expression.npy</code> - Gene expression matrix</li>
                        <li><code>gene_names.npy</code> - Gene names</li>
                        <li><code>metadata.yaml</code> - Dataset metadata</li>
                        <li><code>train_indices.npy</code> - Indices for training set</li>
                        <li><code>val_indices.npy</code> - Indices for validation set</li>
                        <li><code>test_indices.npy</code> - Indices for test set</li>
                    </ul>
                </li>
            </ol>

            <h3>1.2 Model Training</h3>
            <ol>
                <li>
                    <strong>Configure Training Parameters</strong>
                    <p>Edit <code>config/config.yaml</code> to point to the small dataset and set training parameters:</p>
                    <pre><code>
# General configuration
output_dir: output
data_path: data/small_dataset
model_path: models/small_dataset_model

# Model parameters
model_type: unified  # Use the unified approach
feature_dim: 512
phi_dim: 256
embedding_dim: 512
n_heads: 4
dropout: 0.3

# Training parameters
batch_size: 32
learning_rate: 0.001
weight_decay: 0.0001
max_epochs: 50
gene_wise_weight: 0.3
cell_wise_weight: 0.7
                    </code></pre>
                </li>
                <li>
                    <strong>Run Training Pipeline</strong>
                    <p>Execute the following command to train the model on the small dataset:</p>
                    <pre><code>python scripts/run_pipeline.py --config config/config.yaml --step train</code></pre>
                    <p>This will train the unified model on the small dataset using the specified parameters.</p>
                </li>
                <li>
                    <strong>Monitor Training Progress</strong>
                    <p>You can monitor the training progress using Weights & Biases:</p>
                    <pre><code>wandb login
wandb dashboard</code></pre>
                    <p>This will open the Weights & Biases dashboard in your browser, where you can view training metrics, loss curves, and model performance.</p>
                </li>
            </ol>

            <h3>1.3 Hyperparameter Tuning</h3>
            <ol>
                <li>
                    <strong>Configure Hyperparameter Search</strong>
                    <p>Edit <code>config/hyperparameter_search.yaml</code> to define the hyperparameter search space:</p>
                    <pre><code>
# Hyperparameter search configuration
output_dir: output/hyperparameter_search
data_path: data/small_dataset
n_trials: 20
metric: val_cell_wise_spearman  # Metric to optimize

# Hyperparameter search space
params:
  learning_rate:
    distribution: log_uniform
    min: 0.0001
    max: 0.01
  weight_decay:
    distribution: log_uniform
    min: 0.00001
    max: 0.001
  dropout:
    distribution: uniform
    min: 0.1
    max: 0.5
  gene_wise_weight:
    distribution: uniform
    min: 0.2
    max: 0.8
  cell_wise_weight:
    distribution: uniform
    min: 0.2
    max: 0.8
                    </code></pre>
                </li>
                <li>
                    <strong>Run Hyperparameter Search</strong>
                    <p>Execute the following command to run the hyperparameter search:</p>
                    <pre><code>python scripts/run_hyperparameter_search.py --config config/hyperparameter_search.yaml</code></pre>
                    <p>This will run multiple training trials with different hyperparameter combinations to find the optimal configuration.</p>
                </li>
                <li>
                    <strong>Analyze Results and Select Best Configuration</strong>
                    <p>After the hyperparameter search is complete, analyze the results:</p>
                    <pre><code>python scripts/analyze_hyperparameter_results.py --results_dir output/hyperparameter_search</code></pre>
                    <p>This will generate a report of the best hyperparameter configurations and their performance metrics.</p>
                </li>
                <li>
                    <strong>Train Final Model with Best Hyperparameters</strong>
                    <p>Use the best hyperparameters to train the final model:</p>
                    <pre><code>python scripts/run_pipeline.py --config output/hyperparameter_search/best_config.yaml --step train</code></pre>
                    <p>This will train the model using the optimal hyperparameter configuration identified during the search.</p>
                </li>
            </ol>
        </section>

        <section id="synthetic-dataset">
            <h2>2. Synthetic Dataset</h2>
            <p>
                Synthetic datasets provide controlled environments for testing model capabilities and validating performance.
                They offer known ground truth and controllable complexity for rigorous evaluation.
            </p>

            <h3>2.1 Dataset Creation</h3>
            <ol>
                <li>
                    <strong>Configure Synthetic Dataset Parameters</strong>
                    <p>Edit <code>config/synthetic_config.yaml</code> to set the parameters for the synthetic dataset:</p>
                    <pre><code>
# Synthetic dataset configuration
output_dir: output
data_path: data/synthetic_dataset

# Dataset parameters
n_cells: 2000
n_genes: 200
n_regions: 5
region_type: circular
space_size: 1000
n_gene_modules: 10
min_module_size: 5
max_module_size: 20
base_expression: 5.0
expression_scale: 2.0
noise_level: 0.2
spatial_effect_strength: 0.5
nb_dispersion: 0.1
quality_noise: 0.1
val_test_size: 0.3
                    </code></pre>
                </li>
                <li>
                    <strong>Run Synthetic Dataset Creation Script</strong>
                    <p>Execute the following command to create the synthetic dataset:</p>
                    <pre><code>python scripts/create_synthetic_dataset.py --config config/synthetic_config.yaml --visualize</code></pre>
                    <p>This will generate a synthetic dataset with the specified properties and create visualizations.</p>
                </li>
                <li>
                    <strong>Validate Synthetic Dataset</strong>
                    <p>Verify that the synthetic dataset has the expected statistical properties:</p>
                    <pre><code>python scripts/validate_synthetic_dataset.py --dataset_dir data/synthetic_dataset --visualize</code></pre>
                    <p>This will run a series of validation tests and generate visualizations to confirm the dataset's properties.</p>
                </li>
            </ol>

            <h3>2.2 Model Training</h3>
            <ol>
                <li>
                    <strong>Configure Training Parameters</strong>
                    <p>Edit <code>config/config.yaml</code> to point to the synthetic dataset and set training parameters:</p>
                    <pre><code>
# General configuration
output_dir: output
data_path: data/synthetic_dataset
model_path: models/synthetic_dataset_model

# Model parameters
model_type: unified  # Use the unified approach
feature_dim: 512
phi_dim: 256
embedding_dim: 512
n_heads: 4
dropout: 0.3

# Training parameters
batch_size: 32
learning_rate: 0.001
weight_decay: 0.0001
max_epochs: 100
gene_wise_weight: 0.3
cell_wise_weight: 0.7
                    </code></pre>
                </li>
                <li>
                    <strong>Run Training Pipeline</strong>
                    <p>Execute the following command to train the model on the synthetic dataset:</p>
                    <pre><code>python scripts/run_pipeline.py --config config/config.yaml --step train</code></pre>
                    <p>This will train the unified model on the synthetic dataset using the specified parameters.</p>
                </li>
                <li>
                    <strong>Evaluate Model Performance</strong>
                    <p>After training, evaluate the model's performance on the synthetic dataset:</p>
                    <pre><code>python scripts/run_pipeline.py --config config/config.yaml --step evaluate</code></pre>
                    <p>This will generate evaluation metrics and visualizations to assess model performance.</p>
                </li>
                <li>
                    <strong>Visualize Results</strong>
                    <p>Generate comprehensive visualizations of model performance:</p>
                    <pre><code>python scripts/visualize_synthetic_dataset.py --dataset_dir data/synthetic_dataset --predictions_dir output/predictions/synthetic_model --output_dir output/visualizations/synthetic_analysis</code></pre>
                    <p>This will create visualizations comparing model predictions with ground truth.</p>
                </li>
            </ol>

            <h3>2.3 Hyperparameter Tuning</h3>
            <ol>
                <li>
                    <strong>Configure Hyperparameter Search</strong>
                    <p>Edit <code>config/hyperparameter_search.yaml</code> to define the hyperparameter search space for the synthetic dataset:</p>
                    <pre><code>
# Hyperparameter search configuration
output_dir: output/hyperparameter_search_synthetic
data_path: data/synthetic_dataset
n_trials: 30
metric: val_cell_wise_spearman  # Metric to optimize

# Hyperparameter search space
params:
  learning_rate:
    distribution: log_uniform
    min: 0.0001
    max: 0.01
  weight_decay:
    distribution: log_uniform
    min: 0.00001
    max: 0.001
  dropout:
    distribution: uniform
    min: 0.1
    max: 0.5
  gene_wise_weight:
    distribution: uniform
    min: 0.2
    max: 0.8
  cell_wise_weight:
    distribution: uniform
    min: 0.2
    max: 0.8
  n_heads:
    distribution: categorical
    values: [2, 4, 8]
  embedding_dim:
    distribution: categorical
    values: [256, 512, 1024]
                    </code></pre>
                </li>
                <li>
                    <strong>Run Hyperparameter Search</strong>
                    <p>Execute the following command to run the hyperparameter search:</p>
                    <pre><code>python scripts/run_hyperparameter_search.py --config config/hyperparameter_search.yaml</code></pre>
                    <p>This will run multiple training trials with different hyperparameter combinations to find the optimal configuration.</p>
                </li>
                <li>
                    <strong>Analyze Results and Select Best Configuration</strong>
                    <p>After the hyperparameter search is complete, analyze the results:</p>
                    <pre><code>python scripts/analyze_hyperparameter_results.py --results_dir output/hyperparameter_search_synthetic</code></pre>
                    <p>This will generate a report of the best hyperparameter configurations and their performance metrics.</p>
                </li>
                <li>
                    <strong>Train Final Model with Best Hyperparameters</strong>
                    <p>Use the best hyperparameters to train the final model:</p>
                    <pre><code>python scripts/run_pipeline.py --config output/hyperparameter_search_synthetic/best_config.yaml --step train</code></pre>
                    <p>This will train the model using the optimal hyperparameter configuration identified during the search.</p>
                </li>
            </ol>

            <h3>2.4 Integrating Synthetic Dataset with Real Data</h3>
            <ol>
                <li>
                    <strong>Configure Integration Parameters</strong>
                    <p>Edit <code>config/integration_config.yaml</code> to set parameters for integrating synthetic and real data:</p>
                    <pre><code>
# Integration configuration
output_dir: output
synthetic_data_path: data/synthetic_dataset
real_data_path: data/full_dataset
integrated_data_path: data/integrated_dataset
integration_method: concatenate  # Options: concatenate, augment, transfer
                    </code></pre>
                </li>
                <li>
                    <strong>Run Integration Script</strong>
                    <p>Execute the following command to integrate synthetic and real datasets:</p>
                    <pre><code>python scripts/integrate_synthetic_dataset.py --config config/integration_config.yaml</code></pre>
                    <p>This will create an integrated dataset combining synthetic and real data according to the specified method.</p>
                </li>
                <li>
                    <strong>Train Model on Integrated Dataset</strong>
                    <p>Train a model using the integrated dataset:</p>
                    <pre><code>python scripts/run_pipeline.py --config config/integration_config.yaml --step train</code></pre>
                    <p>This will train the model on the integrated dataset, potentially improving performance through data augmentation or transfer learning.</p>
                </li>
            </ol>
        </section>

        <section id="large-real-dataset">
            <h2>3. Large Real Dataset</h2>
            <p>
                The large real dataset contains the full set of spatial transcriptomics data, providing the most comprehensive
                and realistic environment for model training and evaluation.
            </p>

            <h3>3.1 Dataset Preparation</h3>
            <ol>
                <li>
                    <strong>Configure Data Preparation Parameters</strong>
                    <p>Edit <code>config/config.yaml</code> to set the parameters for data preparation:</p>
                    <pre><code>
# General configuration
output_dir: output
data_path: data/raw
processed_data_path: data/full_dataset

# Data preparation parameters
patch_size: 224
stride: 112
quality_threshold: 0.5
min_genes_per_cell: 100
min_cells_per_gene: 20
normalization: log1p  # Options: log1p, cpm, scran
batch_correction: True
                    </code></pre>
                </li>
                <li>
                    <strong>Run Data Preparation Pipeline</strong>
                    <p>Execute the following command to prepare the full dataset:</p>
                    <pre><code>python scripts/run_pipeline.py --config config/config.yaml --step data</code></pre>
                    <p>This will process the raw data, extract features, and prepare the dataset for model training.</p>
                </li>
                <li>
                    <strong>Verify Dataset Preparation</strong>
                    <p>Check that the full dataset has been prepared correctly:</p>
                    <pre><code>ls -la data/full_dataset</code></pre>
                    <p>You should see the following files:</p>
                    <ul>
                        <li><code>cell_coordinates.npy</code> - Cell spatial coordinates</li>
                        <li><code>gene_expression.npy</code> - Gene expression matrix</li>
                        <li><code>gene_names.npy</code> - Gene names</li>
                        <li><code>metadata.yaml</code> - Dataset metadata</li>
                        <li><code>image_features.npy</code> - Extracted image features</li>
                        <li><code>train_indices.npy</code> - Indices for training set</li>
                        <li><code>val_indices.npy</code> - Indices for validation set</li>
                        <li><code>test_indices.npy</code> - Indices for test set</li>
                    </ul>
                </li>
            </ol>

            <h3>3.2 Model Training</h3>
            <ol>
                <li>
                    <strong>Configure Training Parameters</strong>
                    <p>Edit <code>config/config.yaml</code> to set training parameters for the full dataset:</p>
                    <pre><code>
# General configuration
output_dir: output
data_path: data/full_dataset
model_path: models/full_dataset_model

# Model parameters
model_type: unified  # Use the unified approach
feature_dim: 512
phi_dim: 256
embedding_dim: 512
n_heads: 8
dropout: 0.3

# Training parameters
batch_size: 64
learning_rate: 0.0005
weight_decay: 0.0001
max_epochs: 200
gene_wise_weight: 0.3
cell_wise_weight: 0.7

# Optimization parameters
optimizer: adamw
scheduler: reduce_on_plateau
patience: 10
factor: 0.5
                    </code></pre>
                </li>
                <li>
                    <strong>Run Training Pipeline</strong>
                    <p>Execute the following command to train the model on the full dataset:</p>
                    <pre><code>python scripts/run_pipeline.py --config config/config.yaml --step train</code></pre>
                    <p>This will train the unified model on the full dataset using the specified parameters.</p>
                </li>
                <li>
                    <strong>Monitor Training Progress</strong>
                    <p>You can monitor the training progress using Weights & Biases:</p>
                    <pre><code>wandb login
wandb dashboard</code></pre>
                    <p>This will open the Weights & Biases dashboard in your browser, where you can view training metrics, loss curves, and model performance.</p>
                </li>
            </ol>

            <h3>3.3 Hyperparameter Tuning</h3>
            <ol>
                <li>
                    <strong>Apply Transfer Learning from Synthetic Dataset</strong>
                    <p>Use the best hyperparameters found during synthetic dataset tuning as a starting point:</p>
                    <pre><code>python scripts/transfer_hyperparameters.py --source_config output/hyperparameter_search_synthetic/best_config.yaml --target_config config/transfer_config.yaml</code></pre>
                    <p>This will create a new configuration file with the best hyperparameters from the synthetic dataset tuning.</p>
                </li>
                <li>
                    <strong>Configure Hyperparameter Search</strong>
                    <p>Edit <code>config/hyperparameter_search.yaml</code> to define a focused hyperparameter search space based on the transferred parameters:</p>
                    <pre><code>
# Hyperparameter search configuration
output_dir: output/hyperparameter_search_full
data_path: data/full_dataset
n_trials: 15
metric: val_cell_wise_spearman  # Metric to optimize
base_config: config/transfer_config.yaml  # Start from transferred hyperparameters

# Hyperparameter search space (focused around transferred values)
params:
  learning_rate:
    distribution: log_uniform
    min: 0.0001
    max: 0.001
  weight_decay:
    distribution: log_uniform
    min: 0.00001
    max: 0.0001
  dropout:
    distribution: uniform
    min: 0.2
    max: 0.4
                    </code></pre>
                </li>
                <li>
                    <strong>Run Focused Hyperparameter Search</strong>
                    <p>Execute the following command to run the focused hyperparameter search:</p>
                    <pre><code>python scripts/run_hyperparameter_search.py --config config/hyperparameter_search.yaml</code></pre>
                    <p>This will run a smaller number of trials with a focused search space to fine-tune the hyperparameters for the full dataset.</p>
                </li>
                <li>
                    <strong>Train Final Model with Best Hyperparameters</strong>
                    <p>Use the best hyperparameters to train the final model:</p>
                    <pre><code>python scripts/run_pipeline.py --config output/hyperparameter_search_full/best_config.yaml --step train</code></pre>
                    <p>This will train the model using the optimal hyperparameter configuration identified during the search.</p>
                </li>
            </ol>

            <h3>3.4 Evaluation and Analysis</h3>
            <ol>
                <li>
                    <strong>Evaluate Model Performance</strong>
                    <p>After training, evaluate the model's performance on the test set:</p>
                    <pre><code>python scripts/run_pipeline.py --config output/hyperparameter_search_full/best_config.yaml --step evaluate</code></pre>
                    <p>This will generate evaluation metrics and visualizations to assess model performance.</p>
                </li>
                <li>
                    <strong>Run Gene Ranking Analysis</strong>
                    <p>Apply the logFC method to rank genes based on their ability to differentiate between tissue types:</p>
                    <pre><code>python scripts/run_pipeline.py --config output/hyperparameter_search_full/best_config.yaml --step rank_genes</code></pre>
                    <p>This will generate a ranked list of genes and their associated log fold change values.</p>
                </li>
                <li>
                    <strong>Generate Comprehensive Visualizations</strong>
                    <p>Create visualizations to analyze model performance and biological insights:</p>
                    <pre><code>python scripts/run_pipeline.py --config output/hyperparameter_search_full/best_config.yaml --step visualize</code></pre>
                    <p>This will generate a comprehensive set of visualizations for model analysis and interpretation.</p>
                </li>
            </ol>
        </section>

        <section id="comparison">
            <h2>4. Comparison of Dataset Approaches</h2>
            <table>
                <thead>
                    <tr>
                        <th>Feature</th>
                        <th>Small Real Dataset</th>
                        <th>Synthetic Dataset</th>
                        <th>Large Real Dataset</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Size</td>
                        <td>~500 samples</td>
                        <td>Configurable (typically 2000 samples)</td>
                        <td>Full dataset (10,000+ samples)</td>
                    </tr>
                    <tr>
                        <td>Training Time</td>
                        <td>Fast (minutes to hours)</td>
                        <td>Moderate (hours)</td>
                        <td>Slow (hours to days)</td>
                    </tr>
                    <tr>
                        <td>Primary Use</td>
                        <td>Rapid development and testing</td>
                        <td>Controlled experiments and validation</td>
                        <td>Final model training and evaluation</td>
                    </tr>
                    <tr>
                        <td>Hyperparameter Tuning</td>
                        <td>Initial exploration</td>
                        <td>Comprehensive search</td>
                        <td>Focused fine-tuning</td>
                    </tr>
                    <tr>
                        <td>Ground Truth</td>
                        <td>Limited</td>
                        <td>Complete</td>
                        <td>Limited</td>
                    </tr>
                    <tr>
                        <td>Biological Relevance</td>
                        <td>High</td>
                        <td>Moderate</td>
                        <td>High</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <section id="recommended-workflow">
            <h2>5. Recommended Workflow</h2>
            <p>
                For optimal results, we recommend the following workflow that leverages all three dataset types:
            </p>
            <ol>
                <li>
                    <strong>Initial Development with Small Real Dataset</strong>
                    <p>
                        Start with the small real dataset for rapid development, debugging, and initial testing.
                        This allows you to quickly iterate on model architecture and implementation details.
                    </p>
                </li>
                <li>
                    <strong>Controlled Experiments with Synthetic Dataset</strong>
                    <p>
                        Use synthetic datasets to conduct controlled experiments, validate model capabilities,
                        and perform comprehensive hyperparameter tuning. The known ground truth in synthetic data
                        allows for more rigorous evaluation of model performance.
                    </p>
                </li>
                <li>
                    <strong>Transfer Learning to Large Real Dataset</strong>
                    <p>
                        Apply the knowledge gained from synthetic dataset experiments to the large real dataset.
                        Use transfer learning to initialize models and focused hyperparameter tuning to optimize performance.
                    </p>
                </li>
                <li>
                    <strong>Final Evaluation and Analysis</strong>
                    <p>
                        Conduct comprehensive evaluation and analysis on the large real dataset to assess model performance,
                        generate biological insights, and prepare results for publication or deployment.
                    </p>
                </li>
            </ol>
        </section>
    </main>
    <footer>
        <p>&copy; 2025 Catskills Research. All rights reserved.</p>
    </footer>
</body>
</html>

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Training and Hyperparameter Tuning - Spatial Transcriptomics</title>
    <link rel="stylesheet" href="../css/styles.css">
</head>
<body>
    <header>
        <h1>Training and Hyperparameter Tuning</h1>
    </header>
    <nav>
        <ul>
            <li><a href="../index.html">Home</a></li>
            <li><a href="getting_started.html">Getting Started</a></li>
            <li><a href="data_structure.html">Data Structure</a></li>
            <li><a href="synthetic_datasets.html">Synthetic Datasets</a></li>
            <li><a href="deepspot_architecture.html">DeepSpot Architecture</a></li>
            <li><a href="crunch_approaches.html">Crunch Approaches</a></li>
            <li><a href="unified_approach.html">Unified Approach</a></li>
            <li><a href="process_flow.html">Process Flow</a></li>
            <li><a href="training_tuning.html" class="active">Training & Tuning</a></li>
        </ul>
    </nav>
    <main>
        <section>
            <h2>Overview</h2>
            <p>
                This page provides detailed information about training the unified model and tuning hyperparameters
                for optimal performance. We cover training strategies for each dataset type (small real, synthetic, 
                and large real) and provide guidance on hyperparameter tuning approaches.
            </p>
        </section>

        <section id="training-strategies">
            <h2>Training Strategies</h2>
            
            <h3>Small Real Dataset Training</h3>
            <p>
                Training on the small real dataset is focused on rapid development and initial validation of the model architecture.
                The key considerations for small dataset training include:
            </p>
            <ul>
                <li><strong>Batch Size</strong>: Smaller batch sizes (16-32) work well for small datasets</li>
                <li><strong>Learning Rate</strong>: Start with a moderate learning rate (0.001) and use learning rate scheduling</li>
                <li><strong>Regularization</strong>: Apply stronger regularization (higher weight decay, dropout) to prevent overfitting</li>
                <li><strong>Early Stopping</strong>: Use early stopping with a patience of 10-20 epochs to prevent overfitting</li>
                <li><strong>Data Augmentation</strong>: Consider using data augmentation to effectively increase dataset size</li>
            </ul>
            
            <h4>Example Training Configuration</h4>
            <pre><code>
# Small dataset training configuration
model_type: unified
feature_dim: 512
phi_dim: 256
embedding_dim: 512
n_heads: 4
dropout: 0.4
batch_size: 16
learning_rate: 0.001
weight_decay: 0.0005
max_epochs: 100
early_stopping_patience: 15
gene_wise_weight: 0.3
cell_wise_weight: 0.7
            </code></pre>
            
            <h4>Training Command</h4>
            <pre><code>python scripts/run_pipeline.py --config config/small_dataset_config.yaml --step train</code></pre>
            
            <h3>Synthetic Dataset Training</h3>
            <p>
                Training on synthetic datasets allows for controlled experiments and validation of model capabilities.
                The key considerations for synthetic dataset training include:
            </p>
            <ul>
                <li><strong>Dataset Complexity</strong>: Start with simpler synthetic datasets and gradually increase complexity</li>
                <li><strong>Feature Isolation</strong>: Use synthetic datasets that isolate specific features to test model components</li>
                <li><strong>Batch Size</strong>: Moderate batch sizes (32-64) work well for synthetic datasets</li>
                <li><strong>Learning Rate</strong>: Use a slightly lower learning rate (0.0005-0.001) for more stable training</li>
                <li><strong>Validation Split</strong>: Use a larger validation split (20-30%) for more reliable performance assessment</li>
            </ul>
            
            <h4>Example Training Configuration</h4>
            <pre><code>
# Synthetic dataset training configuration
model_type: unified
feature_dim: 512
phi_dim: 256
embedding_dim: 512
n_heads: 4
dropout: 0.3
batch_size: 32
learning_rate: 0.0005
weight_decay: 0.0001
max_epochs: 150
early_stopping_patience: 20
gene_wise_weight: 0.3
cell_wise_weight: 0.7
            </code></pre>
            
            <h4>Training Command</h4>
            <pre><code>python scripts/run_pipeline.py --config config/synthetic_config.yaml --step train</code></pre>
            
            <h3>Large Real Dataset Training</h3>
            <p>
                Training on the large real dataset is focused on achieving the best possible performance for real-world applications.
                The key considerations for large dataset training include:
            </p>
            <ul>
                <li><strong>Batch Size</strong>: Larger batch sizes (64-128) for more stable gradient estimates</li>
                <li><strong>Learning Rate</strong>: Use a lower learning rate (0.0001-0.0005) with careful scheduling</li>
                <li><strong>Regularization</strong>: Apply moderate regularization to balance underfitting and overfitting</li>
                <li><strong>Training Duration</strong>: Train for more epochs (200-300) to ensure convergence</li>
                <li><strong>Gradient Accumulation</strong>: Consider using gradient accumulation for effectively larger batch sizes</li>
                <li><strong>Mixed Precision</strong>: Use mixed precision training for faster computation</li>
            </ul>
            
            <h4>Example Training Configuration</h4>
            <pre><code>
# Large dataset training configuration
model_type: unified
feature_dim: 512
phi_dim: 256
embedding_dim: 512
n_heads: 8
dropout: 0.3
batch_size: 64
learning_rate: 0.0002
weight_decay: 0.0001
max_epochs: 250
early_stopping_patience: 25
gene_wise_weight: 0.3
cell_wise_weight: 0.7
gradient_accumulation_steps: 2
use_mixed_precision: true
            </code></pre>
            
            <h4>Training Command</h4>
            <pre><code>python scripts/run_pipeline.py --config config/large_dataset_config.yaml --step train</code></pre>
        </section>

        <section id="hyperparameter-tuning">
            <h2>Hyperparameter Tuning</h2>
            
            <h3>Key Hyperparameters</h3>
            <p>
                The following hyperparameters have the most significant impact on model performance and should be prioritized during tuning:
            </p>
            <table>
                <thead>
                    <tr>
                        <th>Hyperparameter</th>
                        <th>Description</th>
                        <th>Typical Range</th>
                        <th>Impact</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>learning_rate</td>
                        <td>Step size for gradient updates</td>
                        <td>0.0001 - 0.01</td>
                        <td>High</td>
                    </tr>
                    <tr>
                        <td>weight_decay</td>
                        <td>L2 regularization strength</td>
                        <td>0.00001 - 0.001</td>
                        <td>Medium</td>
                    </tr>
                    <tr>
                        <td>dropout</td>
                        <td>Dropout probability</td>
                        <td>0.1 - 0.5</td>
                        <td>Medium</td>
                    </tr>
                    <tr>
                        <td>gene_wise_weight</td>
                        <td>Weight for gene-wise Spearman loss</td>
                        <td>0.2 - 0.8</td>
                        <td>High</td>
                    </tr>
                    <tr>
                        <td>cell_wise_weight</td>
                        <td>Weight for cell-wise Spearman loss</td>
                        <td>0.2 - 0.8</td>
                        <td>High</td>
                    </tr>
                    <tr>
                        <td>n_heads</td>
                        <td>Number of attention heads</td>
                        <td>2, 4, 8, 16</td>
                        <td>Medium</td>
                    </tr>
                    <tr>
                        <td>embedding_dim</td>
                        <td>Dimension of cell embeddings</td>
                        <td>256, 512, 1024</td>
                        <td>Medium</td>
                    </tr>
                    <tr>
                        <td>batch_size</td>
                        <td>Number of samples per batch</td>
                        <td>16, 32, 64, 128</td>
                        <td>Medium</td>
                    </tr>
                </tbody>
            </table>
            
            <h3>Tuning Approaches</h3>
            
            <h4>1. Grid Search</h4>
            <p>
                Grid search systematically evaluates all combinations of specified hyperparameter values.
                This approach is thorough but can be computationally expensive for large hyperparameter spaces.
            </p>
            <pre><code>
# Example grid search configuration
params:
  learning_rate: [0.0001, 0.0005, 0.001]
  weight_decay: [0.00001, 0.0001, 0.001]
  dropout: [0.2, 0.3, 0.4]
  gene_wise_weight: [0.3, 0.5, 0.7]
  cell_wise_weight: [0.3, 0.5, 0.7]
            </code></pre>
            
            <h4>2. Random Search</h4>
            <p>
                Random search samples hyperparameter values from specified distributions.
                This approach is more efficient than grid search for large hyperparameter spaces.
            </p>
            <pre><code>
# Example random search configuration
params:
  learning_rate:
    distribution: log_uniform
    min: 0.0001
    max: 0.01
  weight_decay:
    distribution: log_uniform
    min: 0.00001
    max: 0.001
  dropout:
    distribution: uniform
    min: 0.1
    max: 0.5
  gene_wise_weight:
    distribution: uniform
    min: 0.2
    max: 0.8
  cell_wise_weight:
    distribution: uniform
    min: 0.2
    max: 0.8
            </code></pre>
            
            <h4>3. Bayesian Optimization</h4>
            <p>
                Bayesian optimization uses probabilistic models to guide the search for optimal hyperparameters.
                This approach is more efficient than both grid and random search for finding optimal hyperparameters.
            </p>
            <pre><code>
# Example Bayesian optimization configuration
optimization_method: bayesian
n_trials: 30
metric: val_cell_wise_spearman
params:
  learning_rate:
    distribution: log_uniform
    min: 0.0001
    max: 0.01
  weight_decay:
    distribution: log_uniform
    min: 0.00001
    max: 0.001
  dropout:
    distribution: uniform
    min: 0.1
    max: 0.5
  gene_wise_weight:
    distribution: uniform
    min: 0.2
    max: 0.8
  cell_wise_weight:
    distribution: uniform
    min: 0.2
    max: 0.8
            </code></pre>
            
            <h3>Tuning Workflow</h3>
            <ol>
                <li>
                    <strong>Initial Exploration on Small Dataset</strong>
                    <p>
                        Start with a broad hyperparameter search on the small dataset to quickly identify promising regions
                        of the hyperparameter space.
                    </p>
                    <pre><code>python scripts/run_hyperparameter_search.py --config config/hyperparameter_search_small.yaml</code></pre>
                </li>
                <li>
                    <strong>Comprehensive Search on Synthetic Dataset</strong>
                    <p>
                        Conduct a more comprehensive search on synthetic datasets to thoroughly explore the hyperparameter space
                        without the computational cost of using the full dataset.
                    </p>
                    <pre><code>python scripts/run_hyperparameter_search.py --config config/hyperparameter_search_synthetic.yaml</code></pre>
                </li>
                <li>
                    <strong>Focused Fine-tuning on Large Dataset</strong>
                    <p>
                        Perform a focused search around the best hyperparameters found in previous steps on the large dataset
                        to fine-tune the model for optimal performance.
                    </p>
                    <pre><code>python scripts/run_hyperparameter_search.py --config config/hyperparameter_search_large.yaml</code></pre>
                </li>
                <li>
                    <strong>Analyze Results and Select Best Configuration</strong>
                    <p>
                        Analyze the results of the hyperparameter search to identify the best configuration and understand
                        the impact of different hyperparameters on model performance.
                    </p>
                    <pre><code>python scripts/analyze_hyperparameter_results.py --results_dir output/hyperparameter_search_large</code></pre>
                </li>
                <li>
                    <strong>Train Final Model with Best Hyperparameters</strong>
                    <p>
                        Train the final model using the best hyperparameters identified during the search.
                    </p>
                    <pre><code>python scripts/run_pipeline.py --config output/hyperparameter_search_large/best_config.yaml --step train</code></pre>
                </li>
            </ol>
        </section>

        <section id="balancing-metrics">
            <h2>Balancing Gene-wise and Cell-wise Metrics</h2>
            <p>
                One of the key aspects of our unified approach is balancing gene-wise and cell-wise Spearman correlation metrics.
                This balance is controlled by the <code>gene_wise_weight</code> and <code>cell_wise_weight</code> hyperparameters.
            </p>
            
            <h3>Impact of Weights on Performance</h3>
            <p>
                The following table shows the impact of different weight combinations on gene-wise and cell-wise Spearman correlation:
            </p>
            <table>
                <thead>
                    <tr>
                        <th>gene_wise_weight</th>
                        <th>cell_wise_weight</th>
                        <th>Gene-wise Spearman</th>
                        <th>Cell-wise Spearman</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>1.0</td>
                        <td>0.0</td>
                        <td>16.168 (DeepSpot-like)</td>
                        <td>45.937</td>
                    </tr>
                    <tr>
                        <td>0.7</td>
                        <td>0.3</td>
                        <td>15.8</td>
                        <td>48.2</td>
                    </tr>
                    <tr>
                        <td>0.5</td>
                        <td>0.5</td>
                        <td>15.5</td>
                        <td>50.1</td>
                    </tr>
                    <tr>
                        <td>0.3</td>
                        <td>0.7</td>
                        <td>15.2</td>
                        <td>51.8</td>
                    </tr>
                    <tr>
                        <td>0.0</td>
                        <td>1.0</td>
                        <td>14.8 (Tarandros-like)</td>
                        <td>52.792</td>
                    </tr>
                </tbody>
            </table>
            
            <h3>Recommended Weight Settings</h3>
            <p>
                Based on our experiments, we recommend the following weight settings for different use cases:
            </p>
            <ul>
                <li><strong>Balanced Performance</strong>: gene_wise_weight=0.5, cell_wise_weight=0.5</li>
                <li><strong>Gene-focused Analysis</strong>: gene_wise_weight=0.7, cell_wise_weight=0.3</li>
                <li><strong>Cell-focused Analysis</strong>: gene_wise_weight=0.3, cell_wise_weight=0.7</li>
            </ul>
            <p>
                For the Crunch 3 task of identifying gene markers that distinguish dysplastic from non-dysplastic regions,
                we found that a cell-focused setting (gene_wise_weight=0.3, cell_wise_weight=0.7) provides the best results,
                as it better preserves the cellular context needed for accurate gene ranking.
            </p>
        </section>

        <section id="monitoring-training">
            <h2>Monitoring Training</h2>
            <p>
                Effective monitoring of the training process is essential for identifying issues and ensuring optimal model performance.
                We use Weights & Biases (W&B) for comprehensive training monitoring and experiment tracking.
            </p>
            
            <h3>Key Metrics to Monitor</h3>
            <ul>
                <li><strong>Training Loss</strong>: Overall training loss combining gene-wise and cell-wise components</li>
                <li><strong>Validation Loss</strong>: Overall validation loss for early stopping and model selection</li>
                <li><strong>Gene-wise Spearman</strong>: Spearman correlation across genes for each cell</li>
                <li><strong>Cell-wise Spearman</strong>: Spearman correlation across cells for each gene</li>
                <li><strong>Learning Rate</strong>: Current learning rate after scheduler adjustments</li>
                <li><strong>Gradient Norm</strong>: L2 norm of gradients for monitoring training stability</li>
            </ul>
            
            <h3>Setting Up W&B Monitoring</h3>
            <ol>
                <li>
                    <strong>Install and Configure W&B</strong>
                    <pre><code>pip install wandb
wandb login</code></pre>
                </li>
                <li>
                    <strong>Enable W&B in Configuration</strong>
                    <pre><code>
# Enable W&B monitoring in configuration
use_wandb: true
wandb_project: spatial_transcriptomics
wandb_entity: your_username
wandb_name: unified_model_run
                    </code></pre>
                </li>
                <li>
                    <strong>Run Training with W&B Monitoring</strong>
                    <pre><code>python scripts/run_pipeline.py --config config/config.yaml --step train</code></pre>
                </li>
                <li>
                    <strong>View Results in W&B Dashboard</strong>
                    <pre><code>wandb dashboard</code></pre>
                </li>
            </ol>
            
            <h3>Interpreting Training Curves</h3>
            <p>
                When monitoring training curves, look for the following patterns:
            </p>
            <ul>
                <li><strong>Smooth Decreasing Loss</strong>: Indicates stable training</li>
                <li><strong>Plateauing Validation Loss</strong>: Indicates convergence</li>
                <li><strong>Diverging Training and Validation Loss</strong>: Indicates overfitting</li>
                <li><strong>Oscillating Loss</strong>: Indicates unstable training, consider reducing learning rate</li>
                <li><strong>Slow Convergence</strong>: Consider increasing learning rate or adjusting batch size</li>
            </ul>
        </section>

        <section id="transfer-learning">
            <h2>Transfer Learning</h2>
            <p>
                Transfer learning can significantly improve model performance and reduce training time by leveraging knowledge
                gained from one dataset to another. Our unified approach supports several transfer learning strategies.
            </p>
            
            <h3>Transfer Learning Strategies</h3>
            <ol>
                <li>
                    <strong>Pre-training on Synthetic Data</strong>
                    <p>
                        Pre-train the model on synthetic data before fine-tuning on real data. This approach helps the model
                        learn general patterns and relationships before adapting to the specific characteristics of real data.
                    </p>
                    <pre><code>
# Pre-train on synthetic data
python scripts/run_pipeline.py --config config/synthetic_config.yaml --step train

# Fine-tune on real data
python scripts/run_transfer_learning.py --source_model output/models/synthetic_model --target_config config/real_config.yaml
                    </code></pre>
                </li>
                <li>
                    <strong>Small to Large Dataset Transfer</strong>
                    <p>
                        Train the model on a small dataset before fine-tuning on the large dataset. This approach allows for
                        rapid initial development and hyperparameter tuning before scaling to the full dataset.
                    </p>
                    <pre><code>
# Train on small dataset
python scripts/run_pipeline.py --config config/small_dataset_config.yaml --step train

# Fine-tune on large dataset
python scripts/run_transfer_learning.py --source_model output/models/small_dataset_model --target_config config/large_dataset_config.yaml
                    </code></pre>
                </li>
                <li>
                    <strong>Feature Extractor Transfer</strong>
                    <p>
                        Use the feature extraction components of a pre-trained model while training new prediction heads.
                        This approach is useful when the input data is similar but the prediction targets are different.
                    </p>
                    <pre><code>
# Train feature extractor
python scripts/run_pipeline.py --config config/feature_extractor_config.yaml --step train

# Transfer feature extractor and train new heads
python scripts/run_transfer_learning.py --source_model output/models/feature_extractor_model --target_config config/prediction_head_config.yaml --transfer_mode feature_extractor
                    </code></pre>
                </li>
            </ol>
            
            <h3>Transfer Learning Configuration</h3>
            <pre><code>
# Transfer learning configuration
transfer_learning:
  source_model: output/models/synthetic_model
  target_config: config/real_config.yaml
  transfer_mode: full  # Options: full, feature_extractor, prediction_head
  freeze_layers: false  # Whether to freeze transferred layers
  learning_rate: 0.0001  # Lower learning rate for fine-tuning
  max_epochs: 100
  early_stopping_patience: 20
            </code></pre>
        </section>
    </main>
    <footer>
        <p>&copy; 2025 Catskills Research. All rights reserved.</p>
    </footer>
</body>
</html>

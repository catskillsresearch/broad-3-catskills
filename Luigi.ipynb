{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00ddc371-4510-4754-ad8b-b18be9eb7e1d",
   "metadata": {},
   "source": [
    "# Broad IBD Challenge Luigi prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee43e44-91db-47f9-a3db-a878b5c6ea47",
   "metadata": {},
   "source": [
    "## Unpack scRNA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfab42b-c2b9-453f-a50d-e282d4ebe319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import spatialdata as sd \n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "from log1p_normalization_scale_factor import log1p_normalization_scale_factor\n",
    "\n",
    "name_data='UC9_I'\n",
    "zarr_path = f\"data/{name_data}.zarr\"\n",
    "sdata = sd.read_zarr(zarr_path)\n",
    "genes460 = sdata['anucleus'].var['gene_symbols'].values.tolist()\n",
    "scRNAseq = sc.read_h5ad('data/Crunch3_scRNAseq.h5ad')\n",
    "\n",
    "genes18615 = list(scRNAseq.var.index)\n",
    "genes458 = [x for x in genes460 if x in genes18615]\n",
    "genes18157 = [gene for gene in genes18615 if gene not in genes458]\n",
    "\n",
    "# scRNA-Seq data log1p-normalized with scale factor 10000 on 18615 genes\n",
    "rna_data_norm_10000_unmeasured_genes = scRNAseq[:, genes18157].X.toarray()\n",
    "# scRNA-Seq data log1p-normalized with scale factor 100 on 460 genes\n",
    "rna_data_norm_100_common_genes = log1p_normalization_scale_factor(scRNAseq[:, genes458].layers[\"counts\"].toarray(), scale_factor=100)\n",
    "\n",
    "os.makedirs('resources/run', exist_ok=True)\n",
    "rundir = 'resources/run'\n",
    "with open(f'{rundir}/genes458.json', 'w') as f:\n",
    "    json.dump(genes458, f)\n",
    "with open(f'{rundir}/genes460.json', 'w') as f:\n",
    "    json.dump(genes460, f)\n",
    "with open(f'{rundir}/genes18615.json', 'w') as f:\n",
    "    json.dump(genes18615, f)\n",
    "with open(f'{rundir}/genes18157.json', 'w') as f:\n",
    "    json.dump(genes18157, f)\n",
    "\n",
    "np.savez_compressed(f'{rundir}/scRNA_18157_gene_expressions', my_array=rna_data_norm_10000_unmeasured_genes)\n",
    "\n",
    "np.savez_compressed(f'{rundir}/scRNA_458_gene_expressions', my_array=rna_data_norm_100_common_genes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dccf3ad-6609-4086-b644-b26b4c0f5060",
   "metadata": {},
   "source": [
    "## Unpack UC9_I Unpack Patches and Genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b77f8d0-dceb-4b46-9a24-9523552e6d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_patch_size=32\n",
    "vis_width=1000\n",
    "dir_dataset='resources/run'\n",
    "dir_processed_dataset = 'resources/run'\n",
    "dir_dataset = os.path.join(dir_dataset, \"chips\")\n",
    "os.makedirs(dir_dataset, exist_ok=True)\n",
    "name_data = 'UC9_I'\n",
    "zarr_path = f\"data/{name_data}.zarr\"\n",
    "\n",
    "import spatialdata as sd  # Manage multi-modal spatial omics datasets\n",
    "sdata = sd.read_zarr(zarr_path)\n",
    "\n",
    "import os\n",
    "rows_to_keep = list(sdata['anucleus'].obs.sample(n=len(sdata['anucleus'].obs)).index)\n",
    "# Extract spatial positions for 'train' cells\n",
    "cell_id_train = sdata['anucleus'].obs[\"cell_id\"].values\n",
    "h5_path = os.path.join(dir_dataset, name_data + '_patches.h5')\n",
    "\n",
    "from extract_spatial_positions import extract_spatial_positions\n",
    "new_spatial_coord_fn = f'{dir_dataset}/{name_data}_spatial_positions.npy'\n",
    "new_spatial_coord = extract_spatial_positions(sdata, cell_id_train)\n",
    "\n",
    "import numpy as np\n",
    "np.save(new_spatial_coord_fn, new_spatial_coord)\n",
    "sdata['anucleus'].obsm['spatial'] = new_spatial_coord\n",
    "\n",
    "# Create the gene expression dataset (Y)\n",
    "y_subtracted = sdata['anucleus'][rows_to_keep].copy()\n",
    "# Trick to set all index to same length to avoid problems when saving to h5\n",
    "y_subtracted.obs.index = ['x' + str(i).zfill(6) for i in y_subtracted.obs.index]\n",
    "# Check\n",
    "for index in y_subtracted.obs.index:\n",
    "    if len(index) != len(y_subtracted.obs.index[0]):\n",
    "        warnings.warn(\"indices of y_subtracted.obs should all have the same length to avoid problems when saving to h5\", UserWarning)\n",
    "\n",
    "# Save the gene expression data to an H5AD file\n",
    "y_subtracted.write(os.path.join(dir_dataset, f'{name_data}_genes.h5ad'))\n",
    "\n",
    "# Extract spatial coordinates and barcodes (cell IDs) for the patches\n",
    "coords_center = y_subtracted.obsm['spatial']\n",
    "barcodes = np.array(y_subtracted.obs.index)\n",
    "\n",
    "# Load the image and transpose it to the correct format\n",
    "intensity_image = np.transpose(sdata['HE_original'].to_numpy(), (1, 2, 0))\n",
    "\n",
    "# Create the patcher object to extract patches (localized square sub-region of an image) from an image at specified coordinates.\n",
    "from Patcher import Patcher\n",
    "patcher = Patcher(\n",
    "    image=intensity_image,\n",
    "    coords=coords_center,\n",
    "    patch_size_target=target_patch_size\n",
    ")\n",
    "\n",
    "# Build and Save patches to an HDF5 file\n",
    "patcher.to_h5(h5_path, extra_assets={'barcode': barcodes})\n",
    "\n",
    "# Visualization\n",
    "print(\"Extracted Images (high time and memory consumption...)\")\n",
    "path1 = os.path.join(dir_dataset, name_data + '_patch_locations_on_image.png')\n",
    "patcher.save_visualization(path1, vis_width=vis_width)\n",
    "\n",
    "print(\"Spatial coordinates\")\n",
    "path2 = os.path.join(dir_dataset, name_data + '_spatial_coordinates.png')\n",
    "patcher.view_coord_points(path2, vis_width=vis_width)\n",
    "\n",
    "from read_assets_from_h5 import read_assets_from_h5\n",
    "# Display some example images from the created dataset\n",
    "print(\"Examples from the created .h5 dataset\")\n",
    "assets, _ = read_assets_from_h5(h5_path)\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "n_images = 3\n",
    "dpi = 150\n",
    "fig, axes = plt.subplots(1, n_images, figsize=(15, 5))\n",
    "for i in range(n_images):\n",
    "    axes[i].imshow(assets[\"img\"][i])\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "path3 = os.path.join(dir_dataset, name_data + '_patch_examples.png')\n",
    "plt.savefig(path3, dpi=dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4d18b3-97af-4592-ab98-0b4c12956ba0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## UC9_I Patches to Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22378f2-cecc-4594-b934-23c0bdb18b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = \"resnet50\"\n",
    "weights_root = \"resources/pytorch_model.bin\"\n",
    "\n",
    "from inf_encoder_factory import inf_encoder_factory\n",
    "encoder = inf_encoder_factory(encoder)(weights_root)\n",
    "\n",
    "patches_path = 'resources/run/UC9_I_patches.h5'\n",
    "embed_path = 'resources/run/UC9_I_features.h5'\n",
    "batch_size = 128\n",
    "num_workers = 0\n",
    "overwrite = True\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate the embeddings and save them to the defined path\n",
    "from generate_embeddings import generate_embeddings\n",
    "generate_embeddings(embed_path, encoder, device, patches_path, batch_size, num_workers, overwrite=overwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24beedcb-b3ab-4f46-a177-689babf8dfa4",
   "metadata": {},
   "source": [
    "## UC9_I Features to PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b980bddb-85d5-4e76-8a71-2ce198221581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_assets_from_h5 import read_assets_from_h5\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from pca_analysis import pca_analysis\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "assets, _ = read_assets_from_h5('resources/run/UC9_I_features.h5')\n",
    "features = assets['embeddings']\n",
    "index = assets['barcodes']\n",
    "plt.hist(features.flatten(), bins=100, density=True)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(\"Chip Resnet50 Feature density\")\n",
    "plt.savefig('resources/run/UC9_I_feature_density.png', dpi=150, bbox_inches='tight')\n",
    "B, scaler, L, V, MSE, pca_mean = pca_analysis(features[::32])\n",
    "joblib.dump(scaler, 'resources/run/UC9_I_feature_PCs_scaler.joblib')\n",
    "np.savez_compressed(f'resources/run/UC9_I_feature_pca_mean.npz', pca_mean)\n",
    "np.savez_compressed('resources/run/UC9_I_pca_basis_explained_variance.npz', V)\n",
    "np.savez_compressed('resources/run/UC9_I_pca_basis_MSE.npz', MSE)\n",
    "mse_goal = 0.16\n",
    "finish = np.where((MSE <=mse_goal))[-1][0]\n",
    "plt.plot(MSE)\n",
    "plt.xlim([finish-20, finish+20])\n",
    "plt.ylim([0.12,0.20])\n",
    "plt.scatter([finish],[mse_goal],color='red', s=40)\n",
    "plt.title(f'Use {finish} PCs for goal of reconstruction MSE <= 0.16')\n",
    "plt.savefig('resources/run/UC9_I_features_PCA_MSE.png', dpi=150, bbox_inches='tight')\n",
    "basis = B[:, :finish]\n",
    "X_scaled = scaler.fit_transform(features)\n",
    "X_centered = X_scaled - pca_mean\n",
    "PCs = X_centered @ basis\n",
    "np.savez_compressed(f'resources/run/UC9_I_feature_PCs.npz', PCs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018a1f68-c286-42e0-ae0b-e46a8cb416a1",
   "metadata": {},
   "source": [
    "## UC9_I Genes 460 to Genes 458"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7d50059b-c1d7-4fdf-be5c-3966d2d8b0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/run/UC9_I_genes.h5ad\n"
     ]
    }
   ],
   "source": [
    "!ls {expr_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a302c674-b7ad-46d3-abdf-948045ea7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_adata import load_adata\n",
    "import json\n",
    "with open('resources/run/genes458.json', 'r') as f:\n",
    "    genes458 = json.load(f)\n",
    "barcodes = np.load('resources/run/UC9_I_barcodes.npy',allow_pickle=True)\n",
    "adata = load_adata('resources/run/UC9_I_genes.h5ad', genes=genes458, barcodes=barcodes, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "064d4c40-eff7-4fef-a883-7e812bcd451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.to_csv('resources/run/UC9_I_genes458.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3e290e-7ece-4796-baa3-2de5be3f72be",
   "metadata": {},
   "source": [
    "## UC9_I Genes 458 to PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496fe330-14f1-462f-b9a1-4316b55e7726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94491fac-17be-4eee-a42b-727f07c02996",
   "metadata": {},
   "source": [
    "## UC9_I Regress Feature PCs to Gene PCs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c362a23b-df61-4c3e-8461-ed66bf125fce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Bioinformatics Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5e2a9c-4f5b-435a-9817-a62c36233108",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff280634-b712-484a-90a2-21d3d03248f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_bioinformatics_plots import generate_bioinformatics_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e953d945-5bf1-4b6b-b4b2-fc4521daae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fn, X_test_fn, y_train_fn, y_test_fn =[\n",
    "'./resources/ST_pred_results/split0/X_train.npy', \n",
    "'./resources/ST_pred_results/split0/X_test.npy', \n",
    "'./resources/ST_pred_results/split0/y_train.npy', \n",
    "'./resources/ST_pred_results/split0/y_test.npy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252da8a0-df5d-4fdb-b6d8-5540bc23c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2976b4e-9d7f-471a-9322-28613ca18aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(X_train_fn).astype(np.float32)\n",
    "X_test = np.load(X_test_fn).astype(np.float32)\n",
    "Y_train = np.load(y_train_fn).astype(np.float32)\n",
    "Y_test = np.load(y_test_fn).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c5ac03-6775-4fa8-956d-f0deeabaa6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24b5e8e-6349-4513-bd50-8f9a2a00d6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa11b6e-4271-4efd-9be9-5bb06ad23c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('resources/processed_dataset/var_genes.json', 'r') as f:\n",
    "    genes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53168f4a-1462-46ca-bdd7-b37e6af0ed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = genes['genes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3196f-b620-49fb-9a5d-43c05142d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1c3009-2a91-44de-a74d-047225303bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Y_train, columns = genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c8a20-292d-4448-98ff-05f29d465bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=list(df.columns), inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cc87da-7932-49ad-9257-400d4b2a437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9b9dc5-e60c-4647-a7a0-3388c3f7d31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1[df1.columns[-20:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aaea6c-2bae-4982-b3f6-86efa0f78155",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_bioinformatics_plots(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720221ea-b98a-4098-aade-ea3b2950adf7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Analyze histology patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891352f5-0758-4ddc-a4ee-e93e0c1bf37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8605ddc8-1d21-4bf5-821e-ae277e97f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyze_histology_patches import analyze_histology_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597642ec-c1ab-42bc-ae62-b5fe2cd6c50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76df1829-a459-45d8-81fb-e3b8fdc826e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3760df5b-36d6-4047-805c-4af1f31a0baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.permutation(len(X_train))[:10000]  # No array copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6af67ca-b117-4f6d-a080-f7526b2a529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train[indices]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01197b71-e22d-46d9-907b-bb97120ad6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Y_train[indices]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd03126-50a8-4242-9395-f7f7a6f9717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c2dba-4035-414b-a6ec-13886acbfa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b41fb52-4dac-4767-bdc3-24bd5228de3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyze_histology_patches import analyze_histology_patches\n",
    "\n",
    "results = analyze_histology_patches(X_scaled)\n",
    "print(f\"Optimal clusters: {results['optimal_k']}\")\n",
    "print(f\"Silhouette scores: {results['silhouette_scores']}\")\n",
    "# Save plots\n",
    "for name, fig in results['plots'].items():\n",
    "    display(fig)\n",
    "    fig.savefig(f\"{name}.png\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba886d9e-4267-4407-8dce-8107a08b66ba",
   "metadata": {},
   "source": [
    "## Try some other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0ae12b-2030-45e9-b5e2-b8c4f00dc3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced = pca.fit_transform(X_scaled)\n",
    "plt.scatter(reduced[:,0], reduced[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373394b0-7ed0-4b2a-9aba-157a51f28e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "import numpy as np\n",
    "\n",
    "def cluster_stability(X, n_clusters=2, n_iter=10):\n",
    "    agreements = []\n",
    "    for _ in range(n_iter):\n",
    "        km1 = KMeans(n_clusters=n_clusters).fit(X)\n",
    "        km2 = KMeans(n_clusters=n_clusters).fit(X)\n",
    "        agreements.append(adjusted_rand_score(km1.labels_, km2.labels_))\n",
    "    return np.mean(agreements)\n",
    "\n",
    "print(f\"Stability Score: {cluster_stability(X_scaled, n_clusters=7)}\")  # Should be >0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6288d1a-302e-4c04-aee6-f83cdfe684b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = [(n,cluster_stability(X_scaled, n_clusters=n)) for n in range(1,15)]\n",
    "search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ad30ee-fa57-4cf9-999f-96d9fa3d5628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "def evaluate_k(X, k):\n",
    "    km = KMeans(n_clusters=k).fit(X)\n",
    "    return {\n",
    "        'stability': cluster_stability(X, k),\n",
    "        'silhouette': silhouette_score(X, km.labels_),\n",
    "        'davies_bouldin': davies_bouldin_score(X, km.labels_)\n",
    "    }\n",
    "\n",
    "metrics = [(k, evaluate_k(X_scaled, k)) for k in tqdm(range(2,15))]\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa80196c-e862-4bb0-8c54-10f40a4286d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace PCA with UMAP for non-linear reduction\n",
    "import umap\n",
    "reducer = umap.UMAP(n_components=15)\n",
    "X_umap = reducer.fit_transform(X_scaled)\n",
    "metrics = [(k, evaluate_k(X_umap, k)) for k in tqdm(range(2,15))]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b99f1b-4bf6-4516-838d-bd691f7d1304",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, n_init=50).fit(X_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f433404-17d6-4050-a60d-fca4fcaae601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal\n",
    "for gene in range(X_scaled.shape[1]):\n",
    "    _, p = kruskal(*[X_scaled[kmeans.labels_==i, gene] for i in range(4)])\n",
    "    if p < 0.001: print(f\"Gene {gene} significant (p={p:.2e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4461cbff-0ad2-49b6-ace8-3330b157a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "models = [\n",
    "    KMeans(n_clusters=4),\n",
    "    AgglomerativeClustering(n_clusters=4)\n",
    "]\n",
    "consensus = np.mean([m.fit_predict(X_umap) for m in models], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f98742-a1d7-453a-91df-a2ed17d0eb87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find indices where both algorithms agree\n",
    "strong_agreement_indices = np.where(np.isin(consensus, [0, 1, 2, 3]))[0]\n",
    "print(f\"Number of points with strong agreement: {len(strong_agreement_indices)}\")\n",
    "# Find indices where algorithms disagree\n",
    "disagreement_indices = np.where(np.isin(consensus, [0.5]))[0]\n",
    "print(f\"Number of points with disagreement: {len(disagreement_indices)}\")\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import numpy as np\n",
    "\n",
    "# Ensure X_umap is defined\n",
    "if 'X_umap' not in locals():\n",
    "    # Assuming X_scaled is your data\n",
    "    reducer = umap.UMAP(n_components=2)\n",
    "    X_umap = reducer.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_umap[:, 0], X_umap[:, 1], c=consensus, cmap='viridis', s=50)\n",
    "plt.colorbar(label='Consensus Value')\n",
    "plt.title('UMAP Visualization with Consensus Clustering')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175f8679-c908-496f-9b27-c32badcf53a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## And again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26750d7-9b21-441d-a386-6a54937b9d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA, NMF, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Assuming X is your standardized 10000 x 460 matrix\n",
    "X = X_scaled\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, _, _ = train_test_split(X, X, test_size=0.2)\n",
    "\n",
    "n_components = 15  # Number of components for most methods\n",
    "\n",
    "methods = {\n",
    "    \"PCA\": PCA(n_components=8),\n",
    "    \"UMAP\": umap.UMAP(n_components=4),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, method in methods.items():\n",
    "    print(f\"Processing {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Fit and transform the training data\n",
    "        X_reduced_train = method.fit_transform(X_train)\n",
    "        \n",
    "        # Transform the test data\n",
    "        if name == \"t-SNE\":\n",
    "            # t-SNE doesn't have a transform method for new data\n",
    "            X_reduced_test = TSNE(n_components=tsne_components, method='exact', \n",
    "                                 random_state=42, n_iter=250).fit_transform(X_test)\n",
    "        else:\n",
    "            X_reduced_test = method.transform(X_test)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Reconstruct the data (approximate inverse transform)\n",
    "        if name == \"UMAP\":\n",
    "            # UMAP doesn't have a direct inverse transform, so use Nearest Neighbors\n",
    "            nn = NearestNeighbors(n_neighbors=1, metric='euclidean')\n",
    "            nn.fit(X_reduced_train)  # Train on reduced training data\n",
    "            distances, indices = nn.kneighbors(X_reduced_test)\n",
    "            # Find the original high-dimensional points corresponding to the reduced points\n",
    "            X_reconstructed = X_train[indices.flatten()].reshape(X_test.shape)\n",
    "        elif name == \"t-SNE\":\n",
    "            # t-SNE doesn't have an inverse transform\n",
    "            X_reconstructed = X_test\n",
    "        else:\n",
    "            X_reconstructed = method.inverse_transform(X_reduced_test)\n",
    "        \n",
    "        # Calculate MSE\n",
    "        mse = mean_squared_error(X_test, X_reconstructed)\n",
    "        \n",
    "        results[name] = {\n",
    "            \"MSE\": mse,\n",
    "            \"Time\": end_time - start_time\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {name}: {str(e)}\")\n",
    "        results[name] = {\n",
    "            \"MSE\": float('nan'),\n",
    "            \"Time\": float('nan')\n",
    "        }\n",
    "\n",
    "# Print Results\n",
    "for name, result in results.items():\n",
    "    print(f\"Method: {name}\")\n",
    "    print(f\"  MSE: {result['MSE']:.4f}\")\n",
    "    print(f\"  Time: {result['Time']:.4f} seconds\")\n",
    "\n",
    "# Filter out methods with NaN MSE for plotting\n",
    "valid_results = {k: v for k, v in results.items() if not np.isnan(v[\"MSE\"])}\n",
    "\n",
    "if valid_results:\n",
    "    # Bar plot for MSE\n",
    "    names = list(valid_results.keys())\n",
    "    mse_values = [result['MSE'] for result in valid_results.values()]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(names, mse_values, color='skyblue')\n",
    "    plt.xlabel('Method')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.title('MSE Comparison for Different Dimensionality Reduction Methods')\n",
    "    plt.ylim(0, max(mse_values) * 1.1)  # Adjust y-axis limit\n",
    "    plt.show()\n",
    "\n",
    "    # Bar plot for time\n",
    "    time_values = [result['Time'] for result in valid_results.values()]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(names, time_values, color='lightgreen')\n",
    "    plt.xlabel('Method')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.title('Time Comparison for Different Dimensionality Reduction Methods')\n",
    "    plt.ylim(0, max(time_values) * 1.1)  # Adjust y-axis limit\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid results to plot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81b0032-41d3-4657-a116-0eeb2cc78f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize_pca_clusters import visualize_pca_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bcc30f-6d9b-4bf3-b9a8-6e75b9d8cec4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Assuming X_scaled is your standardized 10000 x 460 matrix\n",
    "visualize_pca_clusters(X_scaled, n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef1492e-a119-4390-afe0-97796cdf7d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import umap  # Import UMAP\n",
    "\n",
    "def visualize_clusters(X, n_components, method='pca'):\n",
    "    \"\"\"\n",
    "    Applies either PCA or UMAP for initial dimension reduction, then KMeans clustering,\n",
    "    and visualizes the results using UMAP for 2D projection.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Standardized data matrix (samples x features).\n",
    "        n_components (int): Number of components and clusters to use.\n",
    "        method (str): 'pca' or 'umap', specifying the dimension reduction method.\n",
    "    \"\"\"\n",
    "    if method == 'pca':\n",
    "        # Apply PCA\n",
    "        reducer = PCA(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    elif method == 'umap':\n",
    "        # Apply UMAP for initial dimension reduction\n",
    "        reducer = umap.UMAP(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pca' or 'umap'\")\n",
    "\n",
    "    # Apply KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=n_components, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_reduced)\n",
    "\n",
    "    # Visualize using UMAP for 2D projection\n",
    "    reducer_umap = umap.UMAP(n_components=2)\n",
    "    X_umap = reducer_umap.fit_transform(X_reduced)\n",
    "\n",
    "    # Create a scatter plot of the reduced data, colored by cluster labels\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=cluster_labels, cmap='viridis', s=5)\n",
    "    plt.title(f'{method.upper()}-Reduced Data with KMeans Clusters (k={n_components})')\n",
    "    plt.xlabel('UMAP Component 1')\n",
    "    plt.ylabel('UMAP Component 2')\n",
    "\n",
    "    # Add a legend\n",
    "    legend1 = plt.legend(*scatter.legend_elements(),\n",
    "                         loc=\"upper right\", title=\"Clusters\")\n",
    "    plt.gca().add_artist(legend1)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X_scaled is your standardized 10000 x 460 matrix\n",
    "visualize_clusters(X_scaled, n_components=8, method='pca')\n",
    "visualize_clusters(X_scaled, n_components=8, method='umap')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaca4d2-9427-408e-8271-a8b3c9610518",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_clusters(X_scaled, n_components=4, method='pca')\n",
    "visualize_clusters(X_scaled, n_components=4, method='umap')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9596c3-249e-44de-9929-58a00a10e5a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import umap  # Import UMAP\n",
    "\n",
    "def visualize_clusters(X, n_components, method='pca'):\n",
    "    \"\"\"\n",
    "    Applies either PCA or UMAP for initial dimension reduction, then KMeans clustering,\n",
    "    and visualizes the results using UMAP for 2D projection.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Standardized data matrix (samples x features).\n",
    "        n_components (int): Number of components and clusters to use.\n",
    "        method (str): 'pca' or 'umap', specifying the dimension reduction method.\n",
    "    \"\"\"\n",
    "    if method == 'pca':\n",
    "        # Apply PCA\n",
    "        reducer = PCA(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    elif method == 'umap':\n",
    "        # Apply UMAP for initial dimension reduction\n",
    "        reducer = umap.UMAP(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pca' or 'umap'\")\n",
    "\n",
    "    # Apply KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=n_components, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_reduced)\n",
    "\n",
    "    # Visualize using UMAP for 2D projection\n",
    "    reducer_umap = umap.UMAP(n_components=2)\n",
    "    X_umap = reducer_umap.fit_transform(X_reduced)\n",
    "\n",
    "    # Create a scatter plot of the reduced data, colored by cluster labels\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=cluster_labels, cmap='viridis', s=5)\n",
    "    plt.title(f'{method.upper()}-Reduced Data with KMeans Clusters (k={n_components})')\n",
    "    plt.xlabel('UMAP Component 1')\n",
    "    plt.ylabel('UMAP Component 2')\n",
    "\n",
    "    # Add a legend\n",
    "    legend1 = plt.legend(*scatter.legend_elements(),\n",
    "                         loc=\"upper right\", title=\"Clusters\")\n",
    "    plt.gca().add_artist(legend1)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X_scaled is your standardized 10000 x 460 matrix\n",
    "visualize_clusters(X_scaled, n_components=4, method='pca')\n",
    "visualize_clusters(X_scaled, n_components=4, method='umap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58351a2-e3af-493e-956d-946da14fc48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import umap  # Import UMAP\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "\n",
    "def visualize_clusters(X, n_components, method='pca'):\n",
    "    \"\"\"\n",
    "    Applies either PCA or UMAP for initial dimension reduction, then KMeans clustering,\n",
    "    and visualizes the results using UMAP for 2D projection.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Standardized data matrix (samples x features).\n",
    "        n_components (int): Number of components to use.\n",
    "        method (str): 'pca' or 'umap', specifying the dimension reduction method.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dimension Reduction Step\n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    elif method == 'umap':\n",
    "        reducer = umap.UMAP(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    elif method == 'random_projection':\n",
    "        reducer = GaussianRandomProjection(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pca', 'umap', or 'random_projection'\")\n",
    "\n",
    "    # Clustering Step (KMeans)\n",
    "    kmeans = KMeans(n_clusters=n_components, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_reduced)\n",
    "    \n",
    "    # Visualization Step (UMAP)\n",
    "    reducer_umap = umap.UMAP(n_components=2)\n",
    "    X_umap = reducer_umap.fit_transform(X_reduced)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=cluster_labels, cmap='viridis', s=5)\n",
    "    plt.title(f'{method.upper()}-Reduced Data with KMeans Clusters (k={n_components})')\n",
    "    plt.xlabel('UMAP Component 1')\n",
    "    plt.ylabel('UMAP Component 2')\n",
    "    plt.colorbar(label='Cluster')  # Add colorbar for better interpretation\n",
    "\n",
    "    # Add a legend\n",
    "    legend1 = plt.legend(*scatter.legend_elements(),\n",
    "                         loc=\"upper right\", title=\"Clusters\")\n",
    "    plt.gca().add_artist(legend1)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X_scaled is your standardized 10000 x 460 matrix\n",
    "X = X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4689b5-3e3f-4098-b1da-777f8f33d2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now test each visualization method, and pick different numbers of principle components (10)\n",
    "n_components=5\n",
    "visualize_clusters(X, n_components=n_components, method='pca')\n",
    "visualize_clusters(X, n_components=n_components, method='umap')\n",
    "visualize_clusters(X, n_components=n_components, method='random_projection')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546aa1a5-94de-4577-9f24-8c2b2c8086c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## PCA k=6 for 460 gene data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7f08d-865d-4e3a-b61d-d28a182942c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import umap  # Import UMAP\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def evaluate_clustering(X, n_components, method='pca'):\n",
    "    \"\"\"\n",
    "    Applies either PCA or UMAP for initial dimension reduction, then KMeans clustering,\n",
    "    and calculates Silhouette Score and Davies-Bouldin Index.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Standardized data matrix (samples x features).\n",
    "        n_components (int): Number of components to use.\n",
    "        method (str): 'pca' or 'umap', specifying the dimension reduction method.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing Silhouette Score and Davies-Bouldin Index.\n",
    "    \"\"\"\n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    elif method == 'umap':\n",
    "        reducer = umap.UMAP(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pca' or 'umap'\")\n",
    "\n",
    "    # Clustering with KMeans\n",
    "    kmeans = KMeans(n_clusters=n_components, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_reduced)\n",
    "\n",
    "    # Evaluate clustering\n",
    "    silhouette = silhouette_score(X_reduced, cluster_labels)\n",
    "    db_index = davies_bouldin_score(X_reduced, cluster_labels)\n",
    "\n",
    "    return {'Silhouette Score': silhouette, 'Davies-Bouldin Index': db_index, 'cluster_labels': cluster_labels, 'X_reduced': X_reduced}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e9a21-683f-4a44-b7f5-ae1b8a79259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(X, n_components, method='pca'):\n",
    "    \"\"\"\n",
    "    Visualizes clusters after applying either PCA or UMAP for initial dimension reduction,\n",
    "    then KMeans clustering. Adds centroid labels.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Standardized data matrix (samples x features).\n",
    "        n_components (int): Number of components to use.\n",
    "        method (str): 'pca' or 'umap', specifying the dimension reduction method.\n",
    "    \"\"\"\n",
    "    # Evaluate clustering to get cluster labels and reduced data\n",
    "    eval_results = evaluate_clustering(X, n_components, method)\n",
    "    cluster_labels = eval_results['cluster_labels']\n",
    "    X_reduced = eval_results['X_reduced']\n",
    "    \n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    elif method == 'umap':\n",
    "        reducer = umap.UMAP(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pca' or 'umap'\")\n",
    "\n",
    "    # Clustering with KMeans\n",
    "    kmeans = KMeans(n_clusters=n_components, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_reduced)\n",
    "\n",
    "    # Visualize using UMAP for 2D projection\n",
    "    reducer_umap = umap.UMAP(n_components=2)\n",
    "    X_umap = reducer_umap.fit_transform(X_reduced)\n",
    "\n",
    "    # Calculate centroids\n",
    "    centroids = []\n",
    "    for i in range(n_components):\n",
    "        centroids.append(np.mean(X_umap[cluster_labels == i], axis=0))\n",
    "    centroids = np.array(centroids)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=cluster_labels, cmap='viridis', s=5)\n",
    "    plt.title(f'{method.upper()}-Reduced Data with KMeans Clusters (k={n_components})')\n",
    "    plt.xlabel('UMAP Component 1')\n",
    "    plt.ylabel('UMAP Component 2')\n",
    "    plt.colorbar(label='Cluster')  # Add colorbar for better interpretation\n",
    "    \n",
    "    # Add centroid labels\n",
    "    for i, centroid in enumerate(centroids):\n",
    "        plt.text(centroid[0], centroid[1], str(i + 1), color='magenta', fontsize=16, ha='center', va='center', weight='bold')\n",
    "        \n",
    "    # Add a legend\n",
    "    legend1 = plt.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Clusters\")\n",
    "    plt.gca().add_artist(legend1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f83e1-d5c6-470d-9942-3c66dd47d524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Assuming X_scaled is your standardized 10000 x 460 matrix\n",
    "X = X_scaled\n",
    "\n",
    "# Evaluate different methods and number of components\n",
    "results = {}\n",
    "for method in ['pca', 'umap']:\n",
    "    results[method] = {}\n",
    "    for n_components in range(2, 11):  # Test from 2 to 10 components\n",
    "        metrics = evaluate_clustering(X, n_components, method=method)\n",
    "        results[method][n_components] = metrics\n",
    "        print(f\"Method: {method}, Components: {n_components}, Silhouette Score: {metrics['Silhouette Score']:.4f}, Davies-Bouldin Index: {metrics['Davies-Bouldin Index']:.4f}\")\n",
    "\n",
    "# Visualize the best result based on Silhouette Score (you can change this to Davies-Bouldin Index)\n",
    "best_method = max(results, key=lambda m: max(results[m], key=lambda k: results[m][k]['Silhouette Score']))\n",
    "best_n_components = max(results[best_method], key=lambda k: results[best_method][k]['Silhouette Score'])\n",
    "\n",
    "print(f\"Best Method: {best_method}, Best Components: {best_n_components}\")\n",
    "visualize_clusters(X, n_components=best_n_components, method=best_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10484456-316d-4e18-8e5d-89553493a47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import umap  # Import UMAP\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def evaluate_clustering(X, n_components, method='pca'):\n",
    "    \"\"\"\n",
    "    Applies either PCA or UMAP for initial dimension reduction, then KMeans clustering,\n",
    "    and calculates Silhouette Score and Davies-Bouldin Index.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Standardized data matrix (samples x features).\n",
    "        n_components (int): Number of components to use.\n",
    "        method (str): 'pca' or 'umap', specifying the dimension reduction method.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing Silhouette Score and Davies-Bouldin Index.\n",
    "    \"\"\"\n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    elif method == 'umap':\n",
    "        reducer = umap.UMAP(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pca' or 'umap'\")\n",
    "\n",
    "    # Clustering with KMeans\n",
    "    kmeans = KMeans(n_clusters=n_components, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_reduced)\n",
    "\n",
    "    # Evaluate clustering\n",
    "    silhouette = silhouette_score(X_reduced, cluster_labels)\n",
    "    db_index = davies_bouldin_score(X_reduced, cluster_labels)\n",
    "\n",
    "    return {'Silhouette Score': silhouette, 'Davies-Bouldin Index': db_index, 'cluster_labels': cluster_labels, 'X_reduced': X_reduced}\n",
    "\n",
    "def visualize_clusters(X, n_components, method='pca', ax=None):\n",
    "    \"\"\"\n",
    "    Visualizes clusters after applying either PCA or UMAP for initial dimension reduction,\n",
    "    then KMeans clustering. Adds centroid labels.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Standardized data matrix (samples x features).\n",
    "        n_components (int): Number of components to use.\n",
    "        method (str): 'pca' or 'umap', specifying the dimension reduction method.\n",
    "        ax (matplotlib.axes._axes.Axes, optional): The axes object to draw the plot onto. If None, creates a new figure and axes.\n",
    "    \"\"\"\n",
    "    # Evaluate clustering to get cluster labels and reduced data\n",
    "    eval_results = evaluate_clustering(X, n_components, method)\n",
    "    cluster_labels = eval_results['cluster_labels']\n",
    "    X_reduced = eval_results['X_reduced']\n",
    "    silhouette = eval_results['Silhouette Score']\n",
    "    db_index = eval_results['Davies-Bouldin Index']\n",
    "    \n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    elif method == 'umap':\n",
    "        reducer = umap.UMAP(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pca' or 'umap'\")\n",
    "\n",
    "    # Clustering with KMeans\n",
    "    kmeans = KMeans(n_clusters=n_components, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_reduced)\n",
    "\n",
    "    # Visualize using UMAP for 2D projection\n",
    "    reducer_umap = umap.UMAP(n_components=2)\n",
    "    X_umap = reducer_umap.fit_transform(X_reduced)\n",
    "\n",
    "    # Calculate centroids\n",
    "    centroids = []\n",
    "    for i in range(n_components):\n",
    "        centroids.append(np.mean(X_umap[cluster_labels == i], axis=0))\n",
    "    centroids = np.array(centroids)\n",
    "\n",
    "    # Plotting\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    scatter = ax.scatter(X_umap[:, 0], X_umap[:, 1], c=cluster_labels, cmap='viridis', s=5)\n",
    "    ax.set_title(f'{method.upper()} (k={n_components})\\nSilhouette={silhouette:.2f}, DBIndex={db_index:.2f}')\n",
    "    ax.set_xlabel('UMAP Component 1')\n",
    "    ax.set_ylabel('UMAP Component 2')\n",
    "    \n",
    "    # Add centroid labels\n",
    "    for i, centroid in enumerate(centroids):\n",
    "        ax.text(centroid[0], centroid[1], str(i), color=plt.cm.viridis(i/n_components), fontsize=12, ha='center', va='center')\n",
    "\n",
    "    # Add a legend\n",
    "    legend1 = ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Clusters\")\n",
    "    ax.add_artist(legend1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7d87b0-3f1a-437f-9fe8-dc253f2e903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Assuming X_scaled is your standardized 10000 x 460 matrix\n",
    "X = X_scaled\n",
    "\n",
    "# Define the range of n_components to test\n",
    "n_components_range = range(4, 9)  # From 4 to 8\n",
    "\n",
    "# Create a figure with subplots for each n_components\n",
    "fig, axes = plt.subplots(len(n_components_range), 2, figsize=(20, 8 * len(n_components_range)))\n",
    "axes = axes.flatten()  # Flatten the 2D array of axes for easier indexing\n",
    "\n",
    "# Loop over each n_components and visualize PCA and UMAP side by side\n",
    "for i, n_components in enumerate(n_components_range):\n",
    "    visualize_clusters(X, n_components=n_components, method='pca', ax=axes[2*i])\n",
    "    visualize_clusters(X, n_components=n_components, method='umap', ax=axes[2*i + 1])\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdf9cb0-8042-4471-b14c-e52bf0450c8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3fccc8-59b9-438b-b568-040cd4aed033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Assuming X_scaled is your standardized 10000 x 460 matrix\n",
    "X = X_scaled\n",
    "\n",
    "# Define the range of n_components to test\n",
    "n_components_range = range(4, 9)  # From 4 to 8\n",
    "\n",
    "# Loop over each n_components and visualize PCA and UMAP separately\n",
    "for n_components in n_components_range:\n",
    "    print(f\"Plotting for n_components = {n_components}...\")\n",
    "    visualize_clusters(X, n_components=n_components, method='pca')\n",
    "    visualize_clusters(X, n_components=n_components, method='umap')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876315fc-74bf-4306-b364-6a3a1204d68a",
   "metadata": {},
   "source": [
    "## PCA final decision for patch 1024 feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7e3c4b-e6e4-4061-8076-c2fd746097e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_scaled.shape\n",
    "\n",
    "X = X_scaled[0:1000]\n",
    "\n",
    "# Define the range of n_components to test\n",
    "n_components_range = range(2,9)\n",
    "\n",
    "# Loop over each n_components and visualize PCA and UMAP separately\n",
    "for n_components in n_components_range:\n",
    "    print(f\"Plotting for n_components = {n_components}...\")\n",
    "    visualize_clusters(X, n_components=n_components, method='pca')\n",
    "    # visualize_clusters(X, n_components=n_components, method='umap')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b246c37-3d69-4f60-95f9-e1e4a5184684",
   "metadata": {},
   "source": [
    "## Ridge Regress PCA of genes from PCA of feature vectors and measure MSE and Spearman rank correlation at that level, save fit as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfc35c6-a415-4829-ae64-3c46eac663ed",
   "metadata": {},
   "source": [
    "## Project from PCA of genes to genes and measure MSE and Spearman rank correlation at that level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9beafc5-24e6-4e51-b2c1-a9d63926350e",
   "metadata": {},
   "source": [
    "## PCA of 18157 genes less the 460 genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e9e800-df81-4008-8fef-d7edb7a79ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "scRNAseq = sc.read_h5ad('data/Crunch3_scRNAseq.h5ad')\n",
    "\n",
    "filter_value_no_cancer = \"n\" # Filtered column value indicating absence of cancer\n",
    "filter_value_cancer = \"y\" # Filtered column value indicating the presence of cancer\n",
    "filter_column_scRNAseq = \"dysplasia\"\n",
    "\n",
    "# Filter scRNAseq data by dysplasia status\n",
    "scRNAseq_no_cancer = scRNAseq[scRNAseq.obs[filter_column_scRNAseq] == filter_value_no_cancer].copy()\n",
    "scRNAseq_cancer = scRNAseq[scRNAseq.obs[filter_column_scRNAseq] == filter_value_cancer].copy()\n",
    "\n",
    "scRNAseq_no_cancer\n",
    "\n",
    "name_data = 'UC9_I'\n",
    "\n",
    "# Get the 18615 genes to rank\n",
    "gene_18615_list = list(scRNAseq.var.index)\n",
    "\n",
    "gene_460 = genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52892df4-1898-4b4e-a2c7-c78c352cd79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_genes = [g for g in gene_460 if g in gene_18615_list]\n",
    "print(\"Number of shared genes between scRNA-seq and xenium data:\", len(common_genes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcf235c-6408-466b-a0a9-373e2ad62c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmeasured_genes = [gene for gene in gene_18615_list if gene not in common_genes]\n",
    "print(\"Number of unmeasured genes in Xenium data:\", len(unmeasured_genes), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ba661-9c90-460e-8611-f0a2cef735bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scRNA-Seq data log1p-normalized with scale factor 10000 on 18615 genes\n",
    "rna_data_norm_10000_unmeasured_genes = scRNAseq[:, unmeasured_genes].X.toarray()\n",
    "\n",
    "from log1p_normalization_scale_factor import log1p_normalization_scale_factor\n",
    "\n",
    "# scRNA-Seq data log1p-normalized with scale factor 100 on 460 genes\n",
    "rna_data_norm_100_common_genes = log1p_normalization_scale_factor(scRNAseq[:, common_genes].layers[\"counts\"].toarray(), scale_factor=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a256fb-be46-4cb0-b828-f9f2b5aeb8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_data_norm_100_common_genes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee49abc2-9e59-410b-91cf-4464021e27f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Xenium data shape ({len(prediction_460_genes)} samples x {len(common_genes)} shared genes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b202fb3-62e5-41ca-9616-596db645f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity-Based Matching: Find the top_k most similar spots for each query\n",
    "top_k = 30\n",
    "print(f\"\\nCompute COSINE SIMILARITY: Find the top_k(={top_k}) similar scRNA-Seq cells for each Xenium cell...\\n\")\n",
    "matches, similarities = find_matches_cos_similarity(rna_data_norm_100_common_genes, prediction_460_genes[common_genes].values, top_k=top_k)\n",
    "del rna_data_norm_100_common_genes\n",
    "\n",
    "# Weighted Averaging of scRNA-Seq data log1p-normalized with scale factor 10000\n",
    "print(\"Compute WEIGHTED AVERAGE of unmeasured genes from scRNA-Seq based on similarity scores...\")\n",
    "weighted_avg_df_10000 = pd.DataFrame([\n",
    "    {\n",
    "        **dict(zip(unmeasured_genes, np.average(rna_data_norm_10000_unmeasured_genes[indices, :], axis=0, weights=similarity).round(2)))  # updated\n",
    "    }\n",
    "    for i, (indices, similarity) in enumerate(zip(matches, similarities))\n",
    "])\n",
    "weighted_avg_df_10000.index = prediction_460_genes.index\n",
    "\n",
    "# Free memory by deleting large variables and performing garbage collection\n",
    "del rna_data_norm_10000_unmeasured_genes, matches, similarities\n",
    "\n",
    "prediction_18615_genes = pd.concat([prediction_460_genes, weighted_avg_df_10000], axis=1)[gene_18615_list]\n",
    "\n",
    "print(f\"\\n-- {name_data} PREDICTION DONE --\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "broad",
   "language": "python",
   "name": "broad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

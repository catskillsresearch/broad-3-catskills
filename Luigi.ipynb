{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00ddc371-4510-4754-ad8b-b18be9eb7e1d",
   "metadata": {},
   "source": [
    "# Broad IBD Challenge Luigi prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee43e44-91db-47f9-a3db-a878b5c6ea47",
   "metadata": {},
   "source": [
    "## Unpack scRNA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00841b88-12b8-4a9e-895d-9133d5171852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/importlib/__init__.py:126: FutureWarning: The legacy Dask DataFrame implementation is deprecated and will be removed in a future version. Set the configuration option `dataframe.query-planning` to `True` or None to enable the new Dask Dataframe implementation and silence this warning.\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zarr_path data/UC9_I.zarr True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/catskills/Desktop/broad/broad/lib/python3.10/site-packages/zarr/creation.py:614: UserWarning: ignoring keyword argument 'read_only'\n",
      "  compressor, fill_value = _kwargs_compat(compressor, fill_value, kwargs)\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import spatialdata as sd \n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "from log1p_normalization_scale_factor import log1p_normalization_scale_factor\n",
    "\n",
    "name_data='UC9_I'\n",
    "zarr_path = f\"data/{name_data}.zarr\"\n",
    "sdata = sd.read_zarr(zarr_path)\n",
    "genes460 = sdata['anucleus'].var['gene_symbols'].values.tolist()\n",
    "scRNAseq = sc.read_h5ad('data/Crunch3_scRNAseq.h5ad')\n",
    "\n",
    "genes18615 = list(scRNAseq.var.index)\n",
    "genes458 = [x for x in genes460 if x in genes18615]\n",
    "genes18157 = [gene for gene in genes18615 if gene not in genes458]\n",
    "\n",
    "# scRNA-Seq data log1p-normalized with scale factor 10000 on 18615 genes\n",
    "rna_data_norm_10000_unmeasured_genes = scRNAseq[:, genes18157].X.toarray()\n",
    "# scRNA-Seq data log1p-normalized with scale factor 100 on 460 genes\n",
    "rna_data_norm_100_common_genes = log1p_normalization_scale_factor(scRNAseq[:, genes458].layers[\"counts\"].toarray(), scale_factor=100)\n",
    "\n",
    "os.makedirs('resources/run', exist_ok=True)\n",
    "rundir = 'resources/run'\n",
    "with open(f'{rundir}/genes460.json', 'w') as f:\n",
    "    json.dump(genes460, f)\n",
    "with open(f'{rundir}/genes18615.json', 'w') as f:\n",
    "    json.dump(genes18615, f)\n",
    "with open(f'{rundir}/genes18157.json', 'w') as f:\n",
    "    json.dump(genes18157, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caf52799-f115-4adc-8cfb-5e4111f0d990",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(f'{rundir}/scRNA_18157_gene_expressions', my_array=rna_data_norm_10000_unmeasured_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8b86c09-aed8-4baa-bc64-59061d307d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(f'{rundir}/scRNA_458_gene_expressions', my_array=rna_data_norm_100_common_genes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18992ffd-1ae8-44b8-a980-b84d92980c01",
   "metadata": {},
   "source": [
    "## Input data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ef3d99f-caac-493a-92f9-1b2074d5cdbd",
   "metadata": {},
   "source": [
    "### Crunch 1 \n",
    "\n",
    "UC1_I.zarr\n",
    "UC9_I.zarr\n",
    "\n",
    "### Crunch 2\n",
    "\n",
    "Crunch3_gene_list.csv\n",
    "Crunch3_scRNAseq.h5ad\n",
    "\n",
    "### Crunch 3\n",
    "\n",
    "UC9_I-crunch3-HE-dysplasia-ROI.tif\n",
    "UC9_I-crunch3-HE-label-stardist.tif\n",
    "UC9_I-crunch3-HE.tif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babf63a8-fa13-49b8-9f3e-ac8a3329cb90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " !ls data | grep ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e204b2c9-717f-4f3f-9bce-b2b75072617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_data = 'UC1_I'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1086c7-61c7-46ef-93aa-9099a60b89b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory_path = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68e384-be85-428b-9cc0-aa00327df5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spatialdata as sd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ca4eb9-2eb5-4cce-9ee9-08486357e233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ed33c9-1bb6-42eb-bdbb-b9e442caefa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0940351b-2550-4cf5-80f8-200635e4ffca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0955b1-148f-4583-858b-13810b1aed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_subset = len(sdata['anucleus'].obs)\n",
    "rows_to_keep = list(sdata['anucleus'].obs.sample(n=size_subset).index)\n",
    "cell_id_train = sdata['anucleus'].obs[\"cell_id\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5075b161-12aa-4c43-a841-1830276941dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_processed_dataset = 'resources/processed_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96237a62-d9c6-4d5f-b959-d8504510ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_save_dir = os.path.join(dir_processed_dataset, \"patches\")\n",
    "adata_save_dir = os.path.join(dir_processed_dataset, \"adata\")\n",
    "splits_save_dir = os.path.join(dir_processed_dataset, \"splits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f93b9f-4781-44d3-a0de-0b6c8a62d371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for the .h5 image dataset\n",
    "h5_path = os.path.join(patch_save_dir, name_data + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34261cc-934e-4722-a21d-8e262fc3aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdata['anucleus'].obsm['spatial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44646ce-6a8f-4ffb-9122-4e3d3e965534",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in vars(sdata)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437ab87d-0325-4af7-8938-fea70d0c195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_spatial_positions import extract_spatial_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914b5b35-d6d6-426c-b249-56f355efb292",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_spatial_coord = extract_spatial_positions(sdata, cell_id_train)\n",
    "# Store new spatial coordinates into sdata\n",
    "sdata['anucleus'].obsm['spatial'] = new_spatial_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7192f62-b11d-404a-aacc-0c79e93dcfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sdata['anucleus'][0].to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc48e794-214f-4586-9a56-8abc8e07982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, (df != 0).any(axis=0)].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451c463e-10e0-4e0e-a510-e950583bb890",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Create the gene expression dataset (Y)\n",
    "    print(\"Create gene expression dataset (Y) ...\")\n",
    "    y_subtracted = sdata['anucleus'][rows_to_keep].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de10b90-79ad-4131-9db5-170f262a3501",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Trick to set all index to same length to avoid problems when saving to h5\n",
    "    y_subtracted.obs.index = ['x' + str(i).zfill(6) for i in y_subtracted.obs.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e8d6a4-325d-44a8-8be1-88a1a99ac13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_subtracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6717f191-7867-4e72-b87a-9b9cf747687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(adata_save_dir, f'{name_data}.h5ad')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc1c154-e47a-4453-9e50-a51c3de75390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the gene expression data to an H5AD file\n",
    "y_subtracted.write(os.path.join(adata_save_ dir, f'{name_data}.h5ad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdd14de-dc9f-49ba-bac5-c1b0b0836463",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in y_subtracted.obs.index:\n",
    "    if len(index) != len(y_subtracted.obs.index[0]):\n",
    "        warnings.warn(\"indices of y_subtracted.obs should all have the same length to avoid problems when saving to h5\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6e29ee-4e66-4444-9a6b-821b401c2be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e889c5c-f322-4bd5-88c6-84594c8f7e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract spatial coordinates and barcodes (cell IDs) for the patches\n",
    "coords_center = y_subtracted.obsm['spatial']\n",
    "barcodes = np.array(y_subtracted.obs.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140bfaf1-43b4-4df6-b44f-6fd334ace5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "barcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c362a23b-df61-4c3e-8461-ed66bf125fce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Bioinformatics Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5e2a9c-4f5b-435a-9817-a62c36233108",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff280634-b712-484a-90a2-21d3d03248f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_bioinformatics_plots import generate_bioinformatics_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e953d945-5bf1-4b6b-b4b2-fc4521daae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fn, X_test_fn, y_train_fn, y_test_fn =[\n",
    "'./resources/ST_pred_results/split0/X_train.npy', \n",
    "'./resources/ST_pred_results/split0/X_test.npy', \n",
    "'./resources/ST_pred_results/split0/y_train.npy', \n",
    "'./resources/ST_pred_results/split0/y_test.npy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252da8a0-df5d-4fdb-b6d8-5540bc23c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2976b4e-9d7f-471a-9322-28613ca18aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(X_train_fn).astype(np.float32)\n",
    "X_test = np.load(X_test_fn).astype(np.float32)\n",
    "Y_train = np.load(y_train_fn).astype(np.float32)\n",
    "Y_test = np.load(y_test_fn).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c5ac03-6775-4fa8-956d-f0deeabaa6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24b5e8e-6349-4513-bd50-8f9a2a00d6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa11b6e-4271-4efd-9be9-5bb06ad23c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('resources/processed_dataset/var_genes.json', 'r') as f:\n",
    "    genes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53168f4a-1462-46ca-bdd7-b37e6af0ed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = genes['genes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3196f-b620-49fb-9a5d-43c05142d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1c3009-2a91-44de-a74d-047225303bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Y_train, columns = genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c8a20-292d-4448-98ff-05f29d465bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=list(df.columns), inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cc87da-7932-49ad-9257-400d4b2a437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9b9dc5-e60c-4647-a7a0-3388c3f7d31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1[df1.columns[-20:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aaea6c-2bae-4982-b3f6-86efa0f78155",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_bioinformatics_plots(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720221ea-b98a-4098-aade-ea3b2950adf7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Analyze histology patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891352f5-0758-4ddc-a4ee-e93e0c1bf37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8605ddc8-1d21-4bf5-821e-ae277e97f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyze_histology_patches import analyze_histology_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597642ec-c1ab-42bc-ae62-b5fe2cd6c50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76df1829-a459-45d8-81fb-e3b8fdc826e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3760df5b-36d6-4047-805c-4af1f31a0baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.permutation(len(X_train))[:10000]  # No array copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6af67ca-b117-4f6d-a080-f7526b2a529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train[indices]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01197b71-e22d-46d9-907b-bb97120ad6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Y_train[indices]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd03126-50a8-4242-9395-f7f7a6f9717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c2dba-4035-414b-a6ec-13886acbfa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fed36e-e5c2-4ba5-a9b2-42f496dadaf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d96090-7a27-4af8-9238-b8bef494be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = analyze_histology_patches(X_scaled)\n",
    "print(f\"Optimal clusters: {results['optimal_k']}\")\n",
    "print(f\"Silhouette scores: {results['silhouette_scores']}\")\n",
    "# Save plots\n",
    "for name, fig in results['plots'].items():\n",
    "    display(fig)\n",
    "    fig.savefig(f\"{name}.png\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba886d9e-4267-4407-8dce-8107a08b66ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Try some other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0ae12b-2030-45e9-b5e2-b8c4f00dc3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced = pca.fit_transform(X_scaled)\n",
    "plt.scatter(reduced[:,0], reduced[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373394b0-7ed0-4b2a-9aba-157a51f28e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "import numpy as np\n",
    "\n",
    "def cluster_stability(X, n_clusters=2, n_iter=10):\n",
    "    agreements = []\n",
    "    for _ in range(n_iter):\n",
    "        km1 = KMeans(n_clusters=n_clusters).fit(X)\n",
    "        km2 = KMeans(n_clusters=n_clusters).fit(X)\n",
    "        agreements.append(adjusted_rand_score(km1.labels_, km2.labels_))\n",
    "    return np.mean(agreements)\n",
    "\n",
    "print(f\"Stability Score: {cluster_stability(X_scaled, n_clusters=7)}\")  # Should be >0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6288d1a-302e-4c04-aee6-f83cdfe684b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = [(n,cluster_stability(X_scaled, n_clusters=n)) for n in range(1,15)]\n",
    "search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ad30ee-fa57-4cf9-999f-96d9fa3d5628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "def evaluate_k(X, k):\n",
    "    km = KMeans(n_clusters=k).fit(X)\n",
    "    return {\n",
    "        'stability': cluster_stability(X, k),\n",
    "        'silhouette': silhouette_score(X, km.labels_),\n",
    "        'davies_bouldin': davies_bouldin_score(X, km.labels_)\n",
    "    }\n",
    "\n",
    "metrics = [(k, evaluate_k(X_scaled, k)) for k in tqdm(range(2,15))]\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa80196c-e862-4bb0-8c54-10f40a4286d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace PCA with UMAP for non-linear reduction\n",
    "import umap\n",
    "reducer = umap.UMAP(n_components=15)\n",
    "X_umap = reducer.fit_transform(X_scaled)\n",
    "metrics = [(k, evaluate_k(X_umap, k)) for k in tqdm(range(2,15))]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b99f1b-4bf6-4516-838d-bd691f7d1304",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, n_init=50).fit(X_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f433404-17d6-4050-a60d-fca4fcaae601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal\n",
    "for gene in range(X_scaled.shape[1]):\n",
    "    _, p = kruskal(*[X_scaled[kmeans.labels_==i, gene] for i in range(4)])\n",
    "    if p < 0.001: print(f\"Gene {gene} significant (p={p:.2e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4461cbff-0ad2-49b6-ace8-3330b157a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "models = [\n",
    "    KMeans(n_clusters=4),\n",
    "    AgglomerativeClustering(n_clusters=4)\n",
    "]\n",
    "consensus = np.mean([m.fit_predict(X_umap) for m in models], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f98742-a1d7-453a-91df-a2ed17d0eb87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find indices where both algorithms agree\n",
    "strong_agreement_indices = np.where(np.isin(consensus, [0, 1, 2, 3]))[0]\n",
    "print(f\"Number of points with strong agreement: {len(strong_agreement_indices)}\")\n",
    "# Find indices where algorithms disagree\n",
    "disagreement_indices = np.where(np.isin(consensus, [0.5]))[0]\n",
    "print(f\"Number of points with disagreement: {len(disagreement_indices)}\")\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import numpy as np\n",
    "\n",
    "# Ensure X_umap is defined\n",
    "if 'X_umap' not in locals():\n",
    "    # Assuming X_scaled is your data\n",
    "    reducer = umap.UMAP(n_components=2)\n",
    "    X_umap = reducer.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_umap[:, 0], X_umap[:, 1], c=consensus, cmap='viridis', s=50)\n",
    "plt.colorbar(label='Consensus Value')\n",
    "plt.title('UMAP Visualization with Consensus Clustering')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175f8679-c908-496f-9b27-c32badcf53a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## And again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26750d7-9b21-441d-a386-6a54937b9d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA, NMF, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Assuming X is your standardized 10000 x 460 matrix\n",
    "X = X_scaled\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, _, _ = train_test_split(X, X, test_size=0.2)\n",
    "\n",
    "n_components = 15  # Number of components for most methods\n",
    "\n",
    "methods = {\n",
    "    \"PCA\": PCA(n_components=8),\n",
    "    \"UMAP\": umap.UMAP(n_components=4),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, method in methods.items():\n",
    "    print(f\"Processing {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Fit and transform the training data\n",
    "        X_reduced_train = method.fit_transform(X_train)\n",
    "        \n",
    "        # Transform the test data\n",
    "        if name == \"t-SNE\":\n",
    "            # t-SNE doesn't have a transform method for new data\n",
    "            X_reduced_test = TSNE(n_components=tsne_components, method='exact', \n",
    "                                 random_state=42, n_iter=250).fit_transform(X_test)\n",
    "        else:\n",
    "            X_reduced_test = method.transform(X_test)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Reconstruct the data (approximate inverse transform)\n",
    "        if name == \"UMAP\":\n",
    "            # UMAP doesn't have a direct inverse transform, so use Nearest Neighbors\n",
    "            nn = NearestNeighbors(n_neighbors=1, metric='euclidean')\n",
    "            nn.fit(X_reduced_train)  # Train on reduced training data\n",
    "            distances, indices = nn.kneighbors(X_reduced_test)\n",
    "            # Find the original high-dimensional points corresponding to the reduced points\n",
    "            X_reconstructed = X_train[indices.flatten()].reshape(X_test.shape)\n",
    "        elif name == \"t-SNE\":\n",
    "            # t-SNE doesn't have an inverse transform\n",
    "            X_reconstructed = X_test\n",
    "        else:\n",
    "            X_reconstructed = method.inverse_transform(X_reduced_test)\n",
    "        \n",
    "        # Calculate MSE\n",
    "        mse = mean_squared_error(X_test, X_reconstructed)\n",
    "        \n",
    "        results[name] = {\n",
    "            \"MSE\": mse,\n",
    "            \"Time\": end_time - start_time\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {name}: {str(e)}\")\n",
    "        results[name] = {\n",
    "            \"MSE\": float('nan'),\n",
    "            \"Time\": float('nan')\n",
    "        }\n",
    "\n",
    "# Print Results\n",
    "for name, result in results.items():\n",
    "    print(f\"Method: {name}\")\n",
    "    print(f\"  MSE: {result['MSE']:.4f}\")\n",
    "    print(f\"  Time: {result['Time']:.4f} seconds\")\n",
    "\n",
    "# Filter out methods with NaN MSE for plotting\n",
    "valid_results = {k: v for k, v in results.items() if not np.isnan(v[\"MSE\"])}\n",
    "\n",
    "if valid_results:\n",
    "    # Bar plot for MSE\n",
    "    names = list(valid_results.keys())\n",
    "    mse_values = [result['MSE'] for result in valid_results.values()]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(names, mse_values, color='skyblue')\n",
    "    plt.xlabel('Method')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.title('MSE Comparison for Different Dimensionality Reduction Methods')\n",
    "    plt.ylim(0, max(mse_values) * 1.1)  # Adjust y-axis limit\n",
    "    plt.show()\n",
    "\n",
    "    # Bar plot for time\n",
    "    time_values = [result['Time'] for result in valid_results.values()]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(names, time_values, color='lightgreen')\n",
    "    plt.xlabel('Method')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.title('Time Comparison for Different Dimensionality Reduction Methods')\n",
    "    plt.ylim(0, max(time_values) * 1.1)  # Adjust y-axis limit\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid results to plot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81b0032-41d3-4657-a116-0eeb2cc78f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize_pca_clusters import visualize_pca_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bcc30f-6d9b-4bf3-b9a8-6e75b9d8cec4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Assuming X_scaled is your standardized 10000 x 460 matrix\n",
    "visualize_pca_clusters(X_scaled, n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef1492e-a119-4390-afe0-97796cdf7d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import umap  # Import UMAP\n",
    "\n",
    "def visualize_clusters(X, n_components, method='pca'):\n",
    "    \"\"\"\n",
    "    Applies either PCA or UMAP for initial dimension reduction, then KMeans clustering,\n",
    "    and visualizes the results using UMAP for 2D projection.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Standardized data matrix (samples x features).\n",
    "        n_components (int): Number of components and clusters to use.\n",
    "        method (str): 'pca' or 'umap', specifying the dimension reduction method.\n",
    "    \"\"\"\n",
    "    if method == 'pca':\n",
    "        # Apply PCA\n",
    "        reducer = PCA(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    elif method == 'umap':\n",
    "        # Apply UMAP for initial dimension reduction\n",
    "        reducer = umap.UMAP(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pca' or 'umap'\")\n",
    "\n",
    "    # Apply KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=n_components, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_reduced)\n",
    "\n",
    "    # Visualize using UMAP for 2D projection\n",
    "    reducer_umap = umap.UMAP(n_components=2)\n",
    "    X_umap = reducer_umap.fit_transform(X_reduced)\n",
    "\n",
    "    # Create a scatter plot of the reduced data, colored by cluster labels\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=cluster_labels, cmap='viridis', s=5)\n",
    "    plt.title(f'{method.upper()}-Reduced Data with KMeans Clusters (k={n_components})')\n",
    "    plt.xlabel('UMAP Component 1')\n",
    "    plt.ylabel('UMAP Component 2')\n",
    "\n",
    "    # Add a legend\n",
    "    legend1 = plt.legend(*scatter.legend_elements(),\n",
    "                         loc=\"upper right\", title=\"Clusters\")\n",
    "    plt.gca().add_artist(legend1)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X_scaled is your standardized 10000 x 460 matrix\n",
    "visualize_clusters(X_scaled, n_components=8, method='pca')\n",
    "visualize_clusters(X_scaled, n_components=8, method='umap')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaca4d2-9427-408e-8271-a8b3c9610518",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_clusters(X_scaled, n_components=4, method='pca')\n",
    "visualize_clusters(X_scaled, n_components=4, method='umap')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9596c3-249e-44de-9929-58a00a10e5a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import umap  # Import UMAP\n",
    "\n",
    "def visualize_clusters(X, n_components, method='pca'):\n",
    "    \"\"\"\n",
    "    Applies either PCA or UMAP for initial dimension reduction, then KMeans clustering,\n",
    "    and visualizes the results using UMAP for 2D projection.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Standardized data matrix (samples x features).\n",
    "        n_components (int): Number of components and clusters to use.\n",
    "        method (str): 'pca' or 'umap', specifying the dimension reduction method.\n",
    "    \"\"\"\n",
    "    if method == 'pca':\n",
    "        # Apply PCA\n",
    "        reducer = PCA(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    elif method == 'umap':\n",
    "        # Apply UMAP for initial dimension reduction\n",
    "        reducer = umap.UMAP(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pca' or 'umap'\")\n",
    "\n",
    "    # Apply KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=n_components, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_reduced)\n",
    "\n",
    "    # Visualize using UMAP for 2D projection\n",
    "    reducer_umap = umap.UMAP(n_components=2)\n",
    "    X_umap = reducer_umap.fit_transform(X_reduced)\n",
    "\n",
    "    # Create a scatter plot of the reduced data, colored by cluster labels\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=cluster_labels, cmap='viridis', s=5)\n",
    "    plt.title(f'{method.upper()}-Reduced Data with KMeans Clusters (k={n_components})')\n",
    "    plt.xlabel('UMAP Component 1')\n",
    "    plt.ylabel('UMAP Component 2')\n",
    "\n",
    "    # Add a legend\n",
    "    legend1 = plt.legend(*scatter.legend_elements(),\n",
    "                         loc=\"upper right\", title=\"Clusters\")\n",
    "    plt.gca().add_artist(legend1)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X_scaled is your standardized 10000 x 460 matrix\n",
    "visualize_clusters(X_scaled, n_components=4, method='pca')\n",
    "visualize_clusters(X_scaled, n_components=4, method='umap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58351a2-e3af-493e-956d-946da14fc48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import umap  # Import UMAP\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "\n",
    "def visualize_clusters(X, n_components, method='pca'):\n",
    "    \"\"\"\n",
    "    Applies either PCA or UMAP for initial dimension reduction, then KMeans clustering,\n",
    "    and visualizes the results using UMAP for 2D projection.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Standardized data matrix (samples x features).\n",
    "        n_components (int): Number of components to use.\n",
    "        method (str): 'pca' or 'umap', specifying the dimension reduction method.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dimension Reduction Step\n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    elif method == 'umap':\n",
    "        reducer = umap.UMAP(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    elif method == 'random_projection':\n",
    "        reducer = GaussianRandomProjection(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pca', 'umap', or 'random_projection'\")\n",
    "\n",
    "    # Clustering Step (KMeans)\n",
    "    kmeans = KMeans(n_clusters=n_components, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_reduced)\n",
    "    \n",
    "    # Visualization Step (UMAP)\n",
    "    reducer_umap = umap.UMAP(n_components=2)\n",
    "    X_umap = reducer_umap.fit_transform(X_reduced)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=cluster_labels, cmap='viridis', s=5)\n",
    "    plt.title(f'{method.upper()}-Reduced Data with KMeans Clusters (k={n_components})')\n",
    "    plt.xlabel('UMAP Component 1')\n",
    "    plt.ylabel('UMAP Component 2')\n",
    "    plt.colorbar(label='Cluster')  # Add colorbar for better interpretation\n",
    "\n",
    "    # Add a legend\n",
    "    legend1 = plt.legend(*scatter.legend_elements(),\n",
    "                         loc=\"upper right\", title=\"Clusters\")\n",
    "    plt.gca().add_artist(legend1)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X_scaled is your standardized 10000 x 460 matrix\n",
    "X = X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4689b5-3e3f-4098-b1da-777f8f33d2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now test each visualization method, and pick different numbers of principle components (10)\n",
    "n_components=5\n",
    "visualize_clusters(X, n_components=n_components, method='pca')\n",
    "visualize_clusters(X, n_components=n_components, method='umap')\n",
    "visualize_clusters(X, n_components=n_components, method='random_projection')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546aa1a5-94de-4577-9f24-8c2b2c8086c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## PCA k=6 for 460 gene data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7f08d-865d-4e3a-b61d-d28a182942c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import umap  # Import UMAP\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def evaluate_clustering(X, n_components, method='pca'):\n",
    "    \"\"\"\n",
    "    Applies either PCA or UMAP for initial dimension reduction, then KMeans clustering,\n",
    "    and calculates Silhouette Score and Davies-Bouldin Index.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Standardized data matrix (samples x features).\n",
    "        n_components (int): Number of components to use.\n",
    "        method (str): 'pca' or 'umap', specifying the dimension reduction method.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing Silhouette Score and Davies-Bouldin Index.\n",
    "    \"\"\"\n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    elif method == 'umap':\n",
    "        reducer = umap.UMAP(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pca' or 'umap'\")\n",
    "\n",
    "    # Clustering with KMeans\n",
    "    kmeans = KMeans(n_clusters=n_components, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_reduced)\n",
    "\n",
    "    # Evaluate clustering\n",
    "    silhouette = silhouette_score(X_reduced, cluster_labels)\n",
    "    db_index = davies_bouldin_score(X_reduced, cluster_labels)\n",
    "\n",
    "    return {'Silhouette Score': silhouette, 'Davies-Bouldin Index': db_index, 'cluster_labels': cluster_labels, 'X_reduced': X_reduced}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e9a21-683f-4a44-b7f5-ae1b8a79259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(X, n_components, method='pca'):\n",
    "    \"\"\"\n",
    "    Visualizes clusters after applying either PCA or UMAP for initial dimension reduction,\n",
    "    then KMeans clustering. Adds centroid labels.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Standardized data matrix (samples x features).\n",
    "        n_components (int): Number of components to use.\n",
    "        method (str): 'pca' or 'umap', specifying the dimension reduction method.\n",
    "    \"\"\"\n",
    "    # Evaluate clustering to get cluster labels and reduced data\n",
    "    eval_results = evaluate_clustering(X, n_components, method)\n",
    "    cluster_labels = eval_results['cluster_labels']\n",
    "    X_reduced = eval_results['X_reduced']\n",
    "    \n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    elif method == 'umap':\n",
    "        reducer = umap.UMAP(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pca' or 'umap'\")\n",
    "\n",
    "    # Clustering with KMeans\n",
    "    kmeans = KMeans(n_clusters=n_components, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_reduced)\n",
    "\n",
    "    # Visualize using UMAP for 2D projection\n",
    "    reducer_umap = umap.UMAP(n_components=2)\n",
    "    X_umap = reducer_umap.fit_transform(X_reduced)\n",
    "\n",
    "    # Calculate centroids\n",
    "    centroids = []\n",
    "    for i in range(n_components):\n",
    "        centroids.append(np.mean(X_umap[cluster_labels == i], axis=0))\n",
    "    centroids = np.array(centroids)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=cluster_labels, cmap='viridis', s=5)\n",
    "    plt.title(f'{method.upper()}-Reduced Data with KMeans Clusters (k={n_components})')\n",
    "    plt.xlabel('UMAP Component 1')\n",
    "    plt.ylabel('UMAP Component 2')\n",
    "    plt.colorbar(label='Cluster')  # Add colorbar for better interpretation\n",
    "    \n",
    "    # Add centroid labels\n",
    "    for i, centroid in enumerate(centroids):\n",
    "        plt.text(centroid[0], centroid[1], str(i + 1), color='magenta', fontsize=16, ha='center', va='center', weight='bold')\n",
    "        \n",
    "    # Add a legend\n",
    "    legend1 = plt.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Clusters\")\n",
    "    plt.gca().add_artist(legend1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f83e1-d5c6-470d-9942-3c66dd47d524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Assuming X_scaled is your standardized 10000 x 460 matrix\n",
    "X = X_scaled\n",
    "\n",
    "# Evaluate different methods and number of components\n",
    "results = {}\n",
    "for method in ['pca', 'umap']:\n",
    "    results[method] = {}\n",
    "    for n_components in range(2, 11):  # Test from 2 to 10 components\n",
    "        metrics = evaluate_clustering(X, n_components, method=method)\n",
    "        results[method][n_components] = metrics\n",
    "        print(f\"Method: {method}, Components: {n_components}, Silhouette Score: {metrics['Silhouette Score']:.4f}, Davies-Bouldin Index: {metrics['Davies-Bouldin Index']:.4f}\")\n",
    "\n",
    "# Visualize the best result based on Silhouette Score (you can change this to Davies-Bouldin Index)\n",
    "best_method = max(results, key=lambda m: max(results[m], key=lambda k: results[m][k]['Silhouette Score']))\n",
    "best_n_components = max(results[best_method], key=lambda k: results[best_method][k]['Silhouette Score'])\n",
    "\n",
    "print(f\"Best Method: {best_method}, Best Components: {best_n_components}\")\n",
    "visualize_clusters(X, n_components=best_n_components, method=best_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10484456-316d-4e18-8e5d-89553493a47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import umap  # Import UMAP\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def evaluate_clustering(X, n_components, method='pca'):\n",
    "    \"\"\"\n",
    "    Applies either PCA or UMAP for initial dimension reduction, then KMeans clustering,\n",
    "    and calculates Silhouette Score and Davies-Bouldin Index.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Standardized data matrix (samples x features).\n",
    "        n_components (int): Number of components to use.\n",
    "        method (str): 'pca' or 'umap', specifying the dimension reduction method.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing Silhouette Score and Davies-Bouldin Index.\n",
    "    \"\"\"\n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    elif method == 'umap':\n",
    "        reducer = umap.UMAP(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pca' or 'umap'\")\n",
    "\n",
    "    # Clustering with KMeans\n",
    "    kmeans = KMeans(n_clusters=n_components, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_reduced)\n",
    "\n",
    "    # Evaluate clustering\n",
    "    silhouette = silhouette_score(X_reduced, cluster_labels)\n",
    "    db_index = davies_bouldin_score(X_reduced, cluster_labels)\n",
    "\n",
    "    return {'Silhouette Score': silhouette, 'Davies-Bouldin Index': db_index, 'cluster_labels': cluster_labels, 'X_reduced': X_reduced}\n",
    "\n",
    "def visualize_clusters(X, n_components, method='pca', ax=None):\n",
    "    \"\"\"\n",
    "    Visualizes clusters after applying either PCA or UMAP for initial dimension reduction,\n",
    "    then KMeans clustering. Adds centroid labels.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Standardized data matrix (samples x features).\n",
    "        n_components (int): Number of components to use.\n",
    "        method (str): 'pca' or 'umap', specifying the dimension reduction method.\n",
    "        ax (matplotlib.axes._axes.Axes, optional): The axes object to draw the plot onto. If None, creates a new figure and axes.\n",
    "    \"\"\"\n",
    "    # Evaluate clustering to get cluster labels and reduced data\n",
    "    eval_results = evaluate_clustering(X, n_components, method)\n",
    "    cluster_labels = eval_results['cluster_labels']\n",
    "    X_reduced = eval_results['X_reduced']\n",
    "    silhouette = eval_results['Silhouette Score']\n",
    "    db_index = eval_results['Davies-Bouldin Index']\n",
    "    \n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    elif method == 'umap':\n",
    "        reducer = umap.UMAP(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pca' or 'umap'\")\n",
    "\n",
    "    # Clustering with KMeans\n",
    "    kmeans = KMeans(n_clusters=n_components, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_reduced)\n",
    "\n",
    "    # Visualize using UMAP for 2D projection\n",
    "    reducer_umap = umap.UMAP(n_components=2)\n",
    "    X_umap = reducer_umap.fit_transform(X_reduced)\n",
    "\n",
    "    # Calculate centroids\n",
    "    centroids = []\n",
    "    for i in range(n_components):\n",
    "        centroids.append(np.mean(X_umap[cluster_labels == i], axis=0))\n",
    "    centroids = np.array(centroids)\n",
    "\n",
    "    # Plotting\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    scatter = ax.scatter(X_umap[:, 0], X_umap[:, 1], c=cluster_labels, cmap='viridis', s=5)\n",
    "    ax.set_title(f'{method.upper()} (k={n_components})\\nSilhouette={silhouette:.2f}, DBIndex={db_index:.2f}')\n",
    "    ax.set_xlabel('UMAP Component 1')\n",
    "    ax.set_ylabel('UMAP Component 2')\n",
    "    \n",
    "    # Add centroid labels\n",
    "    for i, centroid in enumerate(centroids):\n",
    "        ax.text(centroid[0], centroid[1], str(i), color=plt.cm.viridis(i/n_components), fontsize=12, ha='center', va='center')\n",
    "\n",
    "    # Add a legend\n",
    "    legend1 = ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Clusters\")\n",
    "    ax.add_artist(legend1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7d87b0-3f1a-437f-9fe8-dc253f2e903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Assuming X_scaled is your standardized 10000 x 460 matrix\n",
    "X = X_scaled\n",
    "\n",
    "# Define the range of n_components to test\n",
    "n_components_range = range(4, 9)  # From 4 to 8\n",
    "\n",
    "# Create a figure with subplots for each n_components\n",
    "fig, axes = plt.subplots(len(n_components_range), 2, figsize=(20, 8 * len(n_components_range)))\n",
    "axes = axes.flatten()  # Flatten the 2D array of axes for easier indexing\n",
    "\n",
    "# Loop over each n_components and visualize PCA and UMAP side by side\n",
    "for i, n_components in enumerate(n_components_range):\n",
    "    visualize_clusters(X, n_components=n_components, method='pca', ax=axes[2*i])\n",
    "    visualize_clusters(X, n_components=n_components, method='umap', ax=axes[2*i + 1])\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdf9cb0-8042-4471-b14c-e52bf0450c8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import umap  # Import UMAP\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def evaluate_clustering(X, n_components, method='pca'):\n",
    "    \"\"\"\n",
    "    Applies either PCA or UMAP for initial dimension reduction, then KMeans clustering,\n",
    "    and calculates Silhouette Score and Davies-Bouldin Index.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Standardized data matrix (samples x features).\n",
    "        n_components (int): Number of components to use.\n",
    "        method (str): 'pca' or 'umap', specifying the dimension reduction method.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing Silhouette Score and Davies-Bouldin Index.\n",
    "    \"\"\"\n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    elif method == 'umap':\n",
    "        reducer = umap.UMAP(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pca' or 'umap'\")\n",
    "\n",
    "    # Clustering with KMeans\n",
    "    kmeans = KMeans(n_clusters=n_components, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_reduced)\n",
    "\n",
    "    # Evaluate clustering\n",
    "    silhouette = silhouette_score(X_reduced, cluster_labels)\n",
    "    db_index = davies_bouldin_score(X_reduced, cluster_labels)\n",
    "\n",
    "    return {'Silhouette Score': silhouette, 'Davies-Bouldin Index': db_index, 'cluster_labels': cluster_labels, 'X_reduced': X_reduced}\n",
    "\n",
    "def visualize_clusters(X, n_components, method='pca'):\n",
    "    \"\"\"\n",
    "    Visualizes clusters after applying either PCA or UMAP for initial dimension reduction,\n",
    "    then KMeans clustering. Adds centroid labels.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Standardized data matrix (samples x features).\n",
    "        n_components (int): Number of components to use.\n",
    "        method (str): 'pca' or 'umap', specifying the dimension reduction method.\n",
    "    \"\"\"\n",
    "    # Evaluate clustering to get cluster labels and reduced data\n",
    "    eval_results = evaluate_clustering(X, n_components, method)\n",
    "    cluster_labels = eval_results['cluster_labels']\n",
    "    X_reduced = eval_results['X_reduced']\n",
    "    silhouette = eval_results['Silhouette Score']\n",
    "    db_index = eval_results['Davies-Bouldin Index']\n",
    "    \n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    elif method == 'umap':\n",
    "        reducer = umap.UMAP(n_components=n_components)\n",
    "        X_reduced = reducer.fit_transform(X)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pca' or 'umap'\")\n",
    "\n",
    "    # Clustering with KMeans\n",
    "    kmeans = KMeans(n_clusters=n_components, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_reduced)\n",
    "\n",
    "    # Visualize using UMAP for 2D projection\n",
    "    reducer_umap = umap.UMAP(n_components=2)\n",
    "    X_umap = reducer_umap.fit_transform(X_reduced)\n",
    "\n",
    "    # Calculate centroids\n",
    "    centroids = []\n",
    "    for i in range(n_components):\n",
    "        centroids.append(np.mean(X_umap[cluster_labels == i], axis=0))\n",
    "    centroids = np.array(centroids)\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))  # Increased figure size for better readability\n",
    "    scatter = ax.scatter(X_umap[:, 0], X_umap[:, 1], c=cluster_labels, cmap='viridis', s=5)\n",
    "    ax.set_title(f'{method.upper()} (k={n_components})\\nSilhouette={silhouette:.2f}, DBIndex={db_index:.2f}', fontsize=14)  # Increased fontsize for title\n",
    "    ax.set_xlabel('UMAP Component 1', fontsize=12)  # Increased fontsize for labels\n",
    "    ax.set_ylabel('UMAP Component 2', fontsize=12)\n",
    "    \n",
    "    # Add centroid labels\n",
    "    for i, centroid in enumerate(centroids):\n",
    "        ax.text(centroid[0], centroid[1], str(i + 1), color='magenta', fontsize=18, ha='center', va='center', weight='bold')\n",
    "\n",
    "    # Add a legend\n",
    "    legend1 = ax.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Clusters\")\n",
    "    ax.add_artist(legend1)\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3fccc8-59b9-438b-b568-040cd4aed033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Assuming X_scaled is your standardized 10000 x 460 matrix\n",
    "X = X_scaled\n",
    "\n",
    "# Define the range of n_components to test\n",
    "n_components_range = range(4, 9)  # From 4 to 8\n",
    "\n",
    "# Loop over each n_components and visualize PCA and UMAP separately\n",
    "for n_components in n_components_range:\n",
    "    print(f\"Plotting for n_components = {n_components}...\")\n",
    "    visualize_clusters(X, n_components=n_components, method='pca')\n",
    "    visualize_clusters(X, n_components=n_components, method='umap')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876315fc-74bf-4306-b364-6a3a1204d68a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## PCA final decision for patch 1024 feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a97f9fe-90f6-40bd-852f-dfc0c478d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed2556f-de5e-4136-9001-79415221fd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fff1c9-ed76-4b66-986c-03ae384fe25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124e8339-62f6-4355-8ce3-e8a3200937df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_scaled[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b52479-a85c-46a5-a14e-f6f3e8be5e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of n_components to test\n",
    "n_components_range = range(2,9)\n",
    "\n",
    "# Loop over each n_components and visualize PCA and UMAP separately\n",
    "for n_components in n_components_range:\n",
    "    print(f\"Plotting for n_components = {n_components}...\")\n",
    "    visualize_clusters(X, n_components=n_components, method='pca')\n",
    "    # visualize_clusters(X, n_components=n_components, method='umap')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b246c37-3d69-4f60-95f9-e1e4a5184684",
   "metadata": {},
   "source": [
    "## Ridge Regress PCA of genes from PCA of feature vectors and measure MSE and Spearman rank correlation at that level, save fit as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfc35c6-a415-4829-ae64-3c46eac663ed",
   "metadata": {},
   "source": [
    "## Project from PCA of genes to genes and measure MSE and Spearman rank correlation at that level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9beafc5-24e6-4e51-b2c1-a9d63926350e",
   "metadata": {},
   "source": [
    "## PCA of 18157 genes less the 460 genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e9e800-df81-4008-8fef-d7edb7a79ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "scRNAseq = sc.read_h5ad('data/Crunch3_scRNAseq.h5ad')\n",
    "\n",
    "filter_value_no_cancer = \"n\" # Filtered column value indicating absence of cancer\n",
    "filter_value_cancer = \"y\" # Filtered column value indicating the presence of cancer\n",
    "filter_column_scRNAseq = \"dysplasia\"\n",
    "\n",
    "# Filter scRNAseq data by dysplasia status\n",
    "scRNAseq_no_cancer = scRNAseq[scRNAseq.obs[filter_column_scRNAseq] == filter_value_no_cancer].copy()\n",
    "scRNAseq_cancer = scRNAseq[scRNAseq.obs[filter_column_scRNAseq] == filter_value_cancer].copy()\n",
    "\n",
    "scRNAseq_no_cancer\n",
    "\n",
    "name_data = 'UC9_I'\n",
    "\n",
    "# Get the 18615 genes to rank\n",
    "gene_18615_list = list(scRNAseq.var.index)\n",
    "\n",
    "gene_460 = genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52892df4-1898-4b4e-a2c7-c78c352cd79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_genes = [g for g in gene_460 if g in gene_18615_list]\n",
    "print(\"Number of shared genes between scRNA-seq and xenium data:\", len(common_genes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcf235c-6408-466b-a0a9-373e2ad62c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmeasured_genes = [gene for gene in gene_18615_list if gene not in common_genes]\n",
    "print(\"Number of unmeasured genes in Xenium data:\", len(unmeasured_genes), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ba661-9c90-460e-8611-f0a2cef735bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scRNA-Seq data log1p-normalized with scale factor 10000 on 18615 genes\n",
    "rna_data_norm_10000_unmeasured_genes = scRNAseq[:, unmeasured_genes].X.toarray()\n",
    "\n",
    "from log1p_normalization_scale_factor import log1p_normalization_scale_factor\n",
    "\n",
    "# scRNA-Seq data log1p-normalized with scale factor 100 on 460 genes\n",
    "rna_data_norm_100_common_genes = log1p_normalization_scale_factor(scRNAseq[:, common_genes].layers[\"counts\"].toarray(), scale_factor=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a256fb-be46-4cb0-b828-f9f2b5aeb8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_data_norm_100_common_genes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee49abc2-9e59-410b-91cf-4464021e27f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Xenium data shape ({len(prediction_460_genes)} samples x {len(common_genes)} shared genes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b202fb3-62e5-41ca-9616-596db645f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity-Based Matching: Find the top_k most similar spots for each query\n",
    "top_k = 30\n",
    "print(f\"\\nCompute COSINE SIMILARITY: Find the top_k(={top_k}) similar scRNA-Seq cells for each Xenium cell...\\n\")\n",
    "matches, similarities = find_matches_cos_similarity(rna_data_norm_100_common_genes, prediction_460_genes[common_genes].values, top_k=top_k)\n",
    "del rna_data_norm_100_common_genes\n",
    "\n",
    "# Weighted Averaging of scRNA-Seq data log1p-normalized with scale factor 10000\n",
    "print(\"Compute WEIGHTED AVERAGE of unmeasured genes from scRNA-Seq based on similarity scores...\")\n",
    "weighted_avg_df_10000 = pd.DataFrame([\n",
    "    {\n",
    "        **dict(zip(unmeasured_genes, np.average(rna_data_norm_10000_unmeasured_genes[indices, :], axis=0, weights=similarity).round(2)))  # updated\n",
    "    }\n",
    "    for i, (indices, similarity) in enumerate(zip(matches, similarities))\n",
    "])\n",
    "weighted_avg_df_10000.index = prediction_460_genes.index\n",
    "\n",
    "# Free memory by deleting large variables and performing garbage collection\n",
    "del rna_data_norm_10000_unmeasured_genes, matches, similarities\n",
    "\n",
    "prediction_18615_genes = pd.concat([prediction_460_genes, weighted_avg_df_10000], axis=1)[gene_18615_list]\n",
    "\n",
    "print(f\"\\n-- {name_data} PREDICTION DONE --\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d779692-2e54-4dd6-bfb4-b74009438640",
   "metadata": {},
   "source": [
    "## Ridge Regress 18K gene PCA from 460 gene PCA and measure MSE and Spearman rank correlation at that level, save fit as G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfb3fea-8374-4831-870d-796ced657db3",
   "metadata": {},
   "source": [
    "## Project from 18K gene PCA to 18K genes and measure MSE and Spearman rank correlation at that level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c848bf-b3dd-4e23-8ce6-a59ffba6a9c8",
   "metadata": {},
   "source": [
    "## Apply F to features of sample image to get PCA of genes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c348267f-cf16-4bc4-8925-40d91728a0f4",
   "metadata": {},
   "source": [
    "## Apply G to F(sample) to get 18K genes PCA of sample image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b520eb10-2a56-42c6-88f2-ecaaf6ac8bd0",
   "metadata": {},
   "source": [
    "## Do log fold change of H = G(F(sample)) for H[dysplasia] versus H[non-dysplasia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adedd6f-0d92-42f1-a886-1573b86a69dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "broad",
   "language": "python",
   "name": "broad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
